"tech"	"callret-1"	"callret-2"	"callret-3"
"http://dbpedia.org/resource/Fuzzy_logic"	"Fuzzy logic"	"Artificial intelligence"	"Fuzzy logic is a form of many-valued logic in which the truth values of variables may be any real number between 0 and 1, considered to be ""fuzzy"". By contrast, in Boolean logic, the truth values of variables may only be the ""crisp"" values 0 or 1. Fuzzy logic has been employed to handle the concept of partial truth, where the truth value may range between completely true and completely false. Furthermore, when linguistic variables are used, these degrees may be managed by specific (membership) functions. The term fuzzy logic was introduced with the 1965 proposal of fuzzy set theory by Lotfi Zadeh. Fuzzy logic had however been studied since the 1920s, as infinite-valued logic—notably by Łukasiewicz and Tarski. Fuzzy logic has been applied to many fields, from control theory to artificial intelligence."
"http://dbpedia.org/resource/Pattern_theory"	"Pattern theory"	"Artificial intelligence"	"Pattern theory, formulated by Ulf Grenander, is a mathematical formalism to describe knowledge of the world as patterns. It differs from other approaches to artificial intelligence in that it does not begin by prescribing algorithms and machinery to recognize and classify patterns; rather, it prescribes a vocabulary to articulate and recast the pattern concepts in precise language. In addition to the new algebraic vocabulary, its statistical approach is novel in its aim to: 
*  Identify the hidden variables of a data set using real world data rather than artificial stimuli, which was previously commonplace. 
*  Formulate prior distributions for hidden variables and models for the observed variables that form the vertices of a Gibbs-like graph. 
*  Study the randomness and variability of these graphs. 
*  Create the basic classes of stochastic models applied by listing the deformations of the patterns. 
*  Synthesize (sample) from the models, not just analyze signals with it. Broad in its mathematical coverage, Pattern Theory spans algebra and statistics, as well as local topological and global entropic properties. The Brown University Pattern Theory Group was formed in 1972 by Ulf Grenander. Many mathematicians are currently working in this group, noteworthy among them being the Fields Medalist David Mumford. Mumford regards Grenander as his ""guru"" in this subject."
"http://dbpedia.org/resource/Kinect"	"Kinect"	"Artificial intelligence"	"Kinect (codenamed Project Natal during development) is a line of motion sensing input devices by Microsoft for Xbox 360 and Xbox One video game consoles and Windows PCs. Based around a webcam-style add-on peripheral, it enables users to control and interact with their console/computer without the need for a game controller, through a natural user interface using gestures and spoken commands. The first-generation Kinect was first introduced in November 2010 in an attempt to broaden Xbox 360's audience beyond its typical gamer base. A version for Windows was released on February 1, 2012. Kinect competes with several motion controllers on other home consoles, such as Wii Remote Plus for Wii and Wii U, PlayStation Move/PlayStation Eye for PlayStation 3, and PlayStation Camera for PlayStation 4. Microsoft released the Kinect software development kit for Windows 7 on June 16, 2011. This SDK was meant to allow developers to write Kinecting apps in C++/CLI, C#, or Visual Basic .NET."
"http://dbpedia.org/resource/ICAD_(software)"	"ICAD (software)"	"Artificial intelligence"	"ICAD (Corporate history: ICAD, Inc., Concentra, KTI, Dassault Systemes) is a Knowledge-Based Engineering (KBE) system that enables users to encode design knowledge using a semantic representation that can be evaluated for parasolid output. ICAD has an open architecture that can utilize all the power and flexibility of the underlying language. KBE, as implemented via ICAD, received a lot of attention due to the remarkable results that appeared to take little effort. ICAD allowed one example of end-user computing that in a sense is unparalleled. Most ICAD developers were degreed engineers. Systems developed by ICAD users were non-trivial and consisted of highly complicated code. In the sense of end-user computing, ICAD was the first to allow the power of a domain tool to be in the hands of the user at the same time being open to allow extensions as identified and defined by the domain expert or SME. A COE article looked at the resulting explosion of expectations (see AI Winter), which were not sustainable. However, such a bubble burst does not diminish the existence of capability that would exist if expectations and use were properly managed."
"http://dbpedia.org/resource/Knowledge_level"	"Knowledge level"	"Artificial intelligence"	"In artificial intelligence, knowledge-based agents draw on a pool of logical sentences to infer conclusions about the world. At the knowledge level, we only need to specify what the agent knows and what its goals are; a logical abstraction separate from details of implementation. This notion of knowledge level was first introduced by Allen Newell in the 1980s, to have a way to rationalize an agent's behavior. The agent takes actions based on knowledge it possesses, in an attempt to reach specific goals. It chooses actions according to the principle of rationality. Beneath the knowledge level resides the symbol level. Whereas the knowledge level is world oriented, namely that it concerns the environment in which the agent operates, the symbol level is system oriented, in that it includes the mechanisms the agent has available to operate. The knowledge level rationalizes the agent's behavior, while the symbol level mechanizes the agent's behavior. For example, in a computer program, the knowledge level consists of the information contained in its data structures that it uses to perform certain actions. The symbol level consists of the program's algorithms, the data structures themselves, and so on."
"http://dbpedia.org/resource/Darwin_machine"	"Darwin machine"	"Artificial intelligence"	"A Darwin machine (a 1987 coinage by William H. Calvin, by analogy to a Turing machine) is a machine that, like a Turing machine, involves an iteration process that yields a high-quality result, but, whereas a Turing machine uses logic, the Darwin machine uses rounds of variation, selection, and inheritance. In its original connotation, a Darwin machine is any process that bootstraps quality by utilizing all of the six essential features of a Darwinian process: A pattern is copied with variations, where populations of one variant pattern compete with another population, their relative success biased by a multifaceted environment (natural selection) so that winners predominate in producing the further variants of the next generation (Darwin's inheritance principle). More loosely, a Darwin machine is a process that utilizes some subset of the Darwinian essentials, typically natural selection to create a non-reproducing pattern, as in neural Darwinism. Many aspects of neural development utilize overgrowth followed by pruning to a pattern, but the resulting pattern does not itself create further copies. Darwin machine has been used multiple times to name computer programs after Charles Darwin."
"http://dbpedia.org/resource/Action_selection"	"Action selection"	"Artificial intelligence"	"Action selection is a way of characterizing the most basic problem of intelligent systems: what to do next. In artificial intelligence and computational cognitive science, ""the action selection problem"" is typically associated with intelligent agents and animats—artificial systems that exhibit complex behaviour in an agent environment. The term is also sometimes used in ethology or animal behavior. One problem for understanding action selection is determining the level of abstraction used for specifying an ""act"". At the most basic level of abstraction, an atomic act could be anything from contracting a muscle cell to provoking a war. Typically for any one action-selection mechanism, the set of possible actions is predefined and fixed. Most researchers working in this field place high demands on their agents: 
*  The acting agent typically must select its action in dynamic and unpredictable environments.  
*  The agents typically act in real time; therefore they must make decisions in a timely fashion. 
*  The agents are normally created to perform several different tasks. These tasks may conflict for resource allocation (e.g. can the agent put out a fire and deliver a cup of coffee at the same time?) 
*  The environment the agents operate in may include humans, who may make things more difficult for the agent (either intentionally or by attempting to assist.)  
*  The agents themselves are often intended to model animals or humans, and animal/human behaviour is quite complicated. For these reasons action selection is not trivial and attracts a good deal of research."
"http://dbpedia.org/resource/Computational_creativity"	"Computational creativity"	"Artificial intelligence"	"Computational creativity (also known as artificial creativity, mechanical creativity or creative computation) is a multidisciplinary endeavour that is located at the intersection of the fields of artificial intelligence, cognitive psychology, philosophy, and the arts. The goal of computational creativity is to model, simulate or replicate creativity using a computer, to achieve one of several ends: 
*  To construct a program or computer capable of human-level creativity. 
*  To better understand human creativity and to formulate an algorithmic perspective on creative behavior in humans. 
*  To design programs that can enhance human creativity without necessarily being creative themselves. The field of computational creativity concerns itself with theoretical and practical issues in the study of creativity. Theoretical work on the nature and proper definition of creativity is performed in parallel with practical work on the implementation of systems that exhibit creativity, with one strand of work informing the other."
"http://dbpedia.org/resource/Decision_list"	"Decision list"	"Artificial intelligence"	"Decision lists are a representation for Boolean functions. Single term decision lists are more expressive than disjunctions and conjunctions; however, 1-term decision lists are less expressive than the general disjunctive normal form and the conjunctive normal form. The language specified by a k-length decision list includes as a subset the language specified by a k-depth decision tree. Learning decision lists can be used for attribute efficient learning."
"http://dbpedia.org/resource/Neural_modeling_fields"	"Neural modeling fields"	"Artificial intelligence"	"Neural modeling field (NMF) is a mathematical framework for machine learning which combines ideas from neural networks, fuzzy logic, and model based recognition. It has also been referred to as modeling fields, modeling fields theory (MFT), Maximum likelihood artificial neural networks (MLANS). This framework has been developed by Leonid Perlovsky at the AFRL. NMF is interpreted as a mathematical description of mind’s mechanisms, including concepts, emotions, instincts, imagination, thinking, and understanding. NMF is a multi-level, hetero-hierarchical system. At each level in NMF there are concept-models encapsulating the knowledge; they generate so-called top-down signals, interacting with input, bottom-up signals. These interactions are governed by dynamic equations, which drive concept-model learning, adaptation, and formation of new concept-models for better correspondence to the input, bottom-up signals."
"http://dbpedia.org/resource/Knowledge_acquisition"	"Knowledge acquisition"	"Artificial intelligence"	"Knowledge acquisition is the process used to define the rules and ontologies required for a knowledge-based system. The phrase was first used in conjunction with expert systems to describe the initial tasks associated with developing an expert system, namely finding and interviewing domain experts and capturing their knowledge via rules, objects, and frame-based ontologies. Expert systems were one of the first successful applications of artificial intelligence technology to real world business problems. Researchers at Stanford and other AI laboratories worked with doctors and other highly skilled experts to develop systems that could automate complex tasks such as medical diagnosis. Until this point computers had mostly been used to automate highly data intensive tasks but not for complex reasoning. Technologies such as inference engines allowed developers for the first time to tackle more complex problems. As expert systems scaled up from demonstration prototypes to industrial strength applications it was soon realized that the acquisition of domain expert knowledge was one of if not the most critical task in the knowledge engineering process. This knowledge acquisition process became an intense area of research on its own. One approach to knowledge acquisition investigated was to use natural language parsing and generation to facilitate knowledge acquisition. Natural language parsing could be performed on manuals and other expert documents and an initial first pass at the rules and objects could be developed automatically. Text generation was also extremely useful in generating explanations for system behavior. This greatly facilitated the development and maintenance of expert systems."
"http://dbpedia.org/resource/Winner-take-all_in_action_selection"	"Winner-take-all in action selection"	"Artificial intelligence"	"Winner-take-all is a computer science concept that has been widely applied in behavior-based robotics as a method of action selection for intelligent agents. Winner-take-all systems work by connecting modules (task-designated areas) in such a way that when one action is performed it stops all other actions from being performed, so only one action is occurring at a time. The name comes from the idea that the ""winner"" action takes all of the motor system's power."
"http://dbpedia.org/resource/Outline_of_artificial_intelligence"	"Outline of artificial intelligence"	"Artificial intelligence"	"The following outline is provided as an overview of and topical guide to artificial intelligence: Artificial intelligence (AI) – intelligence exhibited by machines or software. It is also the name of the academic field which studies how to create computers and computer software that are capable of intelligent behaviour."
"http://dbpedia.org/resource/Intelligent_control"	"Intelligent control"	"Artificial intelligence"	"Intelligent control is a class of control techniques that use various artificial intelligence computing approaches like neural networks, Bayesian probability, fuzzy logic, machine learning, evolutionary computation and genetic algorithms."
"http://dbpedia.org/resource/Cognitive_tutor"	"Cognitive tutor"	"Artificial intelligence"	"A cognitive tutor is a particular kind of intelligent tutoring system that utilizes a cognitive model to provide feedback to students as they are working through problems. This feedback will immediately inform students of the correctness, or incorrectness, of their actions in the tutor interface; however, cognitive tutors also have the ability to provide context-sensitive hints and instruction to guide students towards reasonable next steps."
"http://dbpedia.org/resource/Intel_RealSense"	"Intel RealSense"	"Artificial intelligence"	"Intel RealSense, formerly known as Intel Perceptual Computing, is a platform for implementing gesture-based human-computer interaction techniques. It consists of series of consumer grade 3D cameras together with an easy to use machine perception library that simplifies supporting the cameras for third-party software developers. As of March 2015, multiple laptop and tablet computer manufactures offer one or more devices with Intel RealSense camera built in. These are Asus, HP, Dell, Lenovo, and Acer. At the same time, a yet to be released standalone webcam from Razer will feature an Intel RealSense camera. Consumer-ready versions of the RealSense camera are the Razer Stargazer and the Creative BlasterX Senz3D."
"http://dbpedia.org/resource/Machine_perception"	"Machine perception"	"Artificial intelligence"	"Machine perception is the capability of a computer system to interpret data in a manner that is similar to the way humans use their senses to relate to the world around them. The basic method that the computers take in and respond to their environment is through the attached hardware. Until recently input was limited to a keyboard, or a mouse, but advances in technology, both in hardware and software, have allowed computers to take in sensory input in a way similar to humans. Machine perception allows the computer to use this sensory input, as well as conventional computational means of gathering information to gather information with greater accuracy and to present it in a way that is more comfortable for the user. These include computer vision, machine hearing, and machine touch."
"http://dbpedia.org/resource/Conflict_resolution_strategy"	"Conflict resolution strategy"	"Artificial intelligence"	"Conflict resolution strategies are used in production systems in artificial intelligence, such as in rule-based expert systems, to help in choosing which production rule to fire. The need for such a strategy arises when the conditions of two or more rules are satisfied by the currently known facts."
"http://dbpedia.org/resource/Admissible_heuristic"	"Admissible heuristic"	"Artificial intelligence"	"In computer science, specifically in algorithms related to pathfinding, a heuristic function is said to be admissible if it never overestimates the cost of reaching the goal, i.e. the cost it estimates to reach the goal is not higher than the lowest possible cost from the current point in the path."
"http://dbpedia.org/resource/Constructionist_design_methodology"	"Constructionist design methodology"	"Artificial intelligence"	"Constructionist design methodology (CDM) is an approach for building highly modular systems of many interacting components. CDM's strength lies in simplifying the model of complex, multi functional systems that require architectural evolution of tangled data flow and control hierarchies."
"http://dbpedia.org/resource/Colloquis"	"Colloquis"	"Artificial intelligence"	"Colloquis, previously known as ActiveBuddy and Conversagent, was a company that created conversation-based interactive agents originally distributed via instant messaging platforms. The company had offices in New York, NY and Sunnyvale, CA. Founded in 2000, the company was the brainchild of Robert Hoffer, Timothy Kay and Peter Levitan. The idea for interactive agents (also known as Internet bots) came from the team's vision to add functionality to increasingly popular instant messaging services. The original implementation took shape as a word-based adventure game but quickly grew to include a wide range of database applications including access to news, weather, stock information, movie times, yellow pages listings, and detailed sports data, as well as a variety of tools (calculators, translator, etc.). These various applications were bundled into a single entity and launched as SmarterChild in 2001. SmarterChild acted as a showcase for the quick data access and possibilities for fun conversation that the company planned to turn into customized, niche specific products. The rapid success of SmarterChild led to targeted promotional products for Radiohead, Austin Powers, The Sporting News, and others. ActiveBuddy sought to strengthen its hold on the interactive agent market for the future by filing for, and receiving, a controversial patent on their creation in 2002. The company also released the BuddyScript SDK, a free developer kit that allow programmers to design and launch their own interactive agents using ActiveBuddy’s proprietary scripting language, in 2002. Ultimately, however, the decline in ad spending in 2001 and 2002 led to a shift in corporate strategy towards business focused Automated Service Agents, building products for clients including Cingular, Comcast and Cox Communications. The company subsequently changed its name from ActiveBuddy to Conversagent in 2003, and then again to Colloquis in 2006. Colloquis was later purchased by Microsoft in October 2006. Most of the ActiveBuddy, Colloquis and Microsoft team that launched and nurtured SmarterChild is now working on AB2."
"http://dbpedia.org/resource/Project_Joshua_Blue"	"Project Joshua Blue"	"Artificial intelligence"	"Joshua Blue is a project under development by IBM that focuses on advancing the artificial intelligence field by designing and programing computers to emulate human mental functions."
"http://dbpedia.org/resource/Document_mosaicing"	"Document mosaicing"	"Artificial intelligence"	"Document mosaicing is a process that stitches multiple, overlapping snapshot images of a document together in order to produce one large, high resolution composite. The document is slid under a stationary, over-the-desk camera by hand until all parts of the document are snapshotted by the camera’s field of view. As the document slid under the camera, all motion of the document is coarsely tracked by the vision system. The document is periodically snapshotted such that the successive snapshots are overlap by about 50%. The system then finds the overlapped pairs and stitches them together repeatedly until all pairs are stitched together as one piece of document. The document mosaicing can be divided into four main processes. 
* Tracking 
* Feature detecting 
* Correspondences establishing 
* Images mosaicing."
"http://dbpedia.org/resource/GENESIS_(software)"	"GENESIS (software)"	"Artificial intelligence"	"GENESIS (The GEneral NEural SImulation System) is a simulation environment for constructing realistic models of neurobiological systems at many levels of scale including: sub-cellular processes, individual neurons, networks of neurons, and neuronal systems. These simulations are “computer-based implementations of models whose primary objective is to capture what is known of the anatomical structure and physiological characteristics of the neural system of interest”. GENESIS is intended to quantify the physical framework of the nervous system in a way that allows for easy understanding of the physical structure of the nerves in question. “At present only GENESIS allows parallelized modeling of single neurons and networks on multiple-instruction-multiple-data parallel computers.” Since its inception, development of GENESIS software has since spread from its home at Caltech to labs at the University of Texas at San Antonio, the University of Antwerp, the National Centre for Biological Sciences in Bangalore, the University of Colorado, the Pittsburgh Supercomputing Center, the San Diego Supercomputer Center, and Emory University."
"http://dbpedia.org/resource/Natural_language_understanding"	"Natural language understanding"	"Artificial intelligence"	"Natural language understanding (NLU) is a subtopic of natural language processing in artificial intelligence that deals with machine reading comprehension. NLU is considered an AI-hard problem. The process of disassembling and parsing input is more complex than the reverse process of assembling output in natural language generation because of the occurrence of unknown and unexpected features in the input and the need to determine the appropriate syntactic and semantic schemes to apply to it, factors which are pre-determined when outputting language. There is considerable commercial interest in the field because of its application to news-gathering, text categorization, voice-activation, archiving, and large-scale content-analysis."
"http://dbpedia.org/resource/Cerebellar_model_articulation_controller"	"Cerebellar model articulation controller"	"Artificial intelligence"	"The cerebellar model arithmetic computer (CMAC) is a type of neural network based on a model of the mammalian cerebellum. It is also known as the cerebellar model articulation controller. It is a type of associative memory. The CMAC was first proposed as a function modeler for robotic controllers by James Albus in 1975 (hence the name), but has been extensively used in reinforcement learning and also as for automated classification in the machine learning community. CMAC computes a function , where is the number of input dimensions. The input space is divided up into hyper-rectangles, each of which is associated with a memory cell. The contents of the memory cells are the weights, which are adjusted during training. Usually, more than one quantisation of input space is used, so that any point in input space is associated with a number of hyper-rectangles, and therefore with a number of memory cells. The output of a CMAC is the algebraic sum of the weights in all the memory cells activated by the input point. A change of value of the input point results in a change in the set of activated hyper-rectangles, and therefore a change in the set of memory cells participating in the CMAC output. The CMAC output is therefore stored in a distributed fashion, such that the output corresponding to any point in input space is derived from the value stored in a number of memory cells (hence the name associative memory). This provides generalisation."
"http://dbpedia.org/resource/LIDA_(cognitive_architecture)"	"LIDA (cognitive architecture)"	"Artificial intelligence"	"The LIDA (Learning Intelligent Distribution Agent) cognitive architecture is an integrated artificial cognitive system that attempts to model a broad spectrum of cognition in biological systems, from low-level perception/action to high-level reasoning. Developed primarily by Stan Franklin and colleagues at the University of Memphis, the LIDA architecture is empirically grounded in cognitive science and cognitive neuroscience. In addition to providing hypotheses to guide further research, the architecture can support control structures for software agents and robots. Providing plausible explanations for many cognitive processes, the LIDA conceptual model is also intended as a tool with which to think about how minds work. Two hypotheses underlie the LIDA architecture and its corresponding conceptual model: 1) Much of human cognition functions by means of frequently iterated (~10 Hz) interactions, called cognitive cycles, between conscious contents, the various memory systems and action selection. 2) These cognitive cycles, serve as the ""atoms"" of cognition of which higher-level cognitive processes are composed."
"http://dbpedia.org/resource/Autonomic_computing"	"Autonomic computing"	"Artificial intelligence"	"Autonomic computing refers to the self-managing characteristics of distributed computing resources, adapting to unpredictable changes while hiding intrinsic complexity to operators and users. Started by IBM in 2001, this initiative ultimately aims to develop computer systems capable of self-management, to overcome the rapidly growing complexity of computing systems management, and to reduce the barrier that complexity poses to further growth. The system makes decisions on its own, using high-level policies; it will constantly check and optimize its status and automatically adapt itself to changing conditions. An autonomic computing framework is composed of autonomic components (AC) interacting with each other. An AC can be modeled in terms of two main control loops (local and global) with sensors (for self-monitoring), effectors (for self-adjustment), knowledge and planner/adapter for exploiting policies based on self- and environment awareness. This architecture is sometimes be referred to as Monitor-Analyze-Plan-Execute (MAPE). Driven by such vision, a variety of architectural frameworks based on “self-regulating” autonomic components has been recently proposed. A very similar trend has recently characterized significant research in the area of multi-agent systems. However, most of these approaches are typically conceived with centralized or cluster-based server architectures in mind and mostly address the need of reducing management costs rather than the need of enabling complex software systems or providing innovative services. Some  involve mobile agents interacting via loosely coupled communication mechanisms. Autonomy-oriented computation is a paradigm proposed by Jiming Liu in 2001 that uses artificial systems imitating social animals' collective behaviours to solve difficult computational problems. For example, ant colony optimization could be studied in this paradigm."
"http://dbpedia.org/resource/Principle_of_rationality"	"Principle of rationality"	"Artificial intelligence"	"The 'principle of rationality' (or 'rationality principle') is coined by Karl R. Popper in his Harvard Lecture of 1963, published in his book Myth of Framework. It is related with what he called the 'logic of the situation' in an Economica article of 1944/1945, published later in his book The Poverty of Historicism. Following Popper the 'logic of the situation' is the result of reconstructing meticulously all circumstances if you are trying to understand a historical event. The 'principle of rationality' is the assumption that people try to reach their goals. Both the reconstruction of the 'logic of the situation' and the application of the 'principle of rationality' constitute an important method of understanding that seems to be useful in all social sciences. It is the method of creating theories due to our knowledge that men and women usually have goals and usually try to reach their goals. If this method is applied to a particular situation it results in a theory which can be proved as right or wrong. Although the 'principle of rationality' is not applicable to (natural) science there is no methodological difference between science and social sciences because both sciences are getting explanations by inventing theories which can be proved. Popper called his 'principle of rationality' nearly empty (a technical term meaning without empirical content) and strictly speaking false, but nonetheless tremendously useful. These remarks earned him a lot of criticism because seemingly he had swerved from his famous Logic of Scientific Discovery. Among the many philosophers having discussed his 'principle of rationality' from the 1960s up to now are Noretta Koertge, R. Nadeau, Viktor J. Vanberg, Hans Albert, E. Matzner, Ian C. Jarvie, Mark A. Notturno, John Wettersten, Ian C. Böhm."
"http://dbpedia.org/resource/Symbol_level"	"Symbol level"	"Artificial intelligence"	"In knowledge-based systems, agents choose actions based on the principle of rationality to move closer to a desired goal. The agent is able to make decisions based on knowledge it has about the world (see knowledge level). But for the agent to actually change its state, it must use whatever means it has available. This level of description for the agent's behavior is the symbol level. For example, in a computer program, the knowledge level consists of the information contained in its data structures that it uses to perform certain actions. The symbol level consists of the program's algorithms, the data structures themselves, and so on."
"http://dbpedia.org/resource/Neuro-fuzzy"	"Neuro-fuzzy"	"Artificial intelligence"	"In the field of artificial intelligence, neuro-fuzzy refers to combinations of artificial neural networks and fuzzy logic. Neuro-fuzzy was proposed by J. S. R. Jang. Neuro-fuzzy hybridization results in a hybrid intelligent system that synergizes these two techniques by combining the human-like reasoning style of fuzzy systems with the learning and connectionist structure of neural networks. Neuro-fuzzy hybridization is widely termed as Fuzzy Neural Network (FNN) or Neuro-Fuzzy System (NFS) in the literature. Neuro-fuzzy system (the more popular term is used henceforth) incorporates the human-like reasoning style of fuzzy systems through the use of fuzzy sets and a linguistic model consisting of a set of IF-THEN fuzzy rules. The main strength of neuro-fuzzy systems is that they are universal approximators with the ability to solicit interpretable IF-THEN rules. The strength of neuro-fuzzy systems involves two contradictory requirements in fuzzy modeling: interpretability versus accuracy. In practice, one of the two properties prevails. The neuro-fuzzy in fuzzy modeling research field is divided into two areas: linguistic fuzzy modeling that is focused on interpretability, mainly the Mamdani model; and precise fuzzy modeling that is focused on accuracy, mainly the Takagi-Sugeno-Kang (TSK) model. Although generally assumed to be the realization of a fuzzy system through connectionist networks, this term is also used to describe some other configurations including: 
* Deriving fuzzy rules from trained RBF networks. 
* Fuzzy logic based tuning of neural network training parameters. 
* Fuzzy logic criteria for increasing a network size. 
* Realising fuzzy membership function through clustering algorithms in unsupervised learning in SOMs and neural networks. 
* Representing fuzzification, fuzzy inference and defuzzification through multi-layers feed-forward connectionist networks. It must be pointed out that interpretability of the Mamdani-type neuro-fuzzy systems can be lost. To improve the interpretability of neuro-fuzzy systems, certain measures must be taken, wherein important aspects of interpretability of neuro-fuzzy systems are also discussed. A recent research line addresses the data stream mining case, where neuro-fuzzy systems are sequentially updated with new incoming samples on demand and on-the-fly. Thereby, system updates do not only include a recursive adaptation of model parameters, but also a dynamic evolution and pruning of model components (neurons, rules), in order to handle concept drift and dynamically changing system behavior adequately and to keep the systems/models ""up-to-date"" anytime. Comprehensive surveys of various evolving neuro-fuzzy systems approaches can be found in  and."
"http://dbpedia.org/resource/OpenAIR"	"OpenAIR"	"Artificial intelligence"	"OpenAIR is a message routing and communication protocol for artificial intelligence systems that has been gaining in popularity in recent years (2006). The protocol is managed by Mindmakers, and is described on their site in the following manner: ""OpenAIR is a routing and communication protocol based on a publish-subscribe architecture. It is intended to be the ""glue"" that allows numerous A.I. researchers to share code more effectively — ""AIR to share"". It is a definition or a blueprint of the ""post office and mail delivery system"" for distributed, multi-module systems. OpenAIR provides a core foundation upon which subsequent markup languages and semantics can be based, for e.g. gesture recognition and generation, computer vision, hardware-software interfacing etc.; for a recent example see CVML."" OpenAIR was created to allow software components that serve their own purpose to communicate with each other in order to produce large scale, overall behavior of an intelligent system. A simple example would be to have a speech recognition system, and a speech synthesizer communicate with an expert system through OpenAIR messages, to create a system that can hear and answer various questions through spoken dialogue."
"http://dbpedia.org/resource/Fuzzy_agent"	"Fuzzy agent"	"Artificial intelligence"	"In computer science a fuzzy agent is a software agent that implements fuzzy logic. This software entity interacts with its environment through an adaptive rule-base and can therefore be considered as a type of intelligent agent."
"http://dbpedia.org/resource/Noogenesis"	"Noogenesis"	"Artificial intelligence"	"Noogenesis (Ancient Greek: νοῦς=mind + γένεσις = origin, becoming) is the emergence and evolution of intelligence."
"http://dbpedia.org/resource/Belief–desire–intention_model"	"Belief–desire–intention model"	"Artificial intelligence"	"The belief–desire–intention (BDI) model of human practical reasoning was developed by Michael Bratman as a way of explaining future-directed intention. BDI is fundamentally reliant on folk psychology (the 'theory theory'), which is the notion that our mental models of the world are theories. It was used as a basis for developing the belief–desire–intention software model."
"http://dbpedia.org/resource/Rough_fuzzy_hybridization"	"Rough fuzzy hybridization"	"Artificial intelligence"	"Rough fuzzy hybridization is a method of hybrid intelligent system or soft computing, where Fuzzy set theory is used for linguistic representation of patterns, leading to a fuzzy granulation of the feature space. Rough set theory is used to obtain dependency rules which model informative regions in the granulated feature space."
"http://dbpedia.org/resource/3D_reconstruction_from_multiple_images"	"3D reconstruction from multiple images"	"Artificial intelligence"	"3D reconstruction from multiple images is the creation of three-dimensional models from a set of images. It is the reverse process of obtaining 2D images from 3D scenes. The essence of an image is a projection from a 3D scene onto a 2D plane, during which process the depth is lost. The 3D point corresponding to a specific image point is constrained to be on the line of sight. From a single image, it is impossible to determine which point on this line corresponds to the image point. If two images are available, then the position of a 3D point can be found as the intersection of the two projection rays. This process is referred to as triangulation. The key for this process is the relations between multiple views which convey the information that corresponding sets of points must contain some structure and that this structure is related to the poses and the calibration of the camera. In recent decades, there is an important demand for 3D content for computer graphics, virtual reality and communication, triggering a change in emphasis for the requirements. Many existing systems for constructing 3D models are built around specialized hardware (e.g. stereo rigs) resulting in a high cost, which cannot satisfy the requirement of its new applications. This gap stimulates the use of digital imaging facilities (like a camera). Moore's law also tells us that more work can be done in software. An early method was proposed by Tomasi and Kanade. They used an affine factorization approach to extract 3D from images sequences. However, the assumption of orthographic projection is a significant limitation of this system."
"http://dbpedia.org/resource/MNIST_database"	"MNIST database"	"Artificial intelligence"	"The MNIST database (Mixed National Institute of Standards and Technology database) is a large database of handwritten digits that is commonly used for training various image processing systems. The database is also widely used for training and testing in the field of machine learning. It was created by ""re-mixing"" the samples from NIST's original datasets. The creators felt that since NIST's training dataset was taken from American Census Bureau employees, while the testing dataset was taken from American high school students, it was not well-suited for machine learning experiments. Furthermore, the black and white images from NIST were normalized to fit into a 20x20 pixel bounding box and anti-aliased, which introduced grayscale levels. The MNIST database contains 60,000 training images and 10,000 testing images. Half of the training set and half of the test set were taken from NIST's training dataset, while the other half of the training set and the other half of the test set were taken from NIST's testing dataset.There have been a number of scientific papers on attempts to achieve the lowest error rate; one paper, using a hierarchical system of convolutional neural networks, manages to get an error rate on the MNIST database of 0.23 percent. The original creators of the database keep a list of some of the methods tested on it. In their original paper, they use a support vector machine to get an error rate of 0.8 percent."
"http://dbpedia.org/resource/Angel_F"	"Angel F"	"Artificial intelligence"	"Angel_F is a fictional child artificial intelligence that has been used in worldwide art performances focused on the issues of digital liberties, intellectual property and on the evolution of language and behaviour in information society. The character was created by Salvatore Iaconesi in 2007 as a hack to the Biodoll art performance by Italian artist Franca Formenti. The project revolves around the story of the technological sensual relationship between the Biodoll, a digital prostitute, and professor Derrick de Kerckhove, culminating in the birth of the young artificial intelligence. The digital child morphed from a spyware program capturing internet users' browsing activities in order to gather text to feed an algorithmic language generating system (thus letting the ""child"" learn how to speak) and became a digital personality used by the artists for both online and real-world performances. The project was later joined by Oriana Persico who curated communication and part of the theoretical approaches of the action. The Angel_F project has been featured in books, magazines, national televisions, and has been invited to many worldwide conferences and events, both academic and artistic. On one occasion, in October 2007, Angel_F attended the Internet Governance Forum (IGF) in Rio de Janeiro as the only digital being invited to speak: the young artificial intelligence was featured with a video message concerning the struggles for digital liberties, privacy, knowledge sharing and access to networks and technologies."
"http://dbpedia.org/resource/Computational_humor"	"Computational humor"	"Artificial intelligence"	"Computational humor is a branch of computational linguistics and artificial intelligence which uses computers in humor research. It is not to be confused with computer humor (i.e., jokes about computers, programmers, users, and computing). It is a relatively new area, with the first dedicated conference organized in 1996."
"http://dbpedia.org/resource/AI-complete"	"AI-complete"	"Artificial intelligence"	"In the field of artificial intelligence, the most difficult problems are informally known as AI-complete or AI-hard, implying that the difficulty of these computational problems is equivalent to that of solving the central artificial intelligence problem—making computers as intelligent as people, or strong AI. To call a problem AI-complete reflects an attitude that it would not be solved by a simple specific algorithm. AI-complete problems are hypothesised to include computer vision, natural language understanding, and dealing with unexpected circumstances while solving any real world problem. Currently, AI-complete problems cannot be solved with modern computer technology alone, but would also require human computation. This property can be useful, for instance to test for the presence of humans as with CAPTCHAs, and for computer security to circumvent brute-force attacks."
"http://dbpedia.org/resource/Artificial_intelligence"	"Artificial intelligence"	"Artificial intelligence"	"Artificial intelligence (AI) is intelligence exhibited by machines. In computer science, an ideal ""intelligent"" machine is a flexible rational agent that perceives its environment and takes actions that maximize its chance of success at some goal. Colloquially, the term ""artificial intelligence"" is applied when a machine mimics ""cognitive"" functions that humans associate with other human minds, such as ""learning"" and ""problem solving"". As machines become increasingly capable, facilities once thought to require intelligence are removed from the definition. For example, optical character recognition is no longer perceived as an exemplar of ""artificial intelligence"" having become a routine technology. Capabilities currently classified as AI include successfully understanding human speech, competing at a high level in strategic game systems (such as Chess and Go), self-driving cars, and interpreting complex data. AI research is divided into subfields that focus on specific  or on specific  or on the use of a particular  or towards satisfying particular . The central problems (or goals) of AI research include reasoning, knowledge, planning, learning, natural language processing (communication), perception and the ability to move and manipulate objects. General intelligence is among the field's long-term goals. Approaches include , , soft computing (e.g. machine learning), and . Many tools are used in AI, including versions of , , . The AI field draws upon computer science, mathematics, psychology, linguistics, philosophy, neuroscience and artificial psychology. The field was founded on the claim that human intelligence ""can be so precisely described that a machine can be made to simulate it."" This raises philosophical arguments about the nature of the mind and the ethics of creating artificial beings endowed with human-like intelligence, issues which have been explored by myth, fiction and philosophy since antiquity. Attempts to create artificial intelligence have experienced many setbacks, including the ALPAC report of 1966, the abandonment of perceptrons in 1970, the Lighthill Report of 1973 and the collapse of the Lisp machine market in 1987. In the twenty-first century AI techniques became an essential part of the technology industry, helping to solve many challenging problems in computer science."
"http://dbpedia.org/resource/Computer_vision"	"Computer vision"	"Artificial intelligence"	"Computer vision is an interdisciplinary field that deals with how computers can be made to gain high-level understanding from digital images or videos. From the perspective of engineering, it seeks to automate tasks that the human visual system can do. Computer vision tasks include methods for acquiring, processing, analyzing and understanding digital images, and in general, deal with the extraction of high-dimensional data from the real world in order to produce numerical or symbolic information, e.g., in the forms of decisions. Understanding in this context means the transformation of visual images (the input of the retina) into descriptions of the world that can interface with other thought processes and elicit appropriate action. This image understanding can be seen as the disentangling of symbolic information from image data using models constructed with the aid of geometry, physics, statistics, and learning theory. As a scientific discipline, computer vision is concerned with the theory behind artificial systems that extract information from images. The image data can take many forms, such as video sequences, views from multiple cameras, or multi-dimensional data from a medical scanner. As a technological discipline, computer vision seeks to apply its theories and models for the construction of computer vision systems. Sub-domains of computer vision include scene reconstruction, event detection, video tracking, object recognition, object pose estimation, learning, indexing, motion estimation, and image restoration."
"http://dbpedia.org/resource/Expert_system"	"Expert system"	"Artificial intelligence"	"In artificial intelligence, an expert system is a computer system that emulates the decision-making ability of a human expert.Expert systems are designed to solve complex problems by reasoning about knowledge, represented primarily as if–then rules rather than through conventional procedural code. The first expert systems were created in the 1970s and then proliferated in the 1980s. Expert systems were among the first truly successful forms of AI software. An expert system is divided into two sub-systems: the inference engine and the knowledge base. The knowledge base represents facts and rules. The inference engine applies the rules to the known facts to deduce new facts. Inference engines can also include explanation and debugging capabilities."
"http://dbpedia.org/resource/Frame_problem"	"Frame problem"	"Artificial intelligence"	"In artificial intelligence, the frame problem describes an issue with using first-order logic (FOL) to express facts about a robot in the world. Representing the state of a robot with traditional FOL requires the use of many axioms that simply imply that things in the environment do not change arbitrarily. For example, Hayes describes a ""block world"" with rules about stacking blocks together. In a FOL system, additional axioms are required to make inferences about the environment (for example, that a block cannot change position unless it's physically moved). The frame problem is the problem of finding adequate collections of axioms for a viable description of a robot environment. John McCarthy and Patrick J. Hayes defined this problem in their 1969 article, Some Philosophical Problems from the Standpoint of Artificial Intelligence. In this paper and many that came after the formal mathematical problem was a starting point for more general discussions of the difficulty of knowledge representation for artificial intelligence. Issues such as how to provide rational default assumptions and what humans consider common sense in a virtual environment. Later, the term acquired a broader meaning in philosophy, where it is formulated as the problem of limiting the beliefs that have to be updated in response to actions. In the logical context, actions are typically specified by what they change, with the implicit assumption that everything else (the frame) remains unchanged."
"http://dbpedia.org/resource/Game_theory"	"Game theory"	"Artificial intelligence"	"Game theory is ""the study of mathematical models of conflict and cooperation between intelligent rational decision-makers."" Game theory is mainly used in economics, political science, and psychology, as well as logic, computer science, biology and poker. Originally, it addressed zero-sum games, in which one person's gains result in losses for the other participants. Today, game theory applies to a wide range of behavioral relations, and is now an umbrella term for the science of logical decision making in humans, animals, and computers. Modern game theory began with the idea regarding the existence of mixed-strategy equilibria in two-person zero-sum games and its proof by John von Neumann. Von Neumann's original proof used Brouwer fixed-point theorem on continuous mappings into compact convex sets, which became a standard method in game theory and mathematical economics. His paper was followed by the 1944 book Theory of Games and Economic Behavior, co-written with Oskar Morgenstern, which considered cooperative games of several players. The second edition of this book provided an axiomatic theory of expected utility, which allowed mathematical statisticians and economists to treat decision-making under uncertainty. This theory was developed extensively in the 1950s by many scholars. Game theory was later explicitly applied to biology in the 1970s, although similar developments go back at least as far as the 1930s. Game theory has been widely recognized as an important tool in many fields. With the Nobel Memorial Prize in Economic Sciences going to game theorist Jean Tirole in 2014, eleven game-theorists have now won the economics Nobel Prize. John Maynard Smith was awarded the Crafoord Prize for his application of game theory to biology."
"http://dbpedia.org/resource/KL-ONE"	"KL-ONE"	"Artificial intelligence"	"KL-ONE (pronounced ""kay ell won"") is a well known knowledge representation system in the tradition of semantic networks and  frames; that is, it is a frame language. The system is an attempt to overcome semantic indistinctness in semantic network representations and to explicitly represent conceptual information as a structured inheritance network. There is a whole family of KL-ONE-like systems. One of the innovations that KL-ONE initiated was the use of a deductive classifier, an automated reasoning engine that can validate a frame ontology and deduce new information about the ontology based on the initial information provided by a domain expert. Frames in KL-ONE are called concepts. These form hierarchies using subsume-relations; in the KL-ONE terminology a super class is said to subsume its  subclasses. Multiple inheritance is allowed. Actually a concept is said to be well-formed only if it inherits from more than one other concept. All concepts, except the top concept (usually THING), must have at least one super class. In KL-ONE descriptions are separated into two basic classes of concepts: primitive and defined. Primitives are domain concepts that are not fully defined. This means that given all the properties of a concept, this is not sufficient to classify it. They may also be viewed as incomplete definitions. Using the same view, defined concepts are complete definitions. Given the properties of a concept, these are necessary and sufficient conditions to classify the concept. The slot-concept is called roles and the values of the roles are role-fillers. There are several different types of roles to be used in different situations. The most common and important role type is the generic RoleSet that captures the fact that the role may be filled with more than one filler."
"http://dbpedia.org/resource/List_of_artificial_intelligence_projects"	"List of artificial intelligence projects"	"Artificial intelligence"	"The following is a list of current and past, nonclassified notable artificial intelligence projects."
"http://dbpedia.org/resource/Embodied_cognitive_science"	"Embodied cognitive science"	"Artificial intelligence"	"Embodied Cognitive Science is an interdisciplinary field of research, the aim of which is to explain the mechanisms underlying intelligent behavior. It comprises three main methodologies: 1) the modeling of psychological and biological systems in a holistic manner that considers the mind and body as a single entity, 2) the formation of a common set of general principles of intelligent behavior, and 3) the experimental use of robotic agents in controlled environments. Embodied cognitive science borrows heavily from embodied philosophy and the related research fields of cognitive science, psychology, neuroscience and artificial intelligence. From the perspective of neuroscience, research in this field was led by Gerald Edelman of the Neurosciences Institute at La Jolla, the late Francisco Varela of CNRS in France, and J. A. Scott Kelso of Florida Atlantic University. From the perspective of psychology, research by Michael Turvey, Lawrence Barsalou and Eleanor Rosch. From the perspective of language acquisition, Eric Lenneberg and Philip Rubin at Haskins Laboratories. From the perspective of autonomous agent design, early work is sometimes attributed to Rodney Brooks or Valentino Braitenberg. From the perspective of artificial intelligence, see Understanding Intelligence by Rolf Pfeifer and Christian Scheier or How the body shapes the way we think, also by Rolf Pfeifer and Josh C. Bongard. From the perspective of philosophy see Andy Clark, Shaun Gallagher, and Evan Thompson. Turing proposed that a machine may need a human-like body to think and speak: It can also be maintained that it is best to provide the machine with the best sense organs that money can buy, and then teach it to understand and speak English. That process could follow the normal teaching of a child. Things would be pointed out and named, etc. Again, I do not know what the right answer is, but I think both approaches should be tried (Turing, 1950)."
"http://dbpedia.org/resource/Generalized_distributive_law"	"Generalized distributive law"	"Artificial intelligence"	"The generalized distributive law (GDL) is a general message passing algorithm devised by Srinivas M. Aji and Robert J. McEliece. It is a synthesis of the work of many authors in the information theory, digital communications, signal processing, statistics, and artificial intelligence communities. This article is based on a semi-tutorial by Srinivas M. Aji and Robert J. McEliece with the same title."
"http://dbpedia.org/resource/Information_extraction"	"Information extraction"	"Artificial intelligence"	"Information extraction (IE) is the task of automatically extracting structured information from unstructured and/or semi-structured machine-readable documents. In most of the cases this activity concerns processing human language texts by means of natural language processing (NLP). Recent activities in multimedia document processing like automatic annotation and content extraction out of images/audio/video could be seen as information extraction. Due to the difficulty of the problem, current approaches to IE focus on narrowly restricted domains. An example is the extraction from news wire reports of corporate mergers, such as denoted by the formal relation: , from an online news sentence such as: ""Yesterday, New York based Foo Inc. announced their acquisition of Bar Corp."" A broad goal of IE is to allow computation to be done on the previously unstructured data. A more specific goal is to allow logical reasoning to draw inferences based on the logical content of the input data. Structured data is semantically well-defined data from a chosen target domain, interpreted with respect to category and context. Information Extraction is the part of a greater puzzle which deals with the problem of devising automatic methods for text management, beyond its transmission, storage and display. The discipline of information retrieval (IR) has developed automatic methods, typically of a statistical flavor, for indexing large document collections and classifying documents. Another complementary approach is that of natural language processing (NLP) which has solved the problem of modelling human language processing with considerable success when taking into account the magnitude of the task. In terms of both difficulty and emphasis, IE deals with tasks in between both IR and NLP. In terms of input, IE assumes the existence of a set of documents in which each document follows a template, i.e. describes one or more entities or events in a manner that is similar to those in other documents but differing in the details. An example, consider a group of newswire articles on Latin American terrorism with each article is presumed to be based upon one or more terroristic acts. We also define for any given IE task a template, which is a(or a set of) case frame(s) to hold the information contained in a single document. For the terrorism example, a template would have slots corresponding to the perpetrator, victim, and weapon of the terroristic act, and the date on which the event happened. An IE system for this problem is required to “understand” an attack article only enough to find data corresponding to the slots in this template."
"http://dbpedia.org/resource/Rule-based_system"	"Rule-based system"	"Artificial intelligence"	"In computer science, rule-based systems are used as a way to store and manipulate knowledge to interpret information in a useful way. They are often used in artificial intelligence applications and research."
"http://dbpedia.org/resource/The_Leaf_(AI)_Project"	"The Leaf (AI) Project"	"Artificial intelligence"	"The Leaf Project is a group robot development program whose objective is to develop a robot platform that supports experiments with artificial intelligence, vision, navigation, etc. Leaf was created by Bruce Weimer, Alex Brown and Robin Hewitt. It is an artificial life program, inspired by Steve Grand's computer game Creatures, in which artificial beings hatch, develop, and interact in a simulated environment. A PC (software-only) version of Leaf was demonstrated in 2003, construction of the first robot began in early 2004."
"http://dbpedia.org/resource/Wetware_(brain)"	"Wetware (brain)"	"Artificial intelligence"	"Wetware is a term drawn from the computer-related idea of hardware or software, but applied to biological life forms. Here the prefix ""wet"" is a reference to the water found in living creatures. Wetware is used to describe the elements equivalent to hardware and software found in a person, namely the central nervous system (CNS) and the human mind. The term wetware finds use both in works of fiction and in scholarly publications. The ""hardware"" component of wetware concerns the bioelectric and biochemical properties of the CNS, specifically the brain. If the sequence of impulses traveling across the various neurons are thought of symbolically as software, then the physical neurons would be the hardware. The amalgamated interaction of this software and hardware is manifested through continuously changing physical connections, and chemical and electrical influences that spread across the body. The process by which the mind and brain interact to produce the collection of experiences that we define as self-awareness is still seriously in question."
"http://dbpedia.org/resource/Nouvelle_AI"	"Nouvelle AI"	"Artificial intelligence"	"Nouvelle artificial intelligence (AI) is an approach to artificial intelligence pioneered in the 1980s by Rodney Brooks, who was then part of MIT artificial intelligence laboratory. Nouvelle AI differs from classical AI by aiming to produce robots with intelligence levels similar to insects. Researchers believe that intelligence can emerge organically from simple behaviors as these intelligences interacted with the ""real world,"" instead of using the constructed worlds which symbolic AIs typically needed to have programmed into them."
"http://dbpedia.org/resource/Anytime_algorithm"	"Anytime algorithm"	"Artificial intelligence"	"In computer science, an anytime algorithm is an algorithm that can return a valid solution to a problem even if it's interrupted at any time before it ends. The algorithm is expected to find better and better solutions the more time it keeps running. Most algorithms run to completion: they provide a single answer after performing some fixed amount of computation. In some cases, however, the user may wish to terminate the algorithm prior to completion. The amount of the computation required may be substantial, for example, and computational resources might need to be reallocated. Most algorithms either run to completion or they provide no useful solution information. Anytime algorithms, however, are able to return a partial answer, whose quality depends on the amount of computation they were able to perform. The answer generated by anytime algorithms is an approximation of the correct answer."
"http://dbpedia.org/resource/Gabbay's_separation_theorem"	"Gabbay's separation theorem"	"Artificial intelligence"	"In mathematical logic and computer science, Gabbay's separation theorem, named after Dov Gabbay, states that any arbitrary temporal logic formula can be rewritten in a logically equivalent ""past → future"" form. I.e. the future becomes what must be satisfied. This form can be used as execution rules; a MetateM program is a set of such rules."
"http://dbpedia.org/resource/Recurrent_neural_network"	"Recurrent neural network"	"Artificial intelligence"	"A recurrent neural network (RNN) is a class of artificial neural network where connections between units form a directed cycle. This creates an internal state of the network which allows it to exhibit dynamic temporal behavior. Unlike feedforward neural networks, RNNs can use their internal memory to process arbitrary sequences of inputs. This makes them applicable to tasks such as unsegmented connected handwriting recognition or speech recognition."
"http://dbpedia.org/resource/Recursive_neural_network"	"Recursive neural network"	"Artificial intelligence"	"A recursive neural network (RNN) is a kind of deep neural network created by applying the same set of weights recursively over a structure, to produce a structured prediction over variable-length input, or a scalar prediction on it, by traversing a given structure in topological order. RNNs have been successful in learning sequence and tree structures in natural language processing, mainly phrase and sentence continuous representations based on word embedding. RNNs have first been introduced to learn distributed representations of structure, such as logical terms."
"http://dbpedia.org/resource/Self-management_(computer_science)"	"Self-management (computer science)"	"Artificial intelligence"	"Self-Management is the process by which computer systems shall manage their own operation without human intervention. Self-Management technologies are expected to pervade the next generation of network management systems. The growing complexity of modern networked computer systems is currently the biggest limiting factor in their expansion. The increasing heterogeneity of big corporate computer systems, the inclusion of mobile computing devices, and the combination of different networking technologies like WLAN, cellular phone networks, and mobile ad hoc networks make the conventional, manual management very difficult, time-consuming, and error-prone. More recently self-management has been suggested as a solution to increasing complexity in cloud computing. Currently, the most important industrial initiative towards realizing self-management is the Autonomic Computing Initiative (ACI) started by IBM in 2001. The ACI defines the following four functional areas: 
*  Self-Configuration: Automatic configuration of components; 
*  Self-Healing: Automatic discovery, and correction of faults; automatically applying all necessary actions to bring system back to normal operation 
*  Self-Optimization: Automatic monitoring and control of resources to ensure the optimal functioning with respect to the defined requirements; 
*  Self-Protection: Proactive identification and protection from arbitrary attacks. The design complexity of Autonomic Systems and self-management systems can be simplified by utilizing design patterns such as the Model View Controller (MVC) to improve concern separation by helping encapsulate functional concerns."
"http://dbpedia.org/resource/Diagnosis_(artificial_intelligence)"	"Diagnosis (artificial intelligence)"	"Artificial intelligence"	"As a subfield in artificial intelligence, Diagnosis is concerned with the development of algorithms and techniques that are able to determine whether the behaviour of a system is correct. If the system is not functioning correctly, the algorithm should be able to determine, as accurately as possible, which part of the system is failing, and which kind of fault it is facing. The computation is based on observations, which provide information on the current behaviour. The expression diagnosis also refers to the answer of the question of whether the system is malfunctioning or not, and to the process of computing the answer. This word comes from the medical context where a diagnosis is the process of identifying a disease by its symptoms."
"http://dbpedia.org/resource/Problem_solving"	"Problem solving"	"Artificial intelligence"	"Problem solving consists of using generic or ad hoc methods, in an orderly manner, for finding solutions to problems. Some of the problem-solving techniques developed and used in artificial intelligence, computer science, engineering, mathematics, or medicine are related to mental problem-solving techniques studied in psychology."
"http://dbpedia.org/resource/Data_pack"	"Data pack"	"Artificial intelligence"	"A data pack (or fact pack) is a pre-made database that can be fed to a software, such as software agents, Internet bots or chatterbots, to teach information and facts, which it can later look up. In other words, a data pack can be used to feed minor updates into a system. Common data packs may include abbreviations, acronyms, dictionaries, lexicons and technical data, such as country codes, RFCs, filename extensions, TCP and UDP port numbers, country calling codes, and so on. Data packs may come in formats of CSV and SQL that can easily be parsed or imported into a database management system. The database may consist of a key-value pair, like an association list. Data packs are commonly used within the gaming industry to provide minor updates within their games. When a user downloads an update for a game they will be downloading loads of data packs which will contain updates for the game such as minor bug fixes or additional content. An example of a data pack used to update a game can be found on the references."
"http://dbpedia.org/resource/Nervous_system_network_models"	"Nervous system network models"	"Artificial intelligence"	"Network of human nervous system comprises nodes (for example, neurons) that are connected by links (for example, synapses). The connectivity may be viewed anatomically, functionally, or electrophysiologically. These are presented in several Wikipedia articles that include Connectionism (a.k.a. Parallel Distributed Processing (PDP)), Biological neural network, Artificial neural network (a.k.a. Neural network), Computational neuroscience, as well as in several books by Ascoli, G. A. (2002), Sterratt, D., Graham, B., Gillies, A., & Willshaw, D. (2011), Gerstner, W., & Kistler, W. (2002), and Rumelhart, J. L., McClelland, J. L., and PDP Research Group (1986) among others. The focus of this article is a comprehensive view of modeling a neural network (technically neuronal network based on neuron model). Once an approach based on the perspective and connectivity is chosen, the models are developed at microscopic (ion and neuron), mesoscopic (functional or population), or macroscopic (system) levels. Computational modeling refers to models that are developed using computing tools."
"http://dbpedia.org/resource/Thompson_sampling"	"Thompson sampling"	"Artificial intelligence"	"In artificial intelligence, Thompson sampling, named after William R. Thompson, is a heuristic for choosing actions that addresses the exploration-exploitation dilemma in the multi-armed bandit problem. It consists in choosing the action that maximizes the expected reward with respect to a randomly drawn belief."
"http://dbpedia.org/resource/0music"	"0music"	"Artificial intelligence"	"0music is the second album produced with Melomics technology. While the first one (Iamus' album) is a compilation of contemporary pieces fully composed by Iamus, 0music compiles pieces of popular genres, composed and interpreted without any human intervention by Melomics109, a computer cluster hosted at the University of Malaga. The pieces in this album, and all the production of Melomics109, is distributed under CC0 licensing, and it is available in audible and editable (MIDI) formats. The album was launched during a one-day symposium held in Malaga on July 21, 2014."
"http://dbpedia.org/resource/Instrumental_convergence"	"Instrumental convergence"	"Artificial intelligence"	"Instrumental convergence is the hypothetical tendency for most sufficiently intelligent agents to pursue certain instrumental goals such as self-preservation and resource acquisition. Instrumental convergence suggests that an intelligent agent with apparently harmless goals can act in surprisingly harmful ways. For example, a computer with the sole goal of solving the Riemann hypothesis could attempt to turn the entire Earth into computronium in an effort to increase its computing power so that it can succeed in its calculations."
"http://dbpedia.org/resource/Superintelligence:_Paths,_Dangers,_Strategies"	"Superintelligence: Paths, Dangers, Strategies"	"Artificial intelligence"	"Superintelligence: Paths, Dangers, Strategies is a 2014 book by the Swedish philosopher Nick Bostrom from the University of Oxford. It argues that if machine brains surpass human brains in general intelligence, then this new superintelligence could replace humans as the dominant lifeform on Earth. Sufficiently intelligent machines could improve their own capabilities faster than human computer scientists. As the fate of gorillas now depends more on humans than on the actions of gorillas themselves, so will the fate of future humanity depend on the actions of the machine superintelligence. The outcome could be an existential catastrophe for humans. Bostrom's book has been translated into many languages and is available as an audiobook."
"http://dbpedia.org/resource/ACROSS_Project"	"ACROSS Project"	"Artificial intelligence"	"ACROSS is a Singular Strategic R&D Project led by Treelogic funded by the Spanish Ministry of Industry, Tourism and Trade activities in the field of Robotics and Cognitive Computing over an execution time-frame from 2009 to 2011. ACROSS project involves a number higher than 100 researchers from 13 Spanish entities."
"http://dbpedia.org/resource/BabelNet"	"BabelNet"	"Artificial intelligence"	"BabelNet is a multilingual lexicalized semantic network and ontology developed at the Linguistic Computing Laboratory in the Department of Computer Science of the Sapienza University of Rome. BabelNet was automatically created by linking the largest multilingual Web encyclopedia, Wikipedia, to the most popular computational lexicon of the English language, WordNet. The integration is performed by means of an automatic mapping and by filling in lexical gaps in resource-poor languages with the aid of statistical machine translation. The result is an ""encyclopedic dictionary""that provides concepts and named entities lexicalized in many languages and connected with large amounts of semantic relations. Additional lexicalizations and definitions are added by linking to free-license wordnets, OmegaWiki, the English Wiktionary, Wikidata, FrameNet, VerbNet and others. Similarly to WordNet, BabelNet groups words in different languages into sets of synonyms, called Babel synsets. For each Babel synset, BabelNet provides short definitions (called glosses) in many languages harvested from both WordNet and Wikipedia."
"http://dbpedia.org/resource/IRCF360"	"IRCF360"	"Artificial intelligence"	"Infrared Control Freak 360 (IRCF360) is a 360 degree proximity sensor and a motion sensing devices, developed by ROBOTmaker. The sensor is in BETA developers release as a low cost (software configurable) sensor for use within research, technical and hobby projects such as Maker Faire type projects, Microbotics, Kinetic art / art, crafts, engineering, UAV, Science, Technology and alternative music type of projects."
"http://dbpedia.org/resource/Maluuba"	"Maluuba"	"Artificial intelligence"	"Maluuba is a Canadian startup conducting research in deep and reinforcement learning to solve the problem of machine literacy through common sense reasoning, memory and communication. This technology will allow machines to understand and answer questions about written documents, and have natural conversations with users. In late March 2016, the company made headlines by demonstrating a machine reading system capable of answering arbitrary questions about J.K Rowling’s Harry Potter and the Philosopher’s Stone. Their natural language understanding technology has been adopted by major consumer electronic brands like LG and can be found on over 50 million devices shipping globally in the smart phone, smart TV and IoT space."
"http://dbpedia.org/resource/Sensorium_Project"	"Sensorium Project"	"Artificial intelligence"	"The Sensorium Project was initiated by ROBOTmaker to broaden the IRCF360 sensor's audience beyond its typical robot sensor usage. To demonstrate the BETA functionality, ROBOTmaker uses opensource Java based Integrated Development Environments (IDE) such as Arduino and Processing (programming language) and has released all the prototyping sketches on their website for advanced developers to expand the functionality and write their own Java interface apps. They also provide a low cost self-assembly PCB kit to allow developers to make a sensor and to reprogramme the embedded micro-controller with own firmware if desired. The sensor is based on ROBOTmaker's Infrared Control Freak 360 (IRCF360) which is a 360 degree proximity sensor and a motion sensing devices, developed by ROBOTmaker. Their goal is to provide low costs configurable sensors & solutions for use within technical projects such as maker and Maker Faire type projects, micro robotic, kinetic art / art, crafts, engineering, science, Do-It-Yourself (DIY) mindset and alternative music type of projects."
"http://dbpedia.org/resource/Speaktoit"	"Speaktoit"	"Artificial intelligence"	"Speaktoit is a developer of human–computer interaction technologies based on natural language conversations. The company is best known for creating the Assistant (by Speaktoit), a virtual buddy for Android, iOS, and Windows Phone smartphones that performs tasks and answers users' question in a natural language. Speaktoit has also created a natural language processing engine that incorporates conversation context like dialogue history, location and user preferences. In May 2012, Speaktoit received a venture round (funding terms undisclosed) from Intel Capital. In July 2014, Speaktoit closed their Series B funding led by Motorola Solutions Venture Capital with participation from new investor Plug and Play Ventures and existing backers Intel Capital and Alpine Technology Fund."
"http://dbpedia.org/resource/Epistemic_modal_logic"	"Epistemic modal logic"	"Artificial intelligence"	"Epistemic modal logic is a subfield of modal logic that is concerned with reasoning about knowledge. While epistemology has a long philosophical tradition dating back to Ancient Greece, epistemic logic is a much more recent development with applications in many fields, including philosophy, theoretical computer science, artificial intelligence, economics and linguistics. While philosophers since Aristotle have discussed modal logic, and Medieval philosophers such as Ockham and Duns Scotus developed many of their observations, it was C. I. Lewis who created the first symbolic and systematic approach to the topic, in 1912. It continued to mature as a field, reaching its modern form in 1963 with the work of Kripke."
"http://dbpedia.org/resource/And–or_tree"	"And–or tree"	"Artificial intelligence"	"An and–or tree is a graphical representation of the reduction of problems (or goals) to conjunctions and disjunctions of subproblems (or subgoals)."
"http://dbpedia.org/resource/Pedagogical_agent"	"Pedagogical agent"	"Artificial intelligence"	"A pedagogical agent is a concept borrowed from Computer Science and Artificial Intelligence and applied to education, usually as part of an Intelligent Tutoring System (ITS). It is a simulated human-like interface between the learner and the content, in an educational environment. A pedagogical agent is designed to model the type of interactions between a student and another person. Mabanza and de Wet define it as “as a character enacted by a computer that interacts with the user in a socially engaging manner”. A pedagogical agent can be assigned different roles in the learning environment, such as tutor or co-learner, depending on the desired purpose of the agent. “A tutor agent plays the role of a teacher, while a co-learner agent plays the role of a learning companion""."
"http://dbpedia.org/resource/Hybrid_neural_network"	"Hybrid neural network"	"Artificial intelligence"	"The term hybrid neural network can have two meanings: 1.  
* biological neural networks interacting with artificial neuronal models, and 2.  
* Artificial neural networks with a symbolic part (or, conversely, symbolic computations with a connectionist part). As for the first meaning, the artificial neurons and synapses in hybrid networks can be digital or analog. For the digital variant voltage clamps are used to monitor the membrane potential of neurons, to computationally simulate artificial neurons and synapses and to stimulate biological neurons by inducing synaptic. For the analog variant, specially designed electronic circuits connect to a network of living neurons through electrodes. As for the second meaning, incorporating elements of symbolic computation and artificial neural networks into one model was an attempt to combine the advantages of both paradigms while avoid the shortcomings. Symbolic representations have advantages with respect to explicit, direct control, fast initial coding, dynamic variable binding and knowledge abstraction. Representations of artificial neural networks, on the other hand, show advantages for biological plausibility, learning, robustness (fault-tolerant processing and graceful decay), and generalization to similar input. Since the early 1990s many attempts have been made to reconcile the two approaches."
"http://dbpedia.org/resource/Our_Final_Invention"	"Our Final Invention"	"Artificial intelligence"	"Our Final Invention: Artificial Intelligence and the End of the Human Era is a 2013 non-fiction book by the American author James Barrat. The book discusses the potential benefits and possible risks of human-level or super-human artificial intelligence. Those purported risks include extermination of the human race."
"http://dbpedia.org/resource/Syman"	"Syman"	"Artificial intelligence"	"SYMAN is an artificial intelligence technology that uses data from social media profiles to identify trends in the job market. SYMAN is designed to organize actionable data for products and services including recruiting, human capital management, CRM, and marketing. SYMAN was developed with a $21 million series B financing round secured by Identified, which was led by VantagePoint Capital Partners and Capricorn Investment Group."
"http://dbpedia.org/resource/Voice_Mate"	"Voice Mate"	"Artificial intelligence"	"Voice Mate formerly called Quick Voice and later on as Q Voice is an intelligent personal assistant and knowledge navigator which is only available as a built-in application for the LG Optimus, G and Vu Optimus Vu, Optimus LTE II, L7, L9 and L9 II, Optimus F3, Optimus F5, Optimus F6, Optimus F7, Optimus G, Optimus G Pro, G2, G Pad 8.3, and Vu 3 and as an update to other Optimus devices. The application uses a natural language user interface to answer questions, make recommendations, and perform actions by delegating requests to a set of Web services. It is based on the Maluuba personal assistant. Some of the capabilities of Voice Mate include making appointments, opening apps, setting alarms, updating social network websites such as Facebook or Twitter and navigation. Voice Mate also offers efficient multitasking as well as automatic activation features, for example when the car engine is started."
"http://dbpedia.org/resource/Means-ends_analysis"	"Means-ends analysis"	"Artificial intelligence"	"Means-Ends Analysis (MEA) is a problem solving technique used commonly in Artificial Intelligence (AI) for limiting search in AI programs. It is also a technique used at least since the 1950s as a creativity tool, most frequently mentioned in engineering books on design methods. MEA is also related to Means-Ends Chain Approach used commonly in consumer behavior analysis. It is also a way to clarify one's thoughts when embarking on a mathematical proof."
"http://dbpedia.org/resource/SNePS"	"SNePS"	"Artificial intelligence"	"SNePS is a knowledge representation, reasoning, and acting (KRRA) system developed and maintained by Stuart C. Shapiro and colleagues at the State University of New York at Buffalo. SNePS is simultaneously a logic-based, frame-based, and network-based KRRA system. It uses an assertional model of knowledge, in that a SNePS knowledge base (KB) consists of a set of assertions (propositions) about various entities. Its intended model is of an intensional domain of mental entities---the entities conceived of by some agent, and the propositions believed by it. The intensionality is primarily accomplished by the absence of a built-in equality operator, since any two syntactically different terms might have slightly different Fregean senses. SNePS has three styles of inference: formula-based, derived from its logic-based personality; slot-based, derived from its frame-based personality; and path-based, derived from its network-based personality. However, all three are integrated, operating together. SNePS may be used as a stand-alone KRR system. It has also been used, along with its integrated acting component, to implement the mind of intelligent agents (cognitive robots), in accord with the GLAIR agent architecture (a layered cognitive architecture). The SNePS Research Group often calls its agents Cassie."
"http://dbpedia.org/resource/Anticipation_(artificial_intelligence)"	"Anticipation (artificial intelligence)"	"Artificial intelligence"	"In artificial intelligence (AI), anticipation is the concept of an agent making decisions based on predictions, expectations, or beliefs about the future. It is widely considered that anticipation is a vital component of complex natural cognitive systems. As a branch of AI, anticipatory systems is a specialization still echoing the debates from the 1980s about the necessity for AI for an internal model."
"http://dbpedia.org/resource/Blackboard_system"	"Blackboard system"	"Artificial intelligence"	"A blackboard system is an artificial intelligence approach based on the blackboard architectural model, where a common knowledge base, the ""blackboard"", is iteratively updated by a diverse group of specialist knowledge sources, starting with a problem specification and ending with a solution. Each knowledge source updates the blackboard with a partial solution when its internal constraints match the blackboard state. In this way, the specialists work together to solve the problem. The blackboard model was originally designed as a way to handle complex, ill-defined problems, where the solution is the sum of its parts."
"http://dbpedia.org/resource/Cognitive_robotics"	"Cognitive robotics"	"Artificial intelligence"	"Cognitive robotics is concerned with endowing a robot with intelligent behavior by providing it with a processing architecture that will allow it to learn and reason about how to behave in response to complex goals in a complex world. Cognitive robotics may be considered the engineering branch of embodied cognitive science and embodied embedded cognition."
"http://dbpedia.org/resource/Collective_intelligence"	"Collective intelligence"	"Artificial intelligence"	"Collective intelligence is shared or group intelligence that emerges from the collaboration, collective efforts, and competition of many individuals and appears in consensus decision making. The term appears in sociobiology, political science and in context of mass peer review and crowdsourcing applications. It may involve consensus, social capital and formalisms such as voting systems, social media and other means of quantifying mass activity. Collective IQ is a measure of collective intelligence, although it is often used interchangeably with the term collective intelligence. Collective intelligence has also been attributed to bacteria and animals. It can be understood as an emergent property from the synergies among: 1) data-information-knowledge; 2) software-hardware; and 3) experts (those with new insights as well as recognized authorities) that continually learns from feedback to produce just-in-time knowledge for better decisions than these three elements acting alone. Or more narrowly as an emergent property between people and ways of processing information. This notion of collective intelligence is referred to as Symbiotic intelligence by Norman Lee Johnson. The concept is used in sociology, business, computer science and mass communications: it also appears in science fiction. Pierre Lévy defines collective intelligence as, ""It is a form of universally distributed intelligence, constantly enhanced, coordinated in real time, and resulting in the effective mobilization of skills. I'll add the following indispensable characteristic to this definition: The basis and goal of collective intelligence is mutual recognition and enrichment of individuals rather than the cult of fetishized or hypostatized communities."" According to researchers Lévy and Kerckhove, it refers to capacity of networked ICTs (Information communication technologies) to enhance the collective pool of social knowledge by simultaneously expanding the extent of human interactions. Collective intelligence strongly contributes to the shift of knowledge and power from the individual to the collective. According to Eric S. Raymond (1998) and JC Herz (2005), open source intelligence will eventually generate superior outcomes to knowledge generated by proprietary software developed within corporations (Flew 2008). Media theorist Henry Jenkins sees collective intelligence as an 'alternative source of media power', related to convergence culture. He draws attention to education and the way people are learning to participate in knowledge cultures outside formal learning settings. Henry Jenkins criticizes schools which promote 'autonomous problem solvers and self-contained learners' while remaining hostile to learning through the means of collective intelligence. Both Pierre Lévy (2007) and Henry Jenkins (2008) support the claim that collective intelligence is important for democratization, as it is interlinked with knowledge-based culture and sustained by collective idea sharing, and thus contributes to a better understanding of diverse society. Similar to the g factor for general individual intelligence, a new scientific understanding of collective intelligence aims to extract a general collective intelligence factor c factor for groups indicating a group's ability to perform a wide range of tasks. Definition, operationalization and statistical methods are derived from g. Similarly as g is highly interrelated with the concept of IQ, this measurement of collective intelligence can be interpreted as intelligence quotient for groups (Group-IQ) even though the score is not a quotient per se. Causes for c and predictive validity are investigated as well. Writers who have influenced the idea of collective intelligence include Douglas Hofstadter (1979), Peter Russell (1983), Tom Atlee (1993), Pierre Lévy (1994), Howard Bloom (1995), Francis Heylighen (1995), Douglas Engelbart, Cliff Joslyn, Ron Dembo, Gottfried Mayer-Kress (2003)."
"http://dbpedia.org/resource/Computational_intelligence"	"Computational intelligence"	"Artificial intelligence"	"The expression computational intelligence usually refers to the ability of a computer to learn a specific task from data or experimental observation. Even though it's commonly considered a synonym of soft computing, there is still no commonly accepted definition of computational intelligence. But generally, computational intelligence is a set of nature-inspired computational methodologies and approaches to address complex real-world problems to which mathematical or traditional modelling can be useless for a few reasons: the processes might be too complex for mathematical reasoning, it might contain some uncertainties during the process, or the process might simply be stochastic in nature. Indeed, many real-life problems cannot be translated into binary language (unique values of 0 and 1) for computers to process it. Computational Intelligence therefore provides solutions for such problems. The methods used are close to the human’s way of reasoning, i.e. it uses non exact and non-complete knowledge, and it is able to produce control actions in an adaptive way. CI therefore uses a combination of 5 main complementary techniques. The fuzzy logic which enables the computer to understand natural language, artificial neural networks which permits the system to learn experiential data by operating like the biological one, evolutionary computing which is based on the process of natural selection, learning theory, and probabilistic methods which helps dealing with uncertainty imprecision. Except those main principles, currently popular approaches include biologically inspired algorithms such as swarm intelligence and artificial immune systems, which can be seen as a part of evolutionary computation, image processing, data mining, natural language processing, and artificial intelligence,which tends to be confused with Computational Intelligence. But although both Computational Intelligence (CI) and Artificial Intelligence (AI) seek similar goals, there’s a clear distinction between them. Computational Intelligence is thus a way of performing like human beings. Indeed, the characteristic of ""intelligence"" is usually attributed to humans. More recently, many products and items also claim to be ""intelligent"", an attribute which is directly linked to the reasoning and decision making."
"http://dbpedia.org/resource/Description_logic"	"Description logic"	"Artificial intelligence"	"Description logics (DL) is a family of formal knowledge representation languages. Many DLs are more expressive than propositional logic but less expressive than first-order predicate logic. In contrast to the latter, the core reasoning problems for DLs are (usually) decidable, and efficient decision procedures have been designed and implemented for these problems. DLs are used in artificial intelligence to describe and reason about the relevant concepts of an application domain (known as terminological knowledge). It is of particular importance in providing a logical formalism for ontologies and the Semantic Web: the Web Ontology Language [OWL] and its profile is based on DLs. The most notable application of DLs and OWL is in biomedical informatics where DL assists in the codification of biomedical knowledge."
"http://dbpedia.org/resource/Distributed_artificial_intelligence"	"Distributed artificial intelligence"	"Artificial intelligence"	"Distributed Artificial Intelligence (DAI) is a subfield of artificial intelligence research dedicated to the development of distributed solutions for complex problems regarded as requiring intelligence. DAI is closely related to and a predecessor of the field of Multi-Agent Systems."
"http://dbpedia.org/resource/Ensemble_averaging_(machine_learning)"	"Ensemble averaging (machine learning)"	"Artificial intelligence"	"In machine learning, particularly in the creation of artificial neural networks, ensemble averaging is the process of creating multiple models and combining them to produce a desired output, as opposed to creating just one model. Frequently an ensemble of models performs better than any individual model, because the various errors of the models ""average out."""
"http://dbpedia.org/resource/Evolutionary_developmental_robotics"	"Evolutionary developmental robotics"	"Artificial intelligence"	"Evolutionary developmental robotics (evo-devo-robo for short) refers to methodologies that systematically integrate evolutionary robotics, epigenetic robotics and morphogenetic robotics to study the evolution, physical and mental development and learning of natural intelligent systems in robotic systems. The field was formally suggested and fully discussed in a published paper and further discussed in a published dialogue. The theoretical foundation of evo-devo-robo includes evolutionary developmental biology (evo-devo), evolutionary developmental psychology, developmental cognitive neuroscience etc. Further discussions on evolution, development and learning in robotics and design can be found in a number of papers, including papers on hardware systems and computing tissues."
"http://dbpedia.org/resource/Extremal_optimization"	"Extremal optimization"	"Artificial intelligence"	"Extremal optimization (EO) is an optimization heuristic inspired by the Bak–Sneppen model of self-organized criticality from the field of statistical physics. This heuristic was designed initially to address combinatorial optimization problems such as the travelling salesman problem and spin glasses, although the technique has been demonstrated to function in optimization domains."
"http://dbpedia.org/resource/Frame_language"	"Frame language"	"Artificial intelligence"	"A frame language is a technology used for knowledge representation in artificial intelligence. Frames are stored as ontologies of sets and subsets of the frame concepts. They are similar to class hierarchies in object-oriented languages although their fundamental design goals are different. Frames are focused on explicit and intuitive representation of knowledge whereas objects focus on encapsulation and information hiding. Frames originated in AI research and objects primarily in software engineering. However, in practice the techniques and capabilities of frame and object-oriented languages overlap significantly."
"http://dbpedia.org/resource/Hybrid_intelligent_system"	"Hybrid intelligent system"	"Artificial intelligence"	"Hybrid intelligent system denotes a software system which employs, in parallel, a combination of methods and techniques from artificial intelligence subfields as: 
*  Neuro-fuzzy systems 
*  hybrid connectionist-symbolic models 
*  Fuzzy expert systems 
*  Connectionist expert systems 
*  Evolutionary neural networks 
*  Genetic fuzzy systems 
*  Rough fuzzy hybridization 
*  Reinforcement learning with fuzzy, neural, or evolutionary methods as well as symbolic reasoning methods. From the cognitive science perspective, every natural intelligent system is hybrid because it performs mental operations on both the symbolic and subsymbolic levels. For the past few years there has been an increasing discussion of the importance of A.I. Systems Integration. Based on notions that there have already been created simple and specific AI systems (such as systems for computer vision, speech synthesis, etc., or software that employs some of the models mentioned above) and now is the time for integration to create broad AI systems. Proponents of this approach are researchers such as Marvin Minsky, Ron Sun, Aaron Sloman, and Michael A. Arbib. An example hybrid is a hierarchical control system in which the lowest, reactive layers are sub-symbolic. The higher layers, having relaxed time constraints, are capable of reasoning from an abstract world model and performing planning. Intelligent systems usually rely on hybrid reasoning systems, which include induction, deduction, abduction and reasoning by analogy"
"http://dbpedia.org/resource/Probabilistic_logic_network"	"Probabilistic logic network"	"Artificial intelligence"	"A probabilistic logic network (PLN) is a novel conceptual, mathematical and computational approach to uncertain inference; inspired by logic programming, but using probabilities in place of crisp (true/false) truth values, and fractional uncertainty in place of crisp known/unknown values. In order to carry out effective reasoning in real-world circumstances, artificial intelligence software must robustly handle uncertainty. However, previous approaches to uncertain inference do not have the breadth of scope required to provide an integrated treatment of the disparate forms of cognitively critical uncertainty as they manifest themselves within the various forms of pragmatic inference. Going beyond prior probabilistic approaches to uncertain inference, PLN is able to encompass within uncertain logic such ideas as induction, abduction, analogy, fuzziness and speculation, and reasoning about time and causality. PLN was developed by Ben Goertzel, Matt Ikle, Izabela Lyon Freire Goertzel and Ari Heljakka for use as a cognitive algorithm used by MindAgents within the OpenCog Core. PLN was developed originally for use within the Novamente Cognition Engine."
"http://dbpedia.org/resource/Progress_in_artificial_intelligence"	"Progress in artificial intelligence"	"Artificial intelligence"	"Artificial intelligence applications have been used in a wide range of fields including medical diagnosis, stock trading, robot control, law, scientific discovery and toys. However, many AI applications are not perceived as AI: ""A lot of cutting edge AI has filtered into general applications, often without being called AI because once something becomes useful enough and common enough it's not labeled AI anymore."" ""Many thousands of AI applications are deeply embedded in the infrastructure of every industry."" In the late 1990s and early 21st century, AI technology became widely used as elements of larger systems, but the field is rarely credited for these successes. To allow comparison with human performance, artificial intelligence can be evaluated on constrained and well-defined problems. Such tests have been termed subject matter expert Turing tests. Also, smaller problems provide more achievable goals and there are an ever-increasing number of positive results."
"http://dbpedia.org/resource/Radiant_AI"	"Radiant AI"	"Artificial intelligence"	"The Radiant AI is a technology developed by Bethesda Softworks for The Elder Scrolls video games. It allows non-player characters (NPCs) to make choices and engage in behaviors more complex than in past titles. The technology was developed for The Elder Scrolls IV: Oblivion and expanded in The Elder Scrolls V: Skyrim; it is also used in Fallout 3, Fallout: New Vegas and Fallout 4, also published by Bethesda."
"http://dbpedia.org/resource/Bio-inspired_computing"	"Bio-inspired computing"	"Artificial intelligence"	"Bio-inspired computing, short for biologically inspired computing, is a field of study that loosely knits together subfields related to the topics of connectionism, social behaviour and emergence. It is often closely related to the field of artificial intelligence, as many of its pursuits can be linked to machine learning. It relies heavily on the fields of biology, computer science and mathematics. Briefly put, it is the use of computers to model the living phenomena, and simultaneously the study of life to improve the usage of computers. Biologically inspired computing is a major subset of natural computation."
"http://dbpedia.org/resource/Symbolic_artificial_intelligence"	"Symbolic artificial intelligence"	"Artificial intelligence"	"Symbolic artificial intelligence is the collective name for all methods in artificial intelligence research that are based on high-level ""symbolic"" (human-readable) representations of problems, logic and search. Symbolic AI was the dominant paradigm of AI research from the mid-1950s until the late 1980s. John Haugeland gave the name GOFAI (""Good Old-Fashioned Artificial Intelligence"") to symbolic AI in his 1985 book Artificial Intelligence: The Very Idea, which explored the philosophical implications of artificial intelligence research. In robotics the analogous term is GOFR (""Good Old-Fashioned Robotics""). The approach is based on the assumption that many aspects of intelligence can be achieved by the manipulation of symbols, an assumption defined as the ""physical symbol systems hypothesis"" by Allen Newell and Herbert A. Simon in the middle 1960s: The most successful form of symbolic AI is expert systems, which use a network of production rules. Production rules connect symbols in a relationship similar to an If-Then statement. The expert system processes the rules to make deductions and to determine what additional information it needs, i.e. what questions to ask, using human-readable symbols. Opponents of the symbolic approach include roboticists such as Rodney Brooks, who aims to produce autonomous robots without symbolic representation (or with only minimal representation) and computational intelligence researchers, who apply techniques such as neural networks and optimization to solve problems in machine learning and control engineering. Symbolic AI was intended to produce general, human-like intelligence in a machine, whereas most modern research is directed at specific sub-problems. Research into general intelligence is now studied in the sub-field of artificial general intelligence. Machines were initially designed to formulate outputs based on the inputs that were represented by symbols. Symbols are used when the input is definite and falls under certainty. But when there is uncertainty involved, for example in formulating predictions, the representation is done using ""fuzzy logic"". This can be seen in artificial neural networks."
"http://dbpedia.org/resource/Algorithmic_probability"	"Algorithmic probability"	"Artificial intelligence"	"In algorithmic information theory, algorithmic (Solomonoff) probability is a mathematical method of assigning a prior probability to a given observation. In a theoretic sense, the prior is universal. It is used in inductive inference theory, and analyses of algorithms. Since it is not computable, it must be approximated."
"http://dbpedia.org/resource/User_illusion"	"User illusion"	"Artificial intelligence"	"The user illusion is the illusion created for the user by a human–computer interface, for example the visual metaphor of a desktop used in many graphical user interfaces. The phrase originated at Xerox PARC. Some philosophers of mind have argued that consciousness is a form of user illusion. This notion is explored by Tor Nørretranders in his 1991 Danish book Mærk verden, issued in a 1998 English edition as The User Illusion: Cutting Consciousness Down to Size. He introduced the notion of exformation in this book. According to this picture, our experience of the world is not immediate, as all sensation requires processing time. It follows that our conscious experience is less a perfect reflection of what is occurring, and more a simulation produced unconsciously by the brain. Therefore, there may be phenomena that exist beyond our peripheries, beyond what consciousness could create to isolate or reduce them."
"http://dbpedia.org/resource/Model-based_reasoning"	"Model-based reasoning"	"Artificial intelligence"	"In artificial intelligence, model-based reasoning refers to an inference method used in expert systems based on a model of the physical world. With this approach, the main focus of application development is developing the model. Then at run time, an ""engine"" combines this model knowledge with observed data to derive conclusions such as a diagnosis or a prediction."
"http://dbpedia.org/resource/Neurorobotics"	"Neurorobotics"	"Artificial intelligence"	"Neurorobotics, a combined study of neuroscience, robotics, and artificial intelligence, is the science and technology of embodied autonomous neural systems. Neural systems include brain-inspired algorithms (e.g. connectionist networks), computational models of biological neural networks (e.g. artificial spiking neural networks, large-scale simulations of neural microcircuits) and actual biological systems (e.g. in vivo and in vitro neural nets). Such neural systems can be embodied in machines with mechanic or any other forms of physical actuation. This includes robots, prosthetic or wearable systems but at also, at smaller scale, micro-machines and, at the larger scales, furniture and infrastructures. Neurorobotics is that branch of neuroscience with robotics, which deals with the study and application of science and technology of embodied autonomous neural systems like brain-inspired algorithms. At its core, neurorobotics is based on the idea that the brain is embodied and the body is embedded in the environment. Therefore, most neurorobots are required to function in the real world, as opposed to a simulated environment."
"http://dbpedia.org/resource/Psychology_of_reasoning"	"Psychology of reasoning"	"Artificial intelligence"	"The psychology of reasoning is the study of how people reason, often broadly defined as the process of drawing conclusions to inform how people solve problems and make decisions. It is at the intersection of psychology, philosophy, linguistics, cognitive science, artificial intelligence, logic, and probability theory. Psychological experiments on how humans and other animals reason have been carried out for over 100 years. An enduring question is whether or not people have the capacity to be rational. What does it mean to be rational? Current research in this area addresses various questions about reasoning, rationality, judgments, intelligence, relationships between emotion and reasoning, and development."
"http://dbpedia.org/resource/S_Voice"	"S Voice"	"Artificial intelligence"	"S Voice is an intelligent personal assistant and knowledge navigator which is only available as a built-in application for the Samsung Galaxy S III, S III Mini (including NFC Variant), S4, S4 Mini, S4 Active, S5, S5 Mini, S II Plus, Note II, Note 3, Note 4, Note 10.1, Note 8.0, Stellar, Mega, Grand, Avant, Core, Ace 3, Tab 3 7.0, Tab 3 8.0, Tab 3 10.1, Galaxy Camera, and other 2013 or later Samsung Android devices. The application uses a natural language user interface to answer questions, make recommendations, and perform actions by delegating requests to a set of Web services. It is based on the Vlingo personal assistant. The Galaxy S5 and later Samsung Android devices, S Voice runs on Nuance instead of Vlingo. Some of the capabilities of S Voice include making appointments, opening apps, setting alarms, updating social network websites such as Facebook or Twitter and navigation. S Voice also offers multitasking as well as automatic activation features, for example, when the car engine is started. In a disclaimer that pops up on first opening S Voice, Samsung states that the app is provided by a third party which it does not name."
"http://dbpedia.org/resource/Web_intelligence"	"Web intelligence"	"Artificial intelligence"	"Web intelligence is the area of scientific research and development that explores the roles and makes use of artificial intelligence and information technology for new products, services and frameworks that are empowered by the World Wide Web. The term was coined in a paper written by Ning Zhong, Jiming Liu Yao and Y.Y. Ohsuga in the Computer Software and Applications Conference in 2000."
"http://dbpedia.org/resource/Embodied_agent"	"Embodied agent"	"Artificial intelligence"	"In artificial intelligence, an embodied agent, also sometimes referred to as an interface agent, is an intelligent agent that interacts with the environment through a physical body within that environment. Agents that are represented graphically with a body, for example a human or a cartoon animal, are also called embodied agents, although they have only virtual, not physical, embodiment. A branch of artificial intelligence focuses on empowering such agents to interact autonomously with human beings and the environment. Mobile robots are one example of physically embodied agents; Ananova and Microsoft Agent are examples of graphically embodied agents. Embodied conversational agents are embodied agents (usually with a graphical front-end as opposed to a robotic body) that are capable of engaging in conversation with one another and with humans employing the same verbal and nonverbal means that humans do (such as gesture, facial expression, and so forth)."
"http://dbpedia.org/resource/NewsRx"	"NewsRx"	"Artificial intelligence"	"NewsRx is a media and technology company focusing on digital media, printed media, news services, and knowledge discovery through its BUTTER platform. In 1995 the company was the world’s largest producer of health news. The company publishes 194 newsweeklies in health and other fields, which are distributed to subscribers and partners including Factiva, the Wall Street Journal Professional Edition, Thomson Reuters, ProQuest, and Cengage Learning. C W Henderson founded the company in 1984 and its first publication was AIDS Weekly.  In the early 2000s, the firm added the imprint, VerticalNews to publish newsweeklies in non-health fields. Now based in Atlanta, Georgia, the company reports through its daily news service and publishes reference books through its partner, ScholarlyEditions. NewsRx launched its BUTTER platform in 2015, which is a knowledge discovery engine that delivers its content to academics, researchers, and professionals."
"http://dbpedia.org/resource/Autonomic_networking"	"Autonomic networking"	"Artificial intelligence"	"Autonomic Networking follows the concept of Autonomic Computing, an initiative started by IBM in 2001. Its ultimate aim is to create self-managing networks to overcome the rapidly growing complexity of the Internet and other networks and to enable their further growth, far beyond the size of today."
"http://dbpedia.org/resource/Language/action_perspective"	"Language/action perspective"	"Artificial intelligence"	"The language/action perspective (LAP) ""takes language as the primary dimension of human cooperative activity,"" applied not just in person-to-person direct (face-to-face) interactions, but also in the design of systems mediated by information and communication technology. The perspective was developed in the joint authorship of Understanding Computers and Cognition by Fernando Flores and Terry Winograd in 1987."
"http://dbpedia.org/resource/Fred_(chatterbot)"	"Fred (chatterbot)"	"Artificial intelligence"	"Fred, or FRED, was an early chatterbot written by Robby Garner."
"http://dbpedia.org/resource/Intelligent_database"	"Intelligent database"	"Artificial intelligence"	"Until the 1980s, databases were viewed as computer systems that stored record oriented and business type data such as manufacturing inventories, bank records, sales transactions, etc. A database system was not expected to merge numeric data with text, images, or multimedia information, nor was it expected to automatically notice patterns in the data it stored. In the late 1980s the concept of an intelligent database was put forward as a system that manages information (rather than data) in a way that appears natural to users and which goes beyond simple record keeping. The term intelligent database was introduced in 1989 by the book “Intelligent Databases” by Kamran Parsaye, Mark Chignell, Setrag Khoshafian and Harry Wong. This concept postulated three levels of intelligence for such systems: 1. high level tools, 2. the user interface and 3. the database engine. The high level tools manage data quality and automatically discover relevant patterns in the data with a process called data mining. This layer often relies on the use of artificial intelligence techniques. The user interface uses hypermedia in a form that uniformly manages text, images and numeric data. The intelligent database engine supports the other two layers, often merging relational database techniques with object orientation. In the twenty-first century, intelligent databases have now become widespread, e.g. hospital databases can now call up patient histories consisting of charts, text and x-ray images just with a few mouse clicks, and many corporate databases include decision support tools based on sales pattern analysis, etc."
"http://dbpedia.org/resource/Ontology_learning"	"Ontology learning"	"Artificial intelligence"	"Ontology learning (ontology extraction, ontology generation, or ontology acquisition) is the automatic or semi-automatic creation of ontologies, including extracting the corresponding domain's terms and the relationships between those concepts from a corpus of natural language text, and encoding them with an ontology language for easy retrieval. As building ontologies manually is extremely labor-intensive and time-consuming, there is great motivation to automate the process. Typically, the process starts by extracting terms and concepts or noun phrases from plain text using linguistic processors such as part-of-speech tagging and phrase chunking. Then statistical or symbolictechniques are used to extract relation signatures, often based on pattern-based or definition-based hypernym extraction techniques."
"http://dbpedia.org/resource/Personoid"	"Personoid"	"Artificial intelligence"	"Personoid is the concept coined by Stanislaw Lem (1971), a Polish science-fiction writer. His personoids are an abstraction of functions of human mind and they live in computers; they do not need any human-like physical body. In cognitive and software modeling, personoid is a research approach to the development of intelligent autonomous agents.In frame of the IPK (Information, Preferences, Knowledge) architecture, it is a framework of abstract intelligent agent with a cognitive and structural intelligence. It can be seen as an essence of high intelligent entities. From the philosophical and systemics perspectives, personoid societies can also be seen as the carriers of a culture. According to N. Gessler, the personoids study can be a base for the research on artificial culture and culture evolution."
"http://dbpedia.org/resource/A.I._Artificial_Intelligence"	"A.I. Artificial Intelligence"	"Artificial intelligence"	"A.I. Artificial Intelligence, also known as A.I., is a 2001 American science fiction drama film directed by Steven Spielberg. The screenplay by Spielberg was based on a screen story by Ian Watson and the 1969 short story Super-Toys Last All Summer Long by Brian Aldiss. The film was produced by Kathleen Kennedy, Spielberg and Bonnie Curtis. It stars Haley Joel Osment, Jude Law, Frances O'Connor, Brendan Gleeson and William Hurt. Set in a futuristic post-climate change society, A.I. tells the story of David (Osment), a childlike android uniquely programmed with the ability to love. Development of A.I. originally began with producer-director Stanley Kubrick in the early 1970s. Kubrick hired a series of writers until the mid-1990s, including Brian Aldiss, Bob Shaw, Ian Watson, and Sara Maitland. The film languished in protracted development for years, partly because Kubrick felt computer-generated imagery was not advanced enough to create the David character, whom he believed no child actor would convincingly portray. In 1995, Kubrick handed A.I. to Spielberg, but the film did not gain momentum until Kubrick's death in 1999. Spielberg remained close to Watson's film treatment for the screenplay. The film was greeted with generally positive reviews from critics, grossed approximately $235 million, and was nominated for two Academy Awards at the 74th Academy Awards for Best Visual Effects and Best Original Score (by John Williams). The film is dedicated to Stanley Kubrick."
"http://dbpedia.org/resource/Artificial_consciousness"	"Artificial consciousness"	"Artificial intelligence"	"Artificial consciousness (AC), also known as machine consciousness (MC) or synthetic consciousness (; ), is a field related to artificial intelligence and cognitive robotics. The aim of the theory of artificial consciousness is to ""define that which would have to be synthesized were consciousness to be found in an engineered artifact"" (). Neuroscience hypothesizes that consciousness is generated by the interoperation of various parts of the brain, called the neural correlates of consciousness or NCC, though there are challenges to that perspective. Proponents of AC believe it is possible to construct systems (e.g., computer systems) that can emulate this NCC interoperation. Artificial consciousness concepts are also pondered in the philosophy of artificial intelligence through questions about mind, consciousness, and mental states."
"http://dbpedia.org/resource/Aurora_(novel)"	"Aurora (novel)"	"Artificial intelligence"	"Aurora is a 2015 novel by American science fiction author Kim Stanley Robinson. The novel concerns a generation ship traveling to Tau Ceti in order to begin a human colony. The novel narrating voice is the starship computer's artificial intelligence. The novel was well-received by critics."
"http://dbpedia.org/resource/Automated_Mathematician"	"Automated Mathematician"	"Artificial intelligence"	"The Automated Mathematician (AM) is one of the earliest successful discovery systems. It was created by Douglas Lenat in Lisp, and in 1977 led to Lenat being awarded the IJCAI Computers and Thought Award. AM worked by generating and modifying short Lisp programs which were then interpreted as defining various mathematical concepts; for example, a program that tested equality between the length of two lists was considered to represent the concept of numerical equality, while a program that produced a list whose length was the product of the lengths of two other lists was interpreted as representing the concept of multiplication. The system had elaborate heuristics for choosing which programs to extend and modify, based on the experiences of working mathematicians in solving mathematical problems."
"http://dbpedia.org/resource/Intelligent_agent"	"Intelligent agent"	"Artificial intelligence"	"In artificial intelligence, an intelligent agent (IA) is an autonomous entity which observes through sensors and acts upon an environment using actuators (i.e. it is an agent) and directs its activity towards achieving goals (i.e. it is ""rational"", as defined in economics). Intelligent agents may also learn or use knowledge to achieve their goals. They may be very simple or very complex: a reflex machine such as a thermostat is an intelligent agent. Intelligent agents are often described schematically as an abstract functional system similar to a computer program. For this reason, intelligent agents are sometimes called abstract intelligent agents (AIA) to distinguish them from their real world implementations as computer systems, biological systems, or organizations. Some definitions of intelligent agents emphasize their autonomy, and so prefer the term autonomous intelligent agents. Still others (notably ) considered goal-directed behavior as the essence of intelligence and so prefer a term borrowed from economics, ""rational agent"". Intelligent agents in artificial intelligence are closely related to agents in economics, and versions of the intelligent agent paradigm are studied in cognitive science, ethics, the philosophy of practical reason, as well as in many interdisciplinary socio-cognitive modeling and computer social simulations. Intelligent agents are also closely related to software agents (an autonomous computer program that carries out tasks on behalf of users). In computer science, the term intelligent agent may be used to refer to a software agent that has some intelligence, regardless if it is not a rational agent by Russell and Norvig's definition. For example, autonomous programs used for operator assistance or data mining (sometimes referred to as bots) are also called ""intelligent agents""."
"http://dbpedia.org/resource/KAoS"	"KAoS"	"Artificial intelligence"	"KAoS is a policy and domain services framework created by the Florida Institute for Human and Machine Cognition. It uses W3C’s Web Ontology Language (OWL) standard for policy representation and reasoning, and a software guard technology for efficient enforcement of a compiled version of its policies. It has been used in a variety of government-sponsored projects for distributed host and network management and for the coordination of human-agent-robot teams, including DARPA's CoABS Grid, Cougaar, and Common Object Request Broker Architecture (CORBA) models."
"http://dbpedia.org/resource/Mind–body_problem"	"Mind–body problem"	"Artificial intelligence"	"The mind–body problem is the problem of explaining how mental states, events and processes—like beliefs, actions and thinking—are related to the physical states, events and processes, given that the human body is a physical entity and the mind is non-physical. The problem was addressed by René Descartes in the 17th century, resulting in Cartesian dualism, and by pre-Aristotelian philosophers, in Avicennian philosophy, and in earlier Asian traditions. A variety of approaches have been proposed. Most are either dualist or monist. Dualism maintains a rigid distinction between the realms of mind and matter. Monism maintains that there is only one unifying reality, substance or essence in terms of which everything can be explained. Each of these categories contain numerous variants. The two main forms of dualism are substance dualism, which holds that the mind is formed of a distinct type of substance not governed by the laws of physics, and property dualism, which holds that mental properties involving conscious experience are fundamental properties, alongside the fundamental properties identified by a completed physics. The three main forms of monism are physicalism, which holds that the mind consists of matter organized in a particular way; idealism, which holds that only thought truly exists and matter is merely an illusion; and neutral monism, which holds that both mind and matter are aspects of a distinct essence that is itself identical to neither of them. Several philosophical perspectives have been developed which reject the mind–body dichotomy. The historical materialism of Karl Marx and subsequent writers, itself a form of physicalism, held that consciousness was engendered by the material contingencies of one's environment. An explicit rejection of the dichotomy is found in French structuralism, and is a position that generally characterized post-war French philosophy. The absence of an empirically identifiable meeting point between the non-physical mind and its physical extension has proven problematic to dualism and many modern philosophers of mind maintain that the mind is not something separate from the body. These approaches have been particularly influential in the sciences, particularly in the fields of sociobiology, computer science, evolutionary psychology, and the neurosciences. An ancient model of the mind known as the Five-Aggregate Model explains the mind as continuously changing sense impressions and mental phenomena. Considering this model, it is possible to understand that it is the constantly changing sense impressions and mental phenomena (i.e., the mind) that experiences/analyzes all external phenomena in the world as well as all internal phenomena including the body anatomy, the nervous system as well as the organ brain. This conceptualization leads to two levels of analyses: (i) analyses conducted from a third-person perspective on how the brain works, and (ii) analyzing the moment-to-moment manifestation of an individual’s mind-stream (analyses conducted from a first-person perspective). Considering the latter, the manifestation of the mind-stream is described as happening in every person all the time, even in a scientist who analyses various phenomena in the world, including analyzing and hypothesizing about the organ brain."
"http://dbpedia.org/resource/OpenCog"	"OpenCog"	"Artificial intelligence"	"OpenCog is a project that aims to build an open source artificial intelligence framework. OpenCog Prime is a cognitive architecture for robot and virtual embodied cognition that defines a set of interacting components designed to give rise to human-equivalent artificial general intelligence (AGI) as an emergent phenomenon of the whole system. OpenCog Prime's design is primarily the work of Ben Goertzel while the OpenCog framework is intended as a generic framework for broad-based AGI research. Research utilizing OpenCog has been published in journals and presented at conferences and workshops including the annual Conference on Artificial General Intelligence. OpenCog is released under the terms of the GNU Affero General Public License."
"http://dbpedia.org/resource/Rational_agent"	"Rational agent"	"Artificial intelligence"	"In economics, game theory, decision theory, and artificial intelligence, a rational agent is an agent that has clear preferences, models uncertainty via expected values of variables or functions of variables, and always chooses to perform the action with the optimal expected outcome for itself from among all feasible actions. A rational agent can be anything that makes decisions, typically a person, firm, machine, or software. Rational agents are also studied in the fields of cognitive science, ethics, and philosophy, including the philosophy of practical reason."
"http://dbpedia.org/resource/Scilab_Image_Processing"	"Scilab Image Processing"	"Artificial intelligence"	"SIP is a toolbox for processing images in Scilab. SIP is meant to be a free, complete, and useful image toolbox for Scilab. Its goals include tasks such as filtering, blurring, edge detection, thresholding, histogram manipulation, segmentation, mathematical morphology, and color image processing. Though SIP is still in early development it can currently import and output image files in many formats including BMP, JPEG, GIF, PNG, TIFF, XPM, and PCX. SIP uses ImageMagick to accomplish this. SIP is licensed under the GPL."
"http://dbpedia.org/resource/Singleton_(global_governance)"	"Singleton (global governance)"	"Artificial intelligence"	"In futurology, a singleton is a hypothetical world order in which there is a single decision-making agency at the highest level, capable of exerting effective control over its domain, and permanently preventing both internal and external threats to its supremacy. The term has first been defined by Nick Bostrom. An artificial general intelligence having undergone an intelligence explosion could form a singleton, as could a world government armed with mind control and social surveillance technologies. A singleton need not directly micromanage everything in its domain; it could allow diverse forms of organization within itself, albeit guaranteed to function within strict parameters. A singleton need not support a civilization, and in fact could obliterate it upon coming to power. A singleton has both potential risks and potential benefits. Notably, a suitable singleton could solve world coordination problems that would not otherwise be solvable, opening up otherwise unavailable developmental trajectories for civilization. For example, Ben Goertzel, an AGI researcher, suggests humans may instead decide to create an ""AI Nanny"" with ""mildly superhuman intelligence and surveillance powers"", to protect the human race from existential risks like nanotechnology and to delay the development of other (unfriendly) artificial intelligences until and unless the safety issues are solved. Furthermore, Bostrom suggests that a singleton could hold Darwinian evolutionary pressures in check, preventing agents interested only in reproduction from coming to dominate. Yet Bostrom also regards the possibility of a stable, repressive, totalitarian global regime as a serious existential risk. The very stability of a singleton makes the installation of a bad singleton especially catastrophic, since the consequences can never be undone. Bryan Caplan writes that ""perhaps an eternity of totalitarianism would be worse than extinction"". Similarly Hans Morgenthau stressed that the mechanical development of weapons, transportation, and communication makes ""the conquest of the world technically possible, and they make it technically possible to keep the world in that conquered state"". Its lack was the reason why great ancient empires, though vast, failed to complete universal conquest of their world and perpetuate the conquest. Now, however, this is possible. Technology undoes both geographic and climatic barriers. ""Today no technological obstacle stands in the way of a world-wide empire"", as ""modern technology makes it possible to extend the control of mind and action to every corner of the globe regardless of geography and season."" Morgenthau continued on the technological progress: It has also given total war that terrifying, world-embracing impetus which seems to be satisfied with nothing less than world dominion… The machine age begets its own triumphs, each forward step calling forth two or more on the road of technological progress. It also begets its own victories, military and political; for with the ability to conquer the world and keep it conquered, it creates the will to conquer it."
"http://dbpedia.org/resource/Software_agent"	"Software agent"	"Artificial intelligence"	"In computer science, a software agent  is a computer program that acts for a user or other program in a relationship of agency, which derives from the Latin agere (to do): an agreement to act on one's behalf. Such ""action on behalf of"" implies the authority to decide which, if any, action is appropriate. Agents are colloquially known as bots, from robot. They may be embodied, as when execution is paired with a robot body, or as software such as a chatbotexecuting on a phone (e.g. Siri) or other computing device. Software Agents may be autonomous or work together with other agents or people. Software agents interacting with people (e.g. chatbots, human-robot interaction environments) may possess human-like qualities such as natural language understanding and speech, personality or embody humanoid form (see Asimo). Related and derived concepts include intelligent agents (in particular exhibiting some aspect of artificial intelligence, such as learning and reasoning), autonomous agents (capable of modifying the way in which they achieve their objectives), distributed agents (being executed on physically distinct computers), multi-agent systems (distributed agents that work together to achieve an objective that could not be accomplished by a single agent acting alone), and mobile agents (agents that can relocate their execution onto different processors)."
"http://dbpedia.org/resource/Structure_mapping_engine"	"Structure mapping engine"	"Artificial intelligence"	"In artificial intelligence and cognitive science, the structure mapping engine is an implementation in software of an algorithm for analogical matching based on the psychological theory of Dedre Gentner. The basis of Gentner's structure-mapping idea is that an analogy is a mapping of knowledge from one domain (the base) into another (the target). The structure-mapping engine, or SME, is a computer simulation of the analogy and similarity comparisons. As of 1990, more than 40 projects had used it [Falkenhainer, 2005]. R.M. French said that structure mapping theory is ""unquestionably the most influential work to date of the modeling of analogy-making"" [2002]. The theory is useful because it ignores surface features and finds matches between potentially very different things if they have the same representational structure. For example, SME could determine that a pen is like a sponge because both are involved in dispensing liquid, even though they do this very differently."
"http://dbpedia.org/resource/Backward_chaining"	"Backward chaining"	"Artificial intelligence"	"Backward chaining (or backward reasoning) is an inference method that can be described (in lay terms) as working backward from the goal(s). It is used in automated theorem provers, inference engines, proof assistants and other artificial intelligence applications. In game theory, its application to (simpler) subgames in order to find a solution to the game is called backward induction. In chess, it is called retrograde analysis, and it is used to generate tablebases for chess endgames for computer chess. Backward chaining is implemented in logic programming by SLD resolution. Both rules are based on the modus ponens inference rule. It is one of the two most commonly used methods of reasoning with inference rules and logical implications – the other is forward chaining. Backward chaining systems usually employ a depth-first search strategy, e.g. Prolog."
"http://dbpedia.org/resource/Computer-assisted_proof"	"Computer-assisted proof"	"Artificial intelligence"	"A computer-assisted proof is a mathematical proof that has been at least partially generated by computer. Most computer-aided proofs to date have been implementations of large proofs-by-exhaustion of a mathematical theorem. The idea is to use a computer program to perform lengthy computations, and to provide a proof that the result of these computations implies the given theorem. In 1976, the four color theorem was the first major theorem to be verified using a computer program. Attempts have also been made in the area of artificial intelligence research to create smaller, explicit, new proofs of mathematical theorems from the bottom up using machine reasoning techniques such as heuristic search. Such automated theorem provers have proved a number of new results and found new proofs for known theorems. Additionally, interactive proof assistants allow mathematicians to develop human-readable proofs which are nonetheless formally verified for correctness. Since these proofs are generally human-surveyable (albeit with difficulty, as with the proof of the Robbins conjecture) they do not share the controversial implications of computer-aided proofs-by-exhaustion."
"http://dbpedia.org/resource/Automated_reasoning"	"Automated reasoning"	"Artificial intelligence"	"Automated reasoning is an area of computer science and mathematical logic dedicated to understanding different aspects of reasoning. The study of automated reasoning helps produce computer programs that allow computers to reason completely, or nearly completely, automatically. Although automated reasoning is considered a sub-field of artificial intelligence, it also has connections with theoretical computer science, and even philosophy. The most developed subareas of automated reasoning are automated theorem proving (and the less automated but more pragmatic subfield of interactive theorem proving) and automated proof checking (viewed as guaranteed correct reasoning under fixed assumptions). Extensive work has also been done in reasoning by analogy induction and abduction. Other important topics include reasoning under uncertainty and non-monotonic reasoning. An important part of the uncertainty field is that of argumentation, where further constraints of minimality and consistency are applied on top of the more standard automated deduction. John Pollock's OSCAR system is an example of an automated argumentation system that is more specific than being just an automated theorem prover. Tools and techniques of automated reasoning include the classical logics and calculi, fuzzy logic, Bayesian inference, reasoning with maximal entropy and a large number of less formal ad hoc techniques."
"http://dbpedia.org/resource/Artificial_intelligence_systems_integration"	"Artificial intelligence systems integration"	"Artificial intelligence"	"The core idea of A.I. systems integration is making individual software components, such as speech synthesizers, interoperable with other components, such as common sense knowledgebases, in order to create larger, broader and more capable A.I. systems. The main methods that have been proposed for integration are message routing, or communication protocols that the software components use to communicate with each other, often through a middleware blackboard system. Most artificial intelligence systems involve some sort of integrated technologies, for example the integration of speech synthesis technologies with that of speech recognition. However, in recent years there has been an increasing discussion on the importance of systems integration as a field in its own right. Proponents of this approach are researchers such as Marvin Minsky, Aaron Sloman, Deb Roy, Kristinn R. Thórisson and Michael A. Arbib. A reason for the recent attention A.I. integration is attracting is that there have already been created a number of (relatively) simple A.I. systems for specific problem domains (such as computer vision, speech synthesis, etc.), and that integrating what's already available is a more logical approach to broader A.I. than building monolithic systems from scratch."
"http://dbpedia.org/resource/Virtual_intelligence"	"Virtual intelligence"	"Artificial intelligence"	"The emergence of virtual world technologies within these immersive environments. Many virtual worlds have options for persistent avatars that provide information, training, role playing, and social interactions. The immersion of virtual worlds provides a unique platform for VI beyond the traditional paradigm of past user interfaces (UIs). What Alan Turing established as the benchmark for telling the difference between human and computerized intelligence was done void of visual influences. With today's VI bots, virtual intelligence has evolved past the constraints of past testing into a new level of the machine's ability to demonstrate intelligence. The immersive features of these environments provide non verbal elements that affect the realism provided by virtually intelligent agents. Virtual intelligence is the intersection of these two technologies: 
*  Virtual environments: Immersive 3D spaces provide for collaboration, simulations, and role playing interactions for training. Many of these virtual environments are currently being used for government and academic projects, including Second Life, VastPark, Olive, OpenSim, Outerra, Oracle's Open Wonderland, Duke University's Open Cobalt, and many others. Some of the commercial virtual worlds are also taking this technology into new directions, including the high definition virtual world Blue Mars. 
*  Artificial intelligence The virtual environments provide non-verbals and visual cues that can affect not only the believability of the VI, but also the usefulness of it. Because – like many things in technology – it's not just about ""whether or not it works"" but also about ""how we feel about it working"". Virtual Intelligence draws a new distinction as to how this application of AI is different due to the environment in which it operates."
"http://dbpedia.org/resource/Commonsense_reasoning"	"Commonsense reasoning"	"Artificial intelligence"	"Commonsense reasoning is one of the branches ofArtificial intelligence (AI) that is concerned with simulating the humanability to make presumptions about the type and essence of ordinary situations theyencounter every day. These assumptions include judgments about the physicalproperties, purpose, intentions and behavior of people and objects, as well aspossible outcomes of their actions and interactions. A device that exhibitscommonsense reasoning will be capable of predicting results and drawingconclusions that are similar to humans' folk psychology (humans' innate abilityto reason about people’s behavior and intentions) and naive physics (humans' naturalunderstanding of the physical world)."
"http://dbpedia.org/resource/20Q"	"20Q"	"Artificial intelligence"	"20Q is a computerized game of twenty questions that began as a test in artificial intelligence (AI). It was invented by Robin Burgener in 1988. The game was made handheld by Radica in 2004, then it was discontinued in 2011 because Techno Source took the license for 20Q handheld devices. The game 20Q is based on the spoken parlor game known as twenty questions, and is both a website and a handheld device. 20Q asks the player to think of something and will then try to guess what they are thinking of with twenty yes-or-no questions. If it fails to guess in 20 questions, it will ask an additional 5 questions. If it fails to guess even with 25 (or 30) questions, the player is declared the winner. Sometimes the first guess of the object can be asked at question 14."
"http://dbpedia.org/resource/AI_takeover"	"AI takeover"	"Artificial intelligence"	"AI takeover refers to a hypothetical scenario in which artificial intelligence (AI) becomes the dominant form of intelligence on Earth, with computers or robots effectively taking control of the planet away from the human race. Possible scenarios include a takeover by a superintelligent AI and the popular notion of a robot uprising. Some public figures, such as Stephen Hawking and Elon Musk, have advocated research into precautionary measures to ensure future superintelligent machines remain under human control. Robot rebellions have been a major theme throughout science fiction for many decades (notably in the film series The Terminator) though the scenarios dealt with by science fiction are generally very different from those of concern to scientists."
"http://dbpedia.org/resource/Discovery_system"	"Discovery system"	"Artificial intelligence"	"A discovery system is an artificial intelligence system which attempts to discover new scientific concepts or laws. Notable discovery systems have included, 
* Autoclass 
* Automated Mathematician 
* DALTON 
* Eurisko 
* Glauber 
* Machine for Questions and Answers 
* Stahl"
"http://dbpedia.org/resource/Loebner_Prize"	"Loebner Prize"	"Artificial intelligence"	"The Loebner Prize is an annual competition in artificial intelligence that awards prizes to the chatterbot considered by the judges to be the most human-like. The format of the competition is that of a standard Turing test. In each round, a human judge simultaneously holds textual conversations with a computer program and a human being via computer. Based upon the responses, the judge must decide which is which. The contest was launched in 1990 by Hugh Loebner in conjunction with the Cambridge Center for Behavioral Studies, Massachusetts, United States. It has since been associated with Flinders University, Dartmouth College, the Science Museum in London, University of Reading and Ulster University, Magee Campus, Derry, UK City of Culture. In 2004 and 2005, it was held in Loebner's apartment in New York City. Within the field of artificial intelligence, the Loebner Prize is somewhat controversial; the most prominent critic, Marvin Minsky, called it a publicity stunt that does not help the field along."
"http://dbpedia.org/resource/Chess_as_mental_training"	"Chess as mental training"	"Artificial intelligence"	"There are efforts to use the game of chess as a tool to aid the intellectual development of young people. Chess is significant in cognitive psychology and artificial intelligence (AI) studies, because it represents the domain in which expert performance has been most intensively studied and measured. Although the results of research studies have failed to produce unambiguous support for the intellectual benefits of playing chess, several local government, schools, and student organizations all over the world are implementing chess programs. New York based Chess-In-The-Schools, Inc. has been active in the public school system in the city since 1986. It currently reaches more than 30,000 students annually. America's Foundation for Chess has initiated programs in partnership with local school districts in several U.S. cities, including Seattle, San Diego, Philadelphia, and Tampa. The Chess'n Math Association promotes chess at the scholastic level in Canada. Chess for Success is a program for at-risk schools in Oregon. Since 1991, the U.S. Chess Center in Washington, D.C. teaches chess to children, especially those in the inner city, ""as a means of improving their academic and social skills."" There are a number of experiments that suggest that learning and playing chess aids the mind. The Grandmaster Eugene Torre Chess Institute in the Philippines, the United States Chess Federation's chess research bibliography, and English educational consultant Tony Buzan's Brain Foundation, among others, continuously collect such experimental results. The advent of chess software that automatically record and analyze the moves of each player in each game and can tirelessly play with human players of various levels, further helped in giving new directions to experimental designs on chess as mental training."
"http://dbpedia.org/resource/Open_Mind_Common_Sense"	"Open Mind Common Sense"	"Artificial intelligence"	"Open Mind Common Sense (OMCS) is an artificial intelligence project based at the Massachusetts Institute of Technology (MIT) Media Lab whose goal is to build and utilize a large commonsense knowledge base from the contributions of many thousands of people across the Web. Since its founding in 1999, it has accumulated more than a million English facts from over 15,000 contributors in addition to knowledge bases in other languages. Much of OMCS's software is built on three interconnected representations: the natural language corpus that people interact with directly, a semantic network built from this corpus called ConceptNet, and a matrix-based representation of ConceptNet called AnalogySpace that can infer new knowledge using dimensionality reduction. The knowledge collected by Open Mind Common Sense has enabled research projects at MIT and elsewhere."
"http://dbpedia.org/resource/Leverhulme_Centre_for_the_Future_of_Intelligence"	"Leverhulme Centre for the Future of Intelligence"	"Artificial intelligence"	"The Leverhulme Centre for the Future of Intelligence is an interdisciplinary research centre within the University of Cambridge that explores the opportunities and challenges to humanity from the development of artificial intelligence. The Centre brings together academics from the fields of computer science, philosophy, social science and others and is a collaboration led by the University of Cambridge with links to the Oxford Martin School at the University of Oxford, Imperial College London, and the University of California, Berkeley."
"http://dbpedia.org/resource/Roborace"	"Roborace"	"Artificial intelligence"	"Roborace will be a motorsport championship with autonomously driving, electrically powered vehicles. The series will be held on the same tracks as the Formula E championship season, Roborace leasing the tracks and adapting them. This will be the first global championship for driverless cars. The first race is intended to take place during the 2016–17 Formula E season, on the same circuits as the main championship. Ten teams, each with two driverless cars, will compete in one-hour races over the full season. All teams will have the same cars, but will develop their own real-time computing algorithms and artificial intelligence technologies. The cars will be manufactured by a company called Kinetik, which announced that the cars would be capable of top speeds of more than 300 km/h (190 mph). They were designed by Daniel Simon."
"http://dbpedia.org/resource/AlphaGo"	"AlphaGo"	"Artificial intelligence"	"AlphaGo is a computer program developed by Google DeepMind in London to play the board game Go. In October 2015, it became the first Computer Go program to beat a professional human Go player without handicaps on a full-sized 19×19 board. In March 2016, it beat Lee Sedol in a five-game match, the first time a computer Go program has beaten a 9-dan professional without handicaps. Although it lost to Lee Sedol in the fourth game, Lee resigned the final game, giving a final score of 4 games to 1 in favour of AlphaGo. In recognition of beating Lee Sedol, AlphaGo was awarded an honorary 9-dan by the Korea Baduk Association. AlphaGo's algorithm uses a Monte Carlo tree search to find its moves based on knowledge previously ""learned"" by machine learning, specifically by an artificial neural network (a deep learning method) by extensive training, both from human and computer play."
"http://dbpedia.org/resource/LeadCrunch"	"LeadCrunch"	"Artificial intelligence"	"LeadCrunch is a software as a service (SaaS) predictive lead generation company. The company's product finds business-to-business (B2B) leads using artificial intelligence to data mine public, private and proprietary data sources. It offers a self-service portal that automatically identifies the ideal buyer profile by evaluating a company’s existing customer base."
"http://dbpedia.org/resource/Luminoso"	"Luminoso"	"Artificial intelligence"	"Luminoso, a Cambridge, MA-based text analysis and artificial intelligence company, spun out of the MIT Media Lab and its crowd-sourced Open Mind Common Sense (OMCS) project. The company has raised $8 million in financing and its clients include Sony, Autodesk, Intel, NASA, REI and Scotts."
"http://dbpedia.org/resource/Mivar-based_approach"	"Mivar-based approach"	"Artificial intelligence"	"Mivar-based approach is a mathematical tool for designing artificial intelligence systems,which has been developed by combining production approach and Petri nets. Mivar-based approach has been developed for semantic analysis and adequate representation of humanitarian epistemological and axiological principles in the process of developing artificial intelligence (AI). Mivar-based approach incorporates scientific fields of computer sciences, informatics and discrete mathematics, databases, expert systems, graph theory, matrices, logical inference systems. Mivar-based approach involves two basic technologies: 
*  Mivar-based technology of information accumulation is a method of creating global evolutionary data-and-rules bases with variable structure on the basis of adaptive discrete mivar-oriented information space, unified data and rules representation, based on three main concepts: “object, property, relation”. Mivar-based technology of information accumulation is designed to store any information with possible evolutionary structure changing and without limitations concerning the amount of information and forms of its presentation. 
*  Mivar-based technology of data processing is a method of creating logical inference system or automated algorithm construction from modules, services or procedures on the basis of active trained mivar network of rules with the linear computational complexity. Mivar-based technology of data processing is designed for data processing including logical inference, computational procedures and services. In fact, mivar networks allow us to develop production approach (cause-effect dependencies “If-then”) and create an automated trained logically reasoning system."
"http://dbpedia.org/resource/University_of_Pittsburgh_Intelligent_Systems_Program"	"University of Pittsburgh Intelligent Systems Program"	"Artificial intelligence"	"The Intelligent Systems Program (ISP) is a multidisciplinary graduate program in the University of Pittsburgh's Kenneth P. Dietrich School of Arts and Sciences dedicated to applied artificial intelligence (AI). It is located on the University of Pittsburgh's main campus in the Oakland neighborhood of Pittsburgh. ISP offers master's degrees and PhD degrees in the areas of Natural Language Processing (NLP) and Information Retrieval, Intelligent tutoring system and Educational Technology, Machine Learning and Decision Making, Biomedical Informatics and Artificial Intelligence and Law."
"http://dbpedia.org/resource/Combs_method"	"Combs method"	"Artificial intelligence"	"The Combs method is a method of writing fuzzy logic rules described by William E. Combs in 1997. It is designed to prevent combinatorial explosion in fuzzy logic rules. The Combs method takes advantage of the logical equality ."
"http://dbpedia.org/resource/Situated"	"Situated"	"Artificial intelligence"	"In artificial intelligence and cognitive science, the term situated refers to an agent which is embedded in an environment. The term situated is commonly used to refer to robots, but some researchers argue that software agents can also be situated if: 
*  they exist in a dynamic (rapidly changing) environment, which 
*  they can manipulate or change through their actions, and which 
*  they can sense or perceive. Examples might include web-based agents, which can alter data or trigger processes (such as purchases) over the internet, or virtual-reality bots which inhabit and change virtual worlds, such as Second Life. Being situated is generally considered to be part of being embodied, but it is useful to consider each perspective individually. The situated perspective emphasizes that intelligent behaviour derives from the environment and the agent's interactions with it. The nature of these interactions are defined by an agent's embodiment."
"http://dbpedia.org/resource/Autonomous_agent"	"Autonomous agent"	"Artificial intelligence"	"An autonomous agent is an intelligent agent operating on an owner's behalf but without any interference of that ownership entity. An intelligent agent, however appears according to a multiply cited statement in a no longer accessible IBM white paper as follows: Intelligent agents are software entities that carry out some set of operations on behalf of a user or another program with some degree of independence or autonomy, and in so doing, employ some knowledge or representation of the user's goals or desires. Such an agent is a system situated in, and part of, a technical or natural environment, which senses any or some status of that environment, and acts on it in pursuit of its own agenda. Such an agenda evolves from drives (or programmed goals). The agent acts to change part of the environment or of its status and influences what it sensed. Non-biological examples include intelligent agents, autonomous robots, and various software agents, including artificial life agents, and many computer viruses. Biological examples are not yet defined."
"http://dbpedia.org/resource/ASR-complete"	"ASR-complete"	"Artificial intelligence"	"ASR-complete is, by analogy to ""NP-completeness"" in complexity theory, a term to indicate that the difficulty of a computational problem is equivalent to solving the central Automatic Speech Recognition problem, i.e. recognize and understanding spoken language. Unlike ""NP-completeness"", this term is typically used informally. Such problems are hypothesised to include: 
* Spoken natural language understanding 
* Understanding speech from far-field microphones - i.e. handling the reverbation and background noise These problems are easy for humans to do (in fact, they are described directly in terms of imitating humans). Some systems can solve very simple restricted versions of these problems, but none can solve them in their full generality."
"http://dbpedia.org/resource/Connectionist_expert_system"	"Connectionist expert system"	"Artificial intelligence"	"Connectionist expert systems are artificial neural network (ANN) based expert systems where the ANN generates inferencing rules e.g., fuzzy-multi layer perceptron where linguistic and natural form of inputs are used. Apart from that, rough set theory may be used for encoding knowledge in the weights better and also genetic algorithms may be used to optimize the search solutions better. Symbolic reasoning methods may also be incorporated (see hybrid intelligent system). (Also see expert system, neural network, clinical decision support system.)"
"http://dbpedia.org/resource/Argumentation_framework"	"Argumentation framework"	"Artificial intelligence"	"An argumentation framework, or argumentation system, is a way to deal with contentious information and draw conclusions from it. In an abstract argumentation framework, entry-level information is a set of abstract arguments that, for instance, represent data or a proposition. Conflicts between arguments are represented by a binary relation on the set of arguments. In concrete terms, you represent an argumentation framework with a directed graph such that the nodes are the arguments, and the arrows represent the attack relation.There exist some extensions of the Dung's framework, like the logic-based argumentation frameworks or the value-based argumentation frameworks."
"http://dbpedia.org/resource/March_of_the_Machines"	"March of the Machines"	"Artificial intelligence"	"March of the Machines: Why the New Race of Robots Will Rule the World (1997, hardcover), published in paperback as March of the Machines: The Breakthrough in Artificial Intelligence (2004), is a book by Kevin Warwick. It presents an overview of robotics and artificial intelligence (AI) and then imagines future scenarios. In particular, Warwick finds it likely that AIs will become smart enough to replace humans, and humans may be unable to stop them."
"http://dbpedia.org/resource/Existential_risk_from_artificial_general_intelligence"	"Existential risk from artificial general intelligence"	"Artificial intelligence"	"Existential risk from artificial general intelligence is the hypothetical threat that dramatic progress in artificial intelligence (AI) could someday result in human extinction (or some other unrecoverable global catastrophe). The human race currently dominates other species because the human brain has some distinctive capabilities that the brains of other animals lack. If AI surpasses humanity in general intelligence and becomes ""superintelligent"", then this new superintelligence could become powerful and difficult to control. Just as the fate of the mountain gorilla depends on human goodwill, so might the fate of humanity depend on the actions of a future machine superintelligence. The severity of different AI risk scenarios is widely debated, and rests on a number of unresolved questions about future progress in computer science. Two sources of concern are that a sudden and unexpected ""intelligence explosion"" might take an unprepared human race by surprise, and that controlling a superintelligent machine (or even instilling it with human-compatible values) may be an even harder problem than naively supposed."
"http://dbpedia.org/resource/Artificial_general_intelligence"	"Artificial general intelligence"	"Artificial intelligence"	"Artificial general intelligence (AGI) is the intelligence of a (hypothetical) machine that could successfully perform any intellectual task that a human being can. It is a primary goal of artificial intelligence research and an important topic for science fiction writers and futurists. Artificial general intelligence is also referred to as ""strong AI"", ""full AI"" or as the ability of a machine to perform ""general intelligent action"". Some references emphasize a distinction between strong AI and ""applied AI"" (also called ""narrow AI"" or ""weak AI""): the use of software to study or accomplish specific problem solving or reasoning tasks. Weak AI, in contrast to strong AI, does not attempt to perform the full range of human cognitive abilities."
"http://dbpedia.org/resource/CALO"	"CALO"	"Artificial intelligence"	"CALO was an artificial intelligence project that attempted to integrate numerous AI technologies into a cognitive assistant. CALO is an acronym for ""Cognitive Assistant that Learns and Organizes"". The name was inspired by the Latin word ""calonis,"" which means ""soldier’s servant"". The project started in May 2003 and ran for five years, ending in 2008. The CALO effort has had many major spin-offs, most notably the Siri intelligent software assistant that is now part of the Apple iOS since iOS 5 in the iPhone 4S, iPhone 5, iPod Touch 5 and the New iPad; Social Kinetics, a social application that learned personalized intervention and treatment strategies for chronic disease patients, sold to RedBrick Health; the Trapit project, which is a web scraper and news aggregator that makes intelligent selections of web content based on user preferences; Tempo AI, a smart calendar; Desti, a personalized travel guide; and Kuato Studios, a game development startup. CALO was funded by the Defense Advanced Research Projects Agency (DARPA) under its Personalized Assistant that Learns (PAL) program. DARPA's five-year contract brought together over 300 researchers from 25 of the top university and commercial research institutions, with the goal of building a new generation of cognitive assistants that can reason, learn from experience, be told what to do, explain what they are doing, reflect on their experience, and respond robustly to surprise. SRI International was the lead integrator responsible for coordinating the effort to produce an assistant that can live with and learn from its users, provide value to them, and then pass a yearly evaluation that measures how well the system has learned to do its job."
"http://dbpedia.org/resource/Cobweb_(clustering)"	"Cobweb (clustering)"	"Artificial intelligence"	"COBWEB is an incremental system for hierarchical conceptual clustering. COBWEB was invented by Professor Douglas H. Fisher, currently at Vanderbilt University. COBWEB incrementally organizes observations into a classification tree. Each node in a classification tree represents a class (concept) and is labeled by a probabilistic concept that summarizes the attribute-value distributions of objects classified under the node. This classification tree can be used to predict missing attributes or the class of a new object. There are four basic operations COBWEB employs in building the classification tree. Which operation is selected depends on the category utility of the classification achieved by applying it. The operations are: 
*  Merging Two Nodes  Merging two nodes means replacing them by a node whose children is the union of the original nodes' sets of children and which summarizes the attribute-value distributions of all objects classified under them. 
*  Splitting a node A node is split by replacing it with its children. 
*  Inserting a new node A node is created corresponding to the object being inserted into the tree. 
*  Passing an object down the hierarchy Effectively calling the COBWEB algorithm on the object and the subtree rooted in the node."
"http://dbpedia.org/resource/K-line_(artificial_intelligence)"	"K-line (artificial intelligence)"	"Artificial intelligence"	"A K-line, or Knowledge-line, is a mental agent which represents an association of a group of other mental agents found active when a subject solves a certain problem or formulates a new idea. These were first described in Marvin Minsky's essay K-lines: A Theory of Memory, published in 1980 in the journal Cognitive Science: When you ""get an idea,"" or ""solve a problem"" ... you create what we shall call a K-line. ... When that K-line is later ""activated"", it reactivates ... mental agencies, creating a partial mental state ""resembling the original."" ""Whenever you 'get a good idea', solve a problem, or have a memorable experience, you activate a K-line to 'represent' it. A K-line is a wirelike structure that attaches itself to whichever mental agents are active when you solve a problem or have a good idea. When you activate that K-line later, the agents attached to it are aroused, putting you into a 'mental state' much like the one you were in when you solved that problem or got that idea. This should make it relatively easy for you to solve new, similar problems!"" (1998, p. 82.)"
"http://dbpedia.org/resource/IJCAI_Computers_and_Thought_Award"	"IJCAI Computers and Thought Award"	"Artificial intelligence"	"The IJCAI Computers and Thought Award is presented every two years by the International Joint Conferences on Artificial Intelligence (IJCAI), recognizing outstanding young scientists in artificial intelligence. It was originally funded with royalties received from the book Computers and Thought (edited by Edward Feigenbaum and Julian Feldman), and is currently funded by IJCAI. It is considered to be ""the premier award for artificial intelligence researchers under the age of 35""."
"http://dbpedia.org/resource/DeepDream"	"DeepDream"	"Artificial intelligence"	"DeepDream is a computer vision program created by Google which uses a convolutional neural network to find and enhance patterns in images via algorithmic pareidolia, thus creating a dreamlike hallucinogenic appearance in the deliberately over-processed images.Google's program popularized the term (deep) ""dreaming"" to refer to the generation of images that desired activations in a trained deep network, and the term now refers to a collection of related approaches."
"http://dbpedia.org/resource/Applications_of_artificial_intelligence"	"Applications of artificial intelligence"	"Artificial intelligence"	"AI researchers have created many tools to solve the most difficult problems in computer science. Many of their inventions have been adopted by mainstream computer science and are no longer considered a part of AI. (See AI effect). According to , p. 15), all of the following were originally developed in AI laboratories: time sharing,interactive interpreters,graphical user interfaces and the computer mouse,rapid development environments,the linked list data structure,automatic storage management,symbolic programming,functional programming,dynamic programming andobject-oriented programming."
"http://dbpedia.org/resource/Ontology_engineering"	"Ontology engineering"	"Artificial intelligence"	"Ontology engineering in computer science and information science is a field which studies the methods and methodologies for building ontologies: formal representations of a set of concepts within a domain and the relationships between those concepts.A large-scale representation of abstract concepts such as actions, time, physical objects and beliefs would be an example of ontological engineering. Ontology engineering is one of the areas of applied ontology, and can be seen as an application of philosophical ontology. Core ideas and objectives of ontology engineering are also central in conceptual modeling."
"http://dbpedia.org/resource/OpenIRIS"	"OpenIRIS"	"Artificial intelligence"	"OpenIRIS is the open source version of IRIS, a semantic desktop that enables users to create a ""personal map"" across their office-related information objects. The name IRIS is an acronym for ""Integrate. Relate. Infer. Share."" IRIS includes a machine-learning platform to help automate this process. It provides ""dashboard"" views, contextual navigation, and relationship-based structure across an extensible suite of office applications, including a calendar, web and file browser, e-mail client, and instant messaging client. IRIS was built as part of SRI International's CALO project, a very large artificial intelligence funded by the Defense Advanced Research Projects Agency (DARPA) under its Personalized Assistant that Learns program. 1.  
*  Integrate: IRIS harvests and unifies the data from multiple, independently developed applications such as email (Mozilla), web browser (Mozilla), file manager, calendar (OpenOffice), and Chat (XMPP). 2.  
*  Relate: IRIS stores this data an ontology-based KB that supports rich representation and connection to the user's worklife. In IRIS, you can express things like: ""this file, authored by this person, was presented at this meeting about this project"". 3.  
*  Infer: IRIS comes with a learning framework that makes it possible for online learning algorithms (e.g. clustering, classification, extraction, prioritization, association, summarization, various predictors) to plug-in and reason about the rich data and events presented to them. In addition to learning through observation of user activity, CALO's learning algorithms have access to interface mechanisms in IRIS where they can get feedback from the user. 4.  
*  Share: The knowledge created in IRIS by the user and by CALO will eventually be made sharable with selected team members. Currently, the ability to share content across IRIS users is a future capability."
"http://dbpedia.org/resource/Computer_Arimaa"	"Computer Arimaa"	"Artificial intelligence"	"Computer Arimaa refers to the playing of the board game Arimaa by computer programs. In 2002, Indian American computer engineer Omar Syed published the rules to Arimaa and announced a $10,000 prize, available annually until 2020, for the first computer program (running on standard, off-the-shelf hardware) able to defeat each of three top-ranked human players in a three-game series. The prize was claimed in 2015, when a computer program played 7:2 against three human players. The game has been the subject of several ."
"http://dbpedia.org/resource/Enterprise_cognitive_system"	"Enterprise cognitive system"	"Artificial intelligence"	"Enterprise Cognitive Systems (ECS) are part of a broader shift in computing, from a programmatic to a probabilistic approach, called Cognitive computing. An Enterprise Cognitive System makes a new class of complex decision support problems computable, where the business context is ambiguous, multi-faceted, and fast-evolving, and what to do in such a situation is usually assessed today by the business user. An ECS is designed to synthesize a business context and link it to the desired outcome. It recommends evidence-based actions to help the end-user achieve the desired outcome. It does so by finding past situations similar to the current situation, and extracting the repeated actions that best influence the desired outcome. While general-purpose Cognitive Systems can be used for different outputs, prescriptive, suggestive, instructive, or simply entertaining, an Enterprise Cognitive System is focused on action, not insight, to help in assessing what to do in a complex situation."
"http://dbpedia.org/resource/International_Conference_on_Autonomous_Agents_and_Multiagent_Systems"	"International Conference on Autonomous Agents and Multiagent Systems"	"Artificial intelligence"	"The International Conference on Autonomous Agents and Multi-Agent Systems or AAMAS is the leading scientific conference for research in the areas of artificial intelligence, autonomous agents, and multiagent systems. It is annually organized by a non-profit organization called the International Foundation for Autonomous Agents and Multiagent Systems (IFAAMAS)."
"http://dbpedia.org/resource/Plug_&_Pray"	"Plug & Pray"	"Artificial intelligence"	"Plug & Pray is a 2010 documentary film about the promise, problems and ethics of artificial intelligence and robotics. The main protagonists are the former MIT professor Joseph Weizenbaum and the futurist Raymond Kurzweil. The title is a pun on the computer hardware phrase ""Plug and Play""."
"http://dbpedia.org/resource/Weak_AI"	"Weak AI"	"Artificial intelligence"	"Weak AI (also known as narrow AI) is non-sentient artificial intelligence that is focused on one narrow task. Weak AI is defined in contrast to either strong AI (a machine with consciousness, sentience and mind) or artificial general intelligence (a machine with the ability to apply intelligence to any problem, rather than just one specific problem). All currently existing systems considered artificial intelligence of any sort are weak AI at most. Siri is a good example of narrow intelligence. Siri operates within a limited pre-defined range, there is no genuine intelligence, no self-awareness, no life despite being a sophisticated example of weak AI. In Forbes (2011), Ted Greenwald wrote: ""The iPhone/Siri marriage represents the arrival of hybrid AI, combining several narrow AI techniques plus access to massive data in the cloud."" AI researcher Ben Goertzel, on his blog in 2010, stated Siri was ""VERY narrow and brittle"" evidenced by annoying results if you ask questions outside the limits of the application. Some commentators think weak AI could be dangerous. In 2013 George Dvorsky stated via io9: ""Narrow AI could knock out our electric grid, damage nuclear power plants, cause a global-scale economic collapse, misdirect autonomous vehicles and robots..."" The Stanford Center for Internet and Society, in the following quote, contrasts strong AI with weak AI regarding the growth of narrow AI presenting ""real issues."" Weak or ""narrow"" AI, in contrast, is a present-day reality. Software controls many facets of daily life and, in some cases, this control presents real issues. One example is the May 2010 ""flash crash"" that caused a temporary but enormous dip in the market.— Ryan Calo, Center for Internet and Society, Stanford Law School, 30 August 2011. The following two excerpts from Singularity Hub summarise weak-narrow AI: When you call the bank and talk to an automated voice you are probably talking to an AI…just a very annoying one. Our world is full of these limited AI programs which we classify as “weak” or “narrow” or “applied”. These programs are far from the sentient, love-seeking, angst-ridden artificial intelligences we see in science fiction, but that’s temporary. All these narrow AIs are like the amino acids in the primordial ooze of the Earth.We’re slowly building a library of narrow AI talents that are becoming more impressive. Speech recognition and processing allows computers to convert sounds to text with greater accuracy. Google is using AI to caption millions of videos on YouTube. Likewise, computer vision is improving so that programs like Vitamin d Video can recognize objects, classify them, and understand how they move. Narrow AI isn’t just getting better at processing its environment it’s also understanding the difference between what a human says and what a human wants.— Aaron Saenz, Singularity Hub, 10 August 2010."
"http://dbpedia.org/resource/Information_space_analysis"	"Information space analysis"	"Artificial intelligence"	"Information space analysis is a deterministic method, enhanced by machine intelligence, for locating and assessing resources for team centric efforts. Organizations need to be able to quickly assemble teams backed by the support services, information, and material to do the job. To do so, these teams need to find and assess sources of services that are potential participants in the team effort. To support this initial team and resource development, information needs to be developed via analysis tools that help make sense of sets of data sources in an Intranet or Internet. Part of the process is to characterize them, partition them, sort and filter them. These tools focus on three key issues in forming a collaborative team: 1.  
* Help individuals responsible for forming the team understand what is available. 2.  
* Assist team members in identifying the structure and categorize the information available to them in a manner specifically suited to the task at hand. 3.  
* Aid team members in understand the mappings between their organization of the information and those used by others who might participate. Information space analysis tools combine multiple methods to assist in this task. This causes the tools to be particularly well-suited to integrating additional technologies in order to create specialized systems."
"http://dbpedia.org/resource/Knowledge-based_configuration"	"Knowledge-based configuration"	"Artificial intelligence"	"Knowledge-based configuration, or also referred to as product configuration or product customization, is an activity of customising a product to meet the needs of a particular customer. The product in question may consist of mechanical parts, services, and software. Knowledge-based configuration is a major application area for artificial intelligence (AI), and it is based on modelling of the configurations in a manner that allows the utilisation of AI techniques for searching for a valid configuration to meet the needs of a particular customer."
"http://dbpedia.org/resource/Artificial_imagination"	"Artificial imagination"	"Artificial intelligence"	"Artificial imagination (AIm), also called Synthetic imagination or machine imagination is defined as artificial simulation of human imagination by general or special purpose computers or artificial neural networks. The term artificial imagination is also used to describe a property of machines or programs: Among some of the traits that researchers hope to simulate using machines include creativity, vision, digital art, humor, satire, etc. Artificial imagination research uses tools and insights from many fields, including computer science, Rhetoric, psychology, creative arts, philosophy, neuroscience, affective computing, Artificial Intelligence, Artificial intuition, cognitive science, linguistics, operations research, creative writing, probability and logic. The various practitioners in the field are researching various aspects of Artificial imagination, such as Artificial (visual) imagination,Artificial (aural) Imagination, modeling/filtering content based on human emotions and Interactive Search. Some articles on the topic speculate on how artificial imagination may evolve to create an artificial world which people may not want to leave at all. . Some researchers in the field, such as G. Schleis and M. Rizki, Dept. of Comput. Sci., Wayne State Univ. have focused on using artificial neural networks for simulating artificial imagination. The topic of artificial imagination has gotten interest from scholars outside the computer science domain, such as noted communications scholar Ernest Bormann, who came up with the Symbolic Convergence Theory and has worked on a project to develop artificial imagination in computer systems. How to Build a Mind: Toward Machines with Imagination by Igor Aleksander is a good academic book on the topic. Artificial Imagination, a roman à clef, is a good non-academic book supposedly written by an Artificial imagination system."
"http://dbpedia.org/resource/Cognitive_infocommunications"	"Cognitive infocommunications"	"Artificial intelligence"	"Cognitive infocommunications (CogInfoCom) investigates the link between the research areas of infocommunications and the cognitive sciences, as well as the various engineering applications which have emerged as the synergic combination of these sciences. The primary goal of CogInfoCom is to provide a systematic view of how cognitive processes can co-evolve with infocommunications devices so that the capabilities of the human brain may not only be extended through these devices, irrespective of geographical distance, but may also interact with the capabilities of any artificially cognitive system. This merging and extension of cognitive capabilities is targeted towards engineering applications in which artificial and/or natural cognitive systems are enabled to work together more effectively. Two important dimensions of cognitive infocommunications are the mode of communication and the type of communication.The mode of communication refers to the actors at the two endpoints of communication: 
*  Intra-cognitive communication: information transfer occurs between two cognitive beings with equivalent cognitive capabilities (e.g.: between two humans). 
*  Inter-cognitive communication: information transfer occurs between two cognitive beings with different cognitive capabilities (e.g.: between a human and an artificially cognitive system). The type of communication refers to the type of information that is conveyed between the two communicating entities, and the way in which this is done: 
*  Sensor-sharing communication: entities on both ends use the same sensory modality to perceive the communicated information. 
*  Sensor-bridging communication: sensory information obtained or experienced by each of the entities is not only transmitted, but also transformed to an appropriate and different sensory modality. 
*  Representation-sharing communication: the same information representation is used on both ends to communicate information. 
*  Representation-bridging communication: sensory information transferred to the receiver entity is filtered and/or adapted so that a different information representation is used on the two ends. Remarks 1.  
* A sensor-sharing application of CogInfoCom brings novelty to traditional infocommunications in the sense that it can convey any kind of signal normally perceptible through the actor's senses (i.e., without a distance to communicate across) to the other end of the communication line. The transferred information may describe not only the actor involved in the communication, but also the environment in which the actor is located. The key determinant of sensor-sharing communication is that the same sensory modality is used to perceive the sensory information on the receiving end of communication as would be used if the two actors were on the same end (in which case there would be no need for infocommunication). 2.  
* Sensor bridging can in cases reflect not only the way in which the information is conveyed (i.e., by changing sensory modality), but also the kind of information that is conveyed. Whenever the transferred information type is imperceptible to the receiving actor due to the lack of an appropriate sensory modality (e.g., because its cognitive system is incompatible with the information type) the communication of information will necessarily occur through sensor bridging. 3.  
* A CogInfoCom application can be regarded as an instance of representation sharing even if it bridges between different sensors. For example, if text is conveyed to a blind person using Braille writing, the tactile sensory modality is used instead of vision, but the representation still consists of a linear succession of textual elements which represent individual characters in the alphabet. By the same token, a CogInfoCom application can be regarded as representation bridging even if it uses sensor sharing. The first draft definition of CogInfoCom was given in  The definition was finalized based on the paper with the joint participation of the Startup Committee at the 1st International Workshop on Cognitive Infocommunications, held in Tokyo, Japan in 2010. Further information can be found in, and in the two special issues on CogInfoCom which have been published since then, and at the official website of CogInfoCom."
"http://dbpedia.org/resource/Alesis_Artificial_Intelligence"	"Alesis Artificial Intelligence"	"Artificial intelligence"	"Alesis is an artificially intelligent computer system capable of answering questions posed in natural language, developed by Venture Coding; a small tech start-up based in Telford, Alesis's specific function is to assist users on their computers or out and about."
"http://dbpedia.org/resource/Winograd_Schema_Challenge"	"Winograd Schema Challenge"	"Artificial intelligence"	"The Winograd Schema Challenge (WSC) is a test of machine intelligence proposed by Hector Levesque, a computer scientist at the University of Toronto. Designed to be an improvement on the Turing test, it is a multiple-choice test that employs questions of a very specific structure: they are instances of what are called Winograd Schemas, named after Terry Winograd, a professor of computer science at Stanford University. On the surface, Winograd Schema questions simply require the resolution of anaphora: the machine must identify the antecedent of an ambiguous pronoun in a statement. This makes it a task of natural language processing, but Levesque argues that for Winograd Schemas, the task requires the use of knowledge and commonsense reasoning. Nuance Communications announced in July 2014 that it would sponsor an annual WSC competition, with a prize of $25,000 for the best system that could match human performance."
"http://dbpedia.org/resource/User_behavior_analytics"	"User behavior analytics"	"Artificial intelligence"	"User behavior analytics (""UBA"") as defined by Gartner, is a cybersecurity process about detection of insider threats, targeted attacks, and financial fraud. UBA solutions look at patterns of human behavior, and then apply algorithms and statistical analysis to detect meaningful anomalies from those patterns—anomalies that indicate potential threats.' Instead of tracking devices or security events, UBA tracks a system's users. Big data platforms like Apache Hadoop are increasing UBA functionality by allowing them to analyze petabytes worth of data to detect insider threats and advanced persistent threats. The problem UBA responds to, as described by Nemertes Research CEO Johna Till Johnson, is that ""Security systems provide so much information that it's tough to uncover information that truly indicates a potential for real attack. Analytics tools help make sense of the vast amount of data that SIEM, IDS/IPS, system logs, and other tools gather. UBA tools use a specialized type of security analytics that focuses on the behavior of systems and the people using them. UBA technology first evolved in the field of marketing, to help companies understand and predict consumer-buying patterns. But as it turns out, UBA can be extraordinarily useful in the security context too."" Developments in UBA technology led Gartner to also recognize user and entity behavior analytics (""UEBA""). This expanded definition includes devices, applications, servers, data, or anything with an IP address. ""When end users have been compromised, malware can lay dormant and go undetected for months. Rather than trying to find where the outsider entered, UEBAs allow for quicker detection by using algorithms to detect insider threats."" Particularly in the computer security market, there are many vendors for UEBA applications. They can be ""differentiated by whether they are designed to monitor on-premises or cloud-based software as a service (SaaS) applications; the methods in which they obtain the source data; the type of analytics they use (i.e., packaged analytics, user-driven or vendor-written), and the service delivery method (i.e., on-premises or a cloud-based)."" Though not intended to be an exhaustive list, representative vendors include: 
*  Microsoft ATA (Advanced Threat Analytics) 
*  Veriato 
*  Bay Dynamics 
*  Darktrace 
*  E8 Security 
*  Exabeam 
*  Fortscale 
*  Gurucul 
*  LightCyber 
*  Niara 
*  Preempt 
*  Securonix 
*  Sqrrl 
*  Splunk 
*  Varonis According to the 2015 market guide released by Gartner, ""the UEBA market grew substantially in 2015; UEBA vendors grew their customer base, market consolidation began, and Gartner client interest in UEBA and security analytics increased."" The report further projected, ""Over the next three years, leading UEBA platforms will become preferred systems for security operations and investigations at some of the organizations they serve. It will be—and in some cases already is—much easier to discover some security events and analyze individual offenders in UEBA than it is in many legacy security monitoring systems."""
"http://dbpedia.org/resource/AgentSheets"	"AgentSheets"	"Artificial intelligence"	"AgentSheets is a Cyberlearning tool to teach students programming and related information technology skills through game design. AgentSheets is supported by a middle and high school curriculum called Scalable Game Design aligned with the ISTE National Educational Technology Standards (NETS). The mission of this project is to reinvent computer science in public schools by motivating & educating all students including women and underrepresented communities to learn about computer science through game design starting at the middle school level. Through this curriculum students build increasingly sophisticated games and, as part of this process, learn about computational concepts at the level of computational thinking that are relevant to game design as well as to computational science. The curriculum is made available through the Scalable Game Design Wiki. Research investigating motivational aspects of computer science education in public schools is currently exploring the introduction of game design in representative regions of the USA including technology hubs, inner city, rural and remote/tribal areas. Previous research has already found that game design with AgentSheets is universally accessible across gender as well as ethnicity and is not limited to students interested in playing video games. The results of the NSF ITEST program supported research investigating motivational and educational aspects of introducing computer science at the middle school level are extremely positive in terms of motivational levels, number of participants and participation of women and underrepresented communities. The participation is extremely high because most middle schools participating in the study have made Scalable Game Design a module that is part of existing required courses (e.g., computer power with keyboarding and power point). Many of the middle schools instruct all of their students in scalable game design reaching in some schools over 900 students per year, per school. Of the well over 1000 students participating in the project in the first semester over 52% were girls. Of the girls 85% enjoyed the scalable game design course and 78% would like to take another game design course."
"http://dbpedia.org/resource/Soft_computing"	"Soft computing"	"Artificial intelligence"	"In computer science, soft computing (sometimes referred to as computational intelligence, though CI does not have an agreed definition) is the use of inexact solutions to computationally hard tasks such as the solution of NP-complete problems, for which there is no known algorithm that can compute an exact solution in polynomial time. Soft computing differs from conventional (hard) computing in that, unlike hard computing, it is tolerant of imprecision, uncertainty, partial truth, and approximation. In effect, the role model for soft computing is the human mind. The principal constituents of Soft Computing (SC) are Fuzzy Logic (FL), Evolutionary Computation (EC), Machine Learning (ML) and Probabilistic Reasoning (PR), with the latter subsuming belief networks, chaos theory and parts of learning theory."
"http://dbpedia.org/resource/Hindsight_optimization"	"Hindsight optimization"	"Artificial intelligence"	"Hindsight optimisation (HOP) is a computer science technique used in artificial intelligence for analysis of actions which have stochastic results. HOP is used in combination with a deterministic planner. By creating sample results for each of the possible actions from the given state (i.e. determinising the actions), and using the deterministic planner to analyse those sample results, HOP allows an estimate of the actual action."
"http://dbpedia.org/resource/Intelligent_despatch"	"Intelligent despatch"	"Artificial intelligence"	"Intelligent despatch is an artificial intelligence system that automates and controls the allocation of resources for services. Applications range from courier services to taxi services to emergency services to repair services to battlefield management. An example of an intelligent despatch system is the ""Larry"" (previously called AIBA for Advanced Information Based Allocation) system pioneered by eCourier who use rules based software for automatically selecting the most effective courier and service based on parameters it receives."
"http://dbpedia.org/resource/Intelligent_word_recognition"	"Intelligent word recognition"	"Artificial intelligence"	"Intelligent Word Recognition, or IWR, is the recognition of unconstrained handwritten words. IWR recognizes entire handwritten words or phrases instead of character-by-character, like its predecessor, Optical Character Recognition (OCR). IWR technology matches handwritten or printed words to a user-defined dictionary, significantly reducing character errors encountered in typical character-based recognition engines. New technology on the market utilizes IWR, OCR, and ICR together, which opens many doors for the processing of documents, either constrained (hand printed or machine printed) or unconstrained (freeform cursive). IWR also eliminates a large percentage of the manual data entry of handwritten documents that, in the past, could only be keyed by a human, creating an automated workflow. When cursive handwriting is in play, for each word analyzed, the system breaks down the words into a sequence of graphemes, or subparts of letters. These various curves, shapes and lines make up letters and IWR considers these various shape and groupings in order to calculate a confidence value associated with the word in question. IWR is not meant to replace ICR and OCR engines which work well with printed data; however, IWR reduces the number of character errors associated with these engines, and it is ideal for processing real-world documents that contain mostly freeform, hard-to-recognize data, inherently unsuitable for them."
"http://dbpedia.org/resource/Moravec's_paradox"	"Moravec's paradox"	"Artificial intelligence"	"Moravec's paradox is the discovery by artificial intelligence and robotics researchers that, contrary to traditional assumptions, high-level reasoning requires very little computation, but low-level sensorimotor skills require enormous computational resources. The principle was articulated by Hans Moravec, Rodney Brooks, Marvin Minsky and others in the 1980s. As Moravec writes, ""it is comparatively easy to make computers exhibit adult level performance on intelligence tests or playing checkers, and difficult or impossible to give them the skills of a one-year-old when it comes to perception and mobility."" Linguist and cognitive scientist Steven Pinker considers this the most significant discovery uncovered by AI researchers. In his book The Language Instinct, he writes: The main lesson of thirty-five years of AI research is that the hard problems are easy and the easy problems are hard. The mental abilities of a four-year-old that we take for granted – recognizing a face, lifting a pencil, walking across a room, answering a question – in fact solve some of the hardest engineering problems ever conceived... As the new generation of intelligent devices appears, it will be the stock analysts and petrochemical engineers and parole board members who are in danger of being replaced by machines. The gardeners, receptionists, and cooks are secure in their jobs for decades to come. Similarly, Marvin Minsky emphasized that the most difficult human skills to reverse engineer are those that are unconscious. ""In general, we're least aware of what our minds do best,"" he wrote, and added ""we're more aware of simple processes that don't work well than of complex ones that work flawlessly."""
"http://dbpedia.org/resource/Synthetic_Environment_for_Analysis_and_Simulations"	"Synthetic Environment for Analysis and Simulations"	"Artificial intelligence"	"SEAS was developed to help Fortune 500 companies with strategic planning. Then it was used to help ""recruiting commanders to strategize ways to improve recruiting potential soldiers"". In 2004 SEAS was evaluated for its ability to help simulate ""the non-kinetic aspects of combat, things like the diplomatic, economic, political, infrastructure and social issues"". Sentient World Simulation is the name given to the current vision of making SEAS a ""continuously running, continually updated mirror model of the real world that can be used to predict and evaluate future events and courses of action."""
"http://dbpedia.org/resource/Shyster_(expert_system)"	"Shyster (expert system)"	"Artificial intelligence"	"SHYSTER is a legal expert system developed at the Australian National University in Canberra in 1993. It was written as the doctoral dissertation of James Popple under the supervision of Robin Stanton, Roger Clarke, Peter Drahos, and Malcolm Newey. A full technical report of the expert system, and a book further detailing its development and testing have also been published. SHYSTER emphasises its pragmatic approach, and posits that a legal expert system need not be based upon a complex model of legal reasoning in order to produce useful advice. Although SHYSTER attempts to model the way in which lawyers argue with cases, it does not attempt to model the way in which lawyers decide which cases to use in those arguments. SHYSTER is of a general design, permitting its operation in different legal domains. It was designed to provide advice in areas of case law that have been specified by a legal expert using a bespoke specification language. Its knowledge of the law is acquired, and represented, as information about cases. It produces its advice by examining, and arguing about, the similarities and differences between cases. It derives its name from Shyster: a slang word for someone who acts in a disreputable, unethical, or unscrupulous way, especially in the practice of law and politics."
"http://dbpedia.org/resource/Hierarchical_control_system"	"Hierarchical control system"	"Artificial intelligence"	"A hierarchical control system is a form of control system in which a set of devices and governing software is arranged in a hierarchical tree. When the links in the tree are implemented by a computer network, then that hierarchical control system is also a form of networked control system."
"http://dbpedia.org/resource/Informatics"	"Informatics"	"Artificial intelligence"	"Informatics is the science of information and computer information systems. As an academic field it involves the practice of information processing, and the engineering of information systems. The field considers the interaction between humans and information alongside the construction of interfaces, organisations, technologies and systems. It also develops its own conceptual and theoretical foundations and utilizes foundations developed in other fields. As such, the field of informatics has great breadth and encompasses many individual specializations, including disciplines of computer science, information systems, information technology and statistics. Since the advent of computers, individuals and organizations increasingly process information digitally. This has led to the study of informatics with computational, mathematical, biological, cognitive and social aspects, including study of the social impact of information technologies."
"http://dbpedia.org/resource/Percept_(artificial_intelligence)"	"Percept (artificial intelligence)"	"Artificial intelligence"	"A percept is the input that an intelligent agent is perceiving at any given moment. It is essentially the same concept as a percept in psychology, except that it is being perceived not by the brain but by the agent. A percept is detected by a sensor, often a camera, processed accordingly, and acted upon by an actuator. Each percept is added to a percept sequence, which is a complete history of each percept ever detected. An intelligent agent chooses how to act not only based on the current percept, but the percept sequence. The next action is chosen by the agent function, which maps every percept to an action. For example, if a camera were to record a gesture, the agent would process the percepts, calculate the corresponding spatial vectors, examine its percept history, and use the agent program (the application of the agent function) to act accordingly."
"http://dbpedia.org/resource/Artificial_brain"	"Artificial brain"	"Artificial intelligence"	"Artificial brain (or artificial mind) is a term commonly used in the media to describe research that aims to develop software and hardware with cognitive abilities similar to those of the animal or human brain. Research investigating ""artificial brains"" and brain emulation plays three important roles in science: 1.  
* An ongoing attempt by neuroscientists to understand how the human brain works, known as cognitive neuroscience. 2.  
* A thought experiment in the philosophy of artificial intelligence, demonstrating that it is possible, at least in theory, to create a machine that has all the capabilities of a human being. 3.  
* A long term project to create machines exhibiting behavior comparable to those of animals with complex central nervous system such as mammals and most particularly humans. The ultimate goal of creating a machine exhibiting human-like behavior or intelligence is sometimes called strong AI. An example of the first objective is the project reported by Aston University in Birmingham, England where researchers are using biological cells to create ""neurospheres"" (small clusters of neurons) in order to develop new treatments for diseases including Alzheimer's, Motor Neurone and Parkinson's Disease. The second objective is a reply to arguments such as John Searle's Chinese room argument, Hubert Dreyfus' critique of AI or Roger Penrose's argument in The Emperor's New Mind. These critics argued that there are aspects of human consciousness or expertise that can not be simulated by machines. One reply to their arguments is that the biological processes inside the brain can be simulated to any degree of accuracy. This reply was made as early as 1950, by Alan Turing in his classic paper ""Computing Machinery and Intelligence"". The third objective is generally called artificial general intelligence by researchers. However, Ray Kurzweil prefers the more memorable term strong AI. In his book The Singularity is Near, he focuses on whole brain emulation using conventional computing machines as an approach to implementing artificial brains, and claims (on grounds of computer power continuing an exponential growth trend) that this could be done by 2025. Henry Markram, director of the Blue Brain project (which is attempting brain emulation), made a similar claim (2020) at the Oxford TED conference in 2009."
"http://dbpedia.org/resource/Spreading_activation"	"Spreading activation"	"Artificial intelligence"	"Spreading activation is a method for searching associative networks, neural networks, or semantic networks. The search process is initiated by labeling a set of source nodes (e.g. concepts in a semantic network) with weights or ""activation"" and then iteratively propagating or ""spreading"" that activation out to other nodes linked to the source nodes. Most often these ""weights"" are real values that decay as activation propagates through the network. When the weights are discrete this process is often referred to as marker passing. Activation may originate from alternate paths, identified by distinct markers, and terminate when two alternate paths reach the same node. However brain studies show that several different brain areas play an important role in semantic processing. Spreading activation models are used in cognitive psychology to model the fan out effect. Spreading activation can also be applied in information retrieval, by means of a network of nodes representing documents and terms contained in those documents."
"http://dbpedia.org/resource/Artificial_Intelligence_System"	"Artificial Intelligence System"	"Artificial intelligence"	"Artificial Intelligence System (AIS) was a distributed computing project undertaken by Intelligence Realm, Inc. with the long-term goal of simulating the human brain in real time, complete with artificial consciousness and artificial general intelligence. They claimed to have found, in research, the ""mechanisms of knowledge representation in the brain which is equivalent to finding artificial intelligence"", before moving into the developmental phase."
"http://dbpedia.org/resource/Emily_Howell"	"Emily Howell"	"Artificial intelligence"	"Emily Howell is a computer program created by UC Santa Cruz professor of music David Cope. Emily Howell is an interactive interface that ""hears"" feedback from listeners, and builds its own musical compositions from a source database, derived from a previous composing program called Experiments in Musical Intelligence (EMI). Cope attempts to “teach” the program by providing feedback so that it can cultivate its own ""personal"" style. The software appears to be based on latent semantic analysis. Emily Howell’s first album was released in February 2009 by Centaur Records (CRC 3023). Titled From Darkness, Light, this album contains her Opus 1, Opus 2, and Opus 3 compositions for chamber orchestra and multiple pianos. Her second album Breathless was released in December 2012 by Centaur Records (CRC 3255)."
"http://dbpedia.org/resource/Subrata_Dasgupta"	"Subrata Dasgupta"	"Artificial intelligence"	"Subrata Dasgupta is a bi-cultural multidisciplinary scholar, scientist, and writer. Born in Calcutta (in 1944), he was educated in England, India, and Canada."
"http://dbpedia.org/resource/Histogram_of_oriented_displacements"	"Histogram of oriented displacements"	"Artificial intelligence"	"Histogram of Oriented Displacements (HOD) is a 2D trajectory descriptor. The trajectory is described using a histogram of the directions between each two consecutive points. Given a trajectory T = {P1, P2, P3, ..., Pn}, where Pt is the 2D position at time t. For each pair of positions Pt and Pt+1, calculate the direction angle θ(t, t+1). Value of θ is between 0 and 360. A histogram of the quantized values of θ is created. If the histogram is of 8 bins, the first bin represents all θs between 0 and 45. The histogram accumulates the lengths of the consecutive moves. For each θ, a specific histogram bin is determined. The length of the line between Pt and Pt+1 is then added to the specific histogram bin. To show the intuition behind the descriptor, consider the action of waving hands. At the end of the action, the hand falls down. When describing this down movement, the descriptor does not care about the position from which the hand started to fall. This fall will affect the histogram with the appropriate angles and lengths, regardless of the position where the hand started to fall. HOD records for each moving point: how much it moves in each range of directions. HOD has a clear physical interpretation. It proposes that, a simple way to describe the motion of an object, is to indicate how much distance it moves in each direction. If the movement in all directions are saved accurately, the movement can be repeated from the initial position to the final destination regardless of the displacements order. However, the temporal information will be lost, as the order of movements is not stored-this is what we solve by applying the temporal pyramid, as shown in section ef{sec:temp-pyramid}. If the angles quantization range is small, classifiers that use the descriptor will overfit. Generalization needs some slack in directions-which can be done by increasing the quantization range."
"http://dbpedia.org/resource/DragonLord_Enterprises,_Inc."	"DragonLord Enterprises, Inc."	"Artificial intelligence"	"DragonLord Enterprises, Inc. is an American corporation that develops games, mobile apps, and3D simulations. Under the Sequoia Consulting brand,it also specializes in robotics, machine learning,and applied artificial intelligence."
"http://dbpedia.org/resource/Knowledge-based_systems"	"Knowledge-based systems"	"Artificial intelligence"	"A knowledge-based system (KBS) is a computer program that reasons and uses a knowledge base to solve complex problems. The term is broad and is used to refer to many different kinds of systems. The one common theme that unites all knowledge based systems is an attempt to represent knowledge explicitly via tools such as ontologies and rules rather than implicitly via code the way a conventional computer program does. A knowledge based system has two types of sub-systems: a knowledge base and an inference engine. The knowledge base represents facts about the world, often in some form of subsumption ontology. The inference engine represents logical assertions and conditions about the world, usually represented via IF-THEN rules."
"http://dbpedia.org/resource/Ordered_weighted_averaging_aggregation_operator"	"Ordered weighted averaging aggregation operator"	"Artificial intelligence"	"In applied mathematics – specifically in fuzzy logic – the ordered weighted averaging (OWA) operators provide a parameterized class of mean type aggregation operators. They were introduced by Ronald R. Yager. Many notable mean operators such as the max, arithmetic average, median and min, are members of this class. They have been widely used in computational intelligence because of their ability to model linguistically expressed aggregation instructions."
"http://dbpedia.org/resource/ADS-AC"	"ADS-AC"	"Artificial intelligence"	"ADS-AC is an experimental Open Source program which implements Absolutely Dynamic System, a proposed mechanism for AC."
"http://dbpedia.org/resource/Babelfy"	"Babelfy"	"Artificial intelligence"	"Babelfy is an algorithm for the disambiguation of text written in any language. Specifically, Babelfy performs the tasks of multilingual Word Sense Disambiguation (i.e., the disambiguation of common nouns, verbs, adjectives and adverbs) and Entity Linking (i.e. the disambiguation of mentions to encyclopedic entities like people, companies, places, etc.). Babelfy is based on the BabelNet multilingual semantic network and performs disambiguation and entity linking in three steps: 
*  It associates with each vertex of the BabelNet semantic network, i.e., either concept or named entity, a semantic signature, that is, a set of related vertices. This is a preliminary step which needs to be performed only once, independently of the input text. 
*  Given an input text, it extracts all the linkable fragments from this text and, for each of them, lists the possible meanings according to the semantic network. 
*  It creates a graph-based semantic interpretation of the whole text by linking the candidate meanings of the extracted fragments using the previously-computed semantic signatures. It then extracts a dense subgraph of this representation and selects the best candidate meaning for each fragment. As a result, the text, written in any of the 271 languages supported by BabelNet, is output with possibly overlapping semantic annotations."
"http://dbpedia.org/resource/Clone_Algo_Inc"	"Clone Algo Inc"	"Artificial intelligence"	"Clone Algo Inc is an American multinational corporation headquartered in Las Vegas, Nevada, US. A technology company, it primarily creates algorithms based on artificial intelligence for mobile applications. Clone Algo Inc issued 707,646,696 shares of common stock and 150,000,000 shares of preferred stock in its preliminary filing."
"http://dbpedia.org/resource/Emospark"	"Emospark"	"Artificial intelligence"	"EmoSPARK is an artificial intelligence console created in London, United Kingdom by Patrick Levy-Rosenthal. The device uses facial recognition and language analysis to evaluate human emotion and convey responsive content according to the emotion. The console measures 90 mm x 90 mm x 90 mm and is cube shaped. It operates on an ""Emotional Processing Unit"", a microchip that enables the system to create emotional profile graphs of its surroundings. The emotional processing unit is a patent pending technology that is said to create synthesised emotional responses in machines. EmoSPARK was funded through an Indiegogo campaign which aimed to raise $200,000."
"http://dbpedia.org/resource/Uncanny_valley"	"Uncanny valley"	"Artificial intelligence"	"In aesthetics, the uncanny valley is the hypothesis that human replicas that appear almost, but not exactly, like real human beings elicit feelings of eeriness and revulsion among some observers. Valley denotes a dip in the human observer's affinity for the replica, a relation that otherwise increases with the replica's human likeness. Examples can be found in robotics, 3D computer animations, and life-like dolls among others."
"http://dbpedia.org/resource/Mindpixel"	"Mindpixel"	"Artificial intelligence"	"Mindpixel was a web-based collaborative artificial intelligence project which aimed to create a knowledgebase of millions of human validated true/false statements, or probabilistic propositions. It ran from 2000 to 2005."
"http://dbpedia.org/resource/Dartmouth_Conferences"	"Dartmouth Conferences"	"Artificial intelligence"	"The Dartmouth Summer Research Project on Artificial Intelligence was the name of a 1956 summer workshop now considered by many(though not all)to be the seminal event for artificial intelligence as a field. The project lasted approximately 6 to 8 weeks, and was essentially an extended brainstorming session. 11 mathematicians and scientists were originally planned to be attendees, and while not all attended, more than 10 others came for short times."
"http://dbpedia.org/resource/Smart_objects"	"Smart objects"	"Artificial intelligence"	"A Smart Object is an object that enhances the interaction with not only people but also with other Smart Objects. It can not only refer to interaction with physical world objects but also to interaction with virtual (computing environment) objects. A smart physical object may be created either as an artifact or manufactured product or by embedding electronic tags such as RFID tags or sensors into non-smart physical objects. Smart virtual objects are created as software objects that are intrinsic when creating and operating a virtual or cyber world simulation or game. The concept of a smart object has several origins and uses, see History. There are also several overlapping terms, see also Smart device, Tangible Object or Tangible User Interface and Thing as in the Internet of Things."
"http://dbpedia.org/resource/Zeuthen_strategy"	"Zeuthen strategy"	"Artificial intelligence"	"The Zeuthen strategy is a negotiation strategy used by some artificial agents. Its purpose is to measure the willingness to risk conflict. An agent will be more willing to risk conflict if the difference in utility between its current proposal and the conflict deal is low. When used by both agents in the Monotonic Concession Protocol, the Zeuthen strategy leads the agents to agree upon the deal in the negotiation set, the set of all conflict free deals which are individually rational and Pareto optimal plus the conflict deal, which maximizes the Nash product."
"http://dbpedia.org/resource/Cognitive_computing"	"Cognitive computing"	"Artificial intelligence"	"Cognitive computing (CC) describes technology platforms that, broadly speaking, are based on the scientific disciplines of Artificial Intelligence and Signal Processing. These platforms encompass machine learning, reasoning, natural language processing, speech and vision, human-computer interaction, dialog and narrative generation and more."
"http://dbpedia.org/resource/Inductive_programming"	"Inductive programming"	"Artificial intelligence"	"Inductive programming (IP) is a special area of automatic programming, covering research from artificial intelligence and programming, which addresses learning of typically declarative (logic or functional) and often recursive programs from incomplete specifications, such as input/output examples or constraints. Depending on the programming language used, there are several kinds of inductive programming. Inductive functional programming, which uses functional programming languages such as Lisp or Haskell, and most especially inductive logic programming, which uses logic programming languages such as Prolog and other logical representations such as description logics, have been more prominent, but other (programming) language paradigms have also been used, such as constraint programming or probabilistic programming."
"http://dbpedia.org/resource/Contextual_image_classification"	"Contextual image classification"	"Artificial intelligence"	"Contextual image classification, a topic of pattern recognition in computer vision, is an approach of classification based on contextual information in images. ""Contextual"" means this approach is focusing on the relationship of the nearby pixels, which is also called neighbourhood. The goal of this approach is to classify the images by using the contextual information."
"http://dbpedia.org/resource/Darkforest"	"Darkforest"	"Artificial intelligence"	"Darkforest is a computer go program developed by Facebook, based on deep learning techniques using a convolutional neural network. Its updated version Darkfores2 combines the techniques of its predecessor with Monte Carlo tree search. The MCTS effectively takes tree search methods commonly seen in computer chess programs and randomizes them. With the update, the system is known as Darkfmcts3. Darkforest is of similar strength to programs like CrazyStone and Zen. It has not been tested an against professional human player, however, Google's AlphaGo program won against a professional player in October 2015 using a similar combination of techniques . Darkforest is named after Liu Cixin's science fiction novel The Dark Forest."
"http://dbpedia.org/resource/Stochastic_semantic_analysis"	"Stochastic semantic analysis"	"Artificial intelligence"	"Stochastic semantic analysis is an approach used in computer science as a semantic component of natural language understanding. Stochastic models generally use the definition of segments of words as basic semantic units for the semantic models, and in some cases involve a two layered approach. Example applications have a wide range. In machine translation, it has been applied to the translation of spontaneous conversational speech among different languages. In the area of spoken language understanding the fact that spoken sentences often do not follow the grammar of a language and involve self-corrections, repetitions, and other irregularities, the use of stochastic semantic has been suggested as a natural fit to achieve robustness to deal with noise due to the spontaneous nature of spoken language."
"http://dbpedia.org/resource/Behavior_informatics"	"Behavior informatics"	"Artificial intelligence"	"Behavior informatics (BI) is the informatics of behaviors so as to obtain behavior intelligence and behavior insights. Different from applied behavior analysis  from the psychological perspective, BI builds computational theories, systems and tools to qualitatively and quantitatively model, represent, analyze, and manage behaviors of individuals, groups and/or organizations . BI is built on classic study of behavioral science, including behavior modeling, applied behavior analysis, behavior analysis, behavioral economics, and organizational behavior. Typical BI tasks consist of individual and group behavior formation, representation, computational modeling, analysis, learning, simulation, and understanding of behavior impact, utility, non-occurring behaviors etc. for behavior intervention and management."
"http://dbpedia.org/resource/Artificial_empathy"	"Artificial empathy"	"Artificial intelligence"	"Artificial empathy (AE) is considered to be the next step in the development of artificial intelligence, with companion robots becoming able to detect and respond to human emotions. According to scientists, although the technology can be perceived as scary or threatening by many people, it could also have a significant advantage over humans in professions which are traditionally involved in emotional role-playing such as the health care sector. From the care-giver perspective for instance, performing emotional labor above and beyond the requirements of paid labor often results in chronic stress or burnout, and the development of a feeling of being desensitized to patients. However, it is argued that the emotional role-playing between the care-receiver and a robot can actually have a more positive outcome in terms of creating the conditions of less fear and concern for one's own predicament best exemplified by the phrase: ""if it is just a robot taking care of me it cannot be that critical."" Scholars debate the possible outcome of such technology using two different perspectives. Either, the AE could help the socialization of care-givers, or serve as role model for emotional detachment."
"http://dbpedia.org/resource/Cognitive_philology"	"Cognitive philology"	"Artificial intelligence"	"Cognitive philology is the science that studies written and oral texts as the product of human mental processes. Studies in cognitive philology compare documentary evidence emerging from textual investigations with results of experimental research, especially in the fields of cognitive and ecological psychology, neurosciences and artificial intelligence. ""The point is not the text, but the mind that made it"". Cognitive Philology aims to foster communication between literary, textual, philological disciplines on the one hand and researches across the whole range of the cognitive, evolutionary, ecological and human sciences on the other. Cognitive philology: 
*  investigates transmission of oral and written text, and categorization processes which lead to classification of knowledge, mostly relying on the information theory; 
*  studies how narratives emerge in so called natural conversation and selective process which lead to the rise of literary standards for storytelling, mostly relying on embodied semantics; 
*  explores the evolutive and evolutionary role played by rhythm and metre in human ontogenetic and phylogenetic development and the pertinence of the semantic association during processing of cognitive maps; 
*  Provides the scientific ground for multimedia  critical editions of literary texts. Among the founding thinkers and noteworthy scholars devoted to such investigations are: 
* Gilles Fauconnier, A professor in Cognitive science at the University of California, San Diego. He was one of the founders of cognitive linguistics in the 1970s through his work on pragmatic scales and mental spaces. His research explores the areas of conceptual integration and compressions of conceptual mappings in terms of the emergent structure in language. 
* Alan Richardson: Studies Theory of Mind in early-modern and contemporary literature.  
* David Herman: Professor of English at North Carolina State University and an adjunct professor of linguistics at Duke University. He is the author of ""Universal Grammar and Narrative Form"" and the editor of ""Narratologies: New Perspectives on Narrative Analysis 
* Mark Turner 
* Benoît de Cornulier  
* François Recanati 
* Manfred Jahn in Germany  
* Paolo Canettieri 
* Domenico Fiormonte  
* Anatole Pierre Fuksas  
* Luca Nobile  
* Julián Santano Moreno"
"http://dbpedia.org/resource/Grammar_systems_theory"	"Grammar systems theory"	"Artificial intelligence"	"Grammar systems theory is a field of theoretical computer science that studies systems of finite collections of formal grammars generating a formal language. Each grammar works on a string, a so-called sequential form that represents an environment. Grammar systems can thus be used as a formalization of decentralized or distributed systems of agents in artificial intelligence. Let be a simple reactive agent moving on the table and trying not to fall down from the table with two reactions, t for turning and ƒ for moving forward. The set of possible behaviors of can then be described as formal language where ƒ can be done maximally k times and t can be done maximally ℓ times considering the dimensions of the table. Let be a formal grammar which generates language . The behavior of is then described by this grammar. Suppose the has a subsumption architecture; each component of this architecture can be then represented as a formal grammar too and the final behavior of the agent is then described by this system of grammars. The schema on the right describes such a system of grammars which shares a common string representing an environment. The shared sequential form is sequentially rewritten by each grammar, which can represent either a component or generally an agent. If grammars communicate together and work on a shared sequential form, it is called a Cooperating Distributed (DC) grammar system. Shared sequential form is a similar concept to the blackboard approach in AI, which is inspired by an idea of experts solving some problem together while they share their proposals and ideas on a shared blackboard. Each grammar in a grammar system can also work on its own string and communicate with other grammars in a system by sending their sequential forms on request. Such a grammar system is then called a Parallel Communicating (PC) grammar system. PC and DC are inspired by distributed AI. If there is no communication between grammars, the system is close to the decentralized approaches in AI. These kinds of grammar systems are sometimes called colonies or Eco-Grammar systems, depending (besides others) on whether the environment is changing on its own (Eco-Grammar system) or not (colonies)."
"http://dbpedia.org/resource/Type-2_fuzzy_sets_and_systems"	"Type-2 fuzzy sets and systems"	"Artificial intelligence"	"Type-2 fuzzy sets and systems generalize Type-1 fuzzy sets and systems so that more uncertainty can be handled. From the very beginning of fuzzy sets, criticism was made about the fact that the membership function of a type-1 fuzzy set has no uncertainty associated with it, something that seems to contradict the word fuzzy, since that word has the connotation of lots of uncertainty. So, what does one do when there is uncertainty about the value of the membership function? The answer to this question was provided in 1975 by the inventor of fuzzy sets, Prof. Lotfi A. Zadeh [27], when he proposed more sophisticated kinds of fuzzy sets, the first of which he called a type-2 fuzzy set. A type-2 fuzzy set lets us incorporate uncertainty about the membership function into fuzzy set theory, and is a way to address the above criticism of type-1 fuzzy sets head-on. And, if there is no uncertainty, then a type-2 fuzzy set reduces to a type-1 fuzzy set, which is analogous to probability reducing to determinism when unpredictability vanishes,. In order to symbolically distinguish between a type-1 fuzzy set and a type-2 fuzzy set, a tilde symbol is put over the symbol for the fuzzy set; so, A denotes a type-1 fuzzy set, whereas Ã denotes the comparable type-2 fuzzy set. When the latter is done, the resulting type-2 fuzzy set is called a general type-2 fuzzy set (to distinguish it from the special interval type-2 fuzzy set). Prof. Zadeh didn't stop with type-2 fuzzy sets, because in that 1976 paper [27] he also generalized all of this to type-n fuzzy sets. The present article focuses only on type-2 fuzzy sets because they are the next step in the logical progression from type-1 to type-n fuzzy sets, where n = 1, 2, … . Although some researchers are beginning to explore higher than type-2 fuzzy sets, as of early 2009, this work is in its infancy. The membership function of a general type-2 fuzzy set, Ã, is three-dimensional (Fig. 1), where the third dimension is the value of the membership function at each point on its two-dimensional domain that is called its footprint of uncertainty (FOU). For an interval type-2 fuzzy set that third-dimension value is the same (e.g., 1) everywhere, which means that no new information is contained in the third dimension of an interval type-2 fuzzy set. So, for such a set, the third dimension is ignored, and only the FOU is used to describe it. It is for this reason that an interval type-2 fuzzy set is sometimes called a first-order uncertainty fuzzy set model, whereas a general type-2 fuzzy set (with its useful third-dimension) is sometimes referred to as a second-order uncertainty fuzzy set model. The FOU represents the blurring of a type-1 membership function, and is completely described by its two bounding functions (Fig. 2), a lower membership function (LMF) and an upper membership function (UMF), both of which are type-1 fuzzy sets! Consequently, it is possible to use type-1 fuzzy set mathematics to characterize and work with interval type-2 fuzzy sets. This means that engineers and scientists who already know type-1 fuzzy sets will not have to invest a lot of time learning about general type-2 fuzzy set mathematics in order to understand and use interval type-2 fuzzy sets. Work on type-2 fuzzy sets languished during the 1980s and early-to-mid 1990's, although a small number of articles were published about them. People were still trying to figure out what to do with type-1 fuzzy sets, so even though Zadeh proposed type-2 fuzzy sets in 1976, the time was not right for researchers to drop what they were doing with type-1 fuzzy sets to focus on type-2 fuzzy sets. This changed in the latter part of the 1990s as a result of Prof. Jerry Mendel and his student's works on type-2 fuzzy sets and systems (e.g., [12]). Since then, more and more researchers around the world are writing articles about type-2 fuzzy sets and systems."
"http://dbpedia.org/resource/Concurrent_MetateM"	"Concurrent MetateM"	"Artificial intelligence"	"Concurrent MetateM is a multi-agent language in which each agent is programmed using a set of (augmented) temporal logic specifications of the behaviour it should exhibit. These specifications are executed directly to generate the behaviour of the agent. As a result, there is no risk of invalidating the logic as with systems where logical specification must first be translated to a lower-level implementation. The root of the MetateM concept is Gabbay's separation theorem; any arbitrary temporal logic formula can be rewritten in a logically equivalent past → future form. Execution proceeds by a process of continually matching rules against a history, and firing those rules when antecedents are satisfied. Any instantiated future-time consequents become commitments which must subsequently be satisfied, iteratively generating a model for the formula made up of the program rules."
"http://dbpedia.org/resource/CSHALS"	"CSHALS"	"Artificial intelligence"	"The Conference on Semantics in Healthcare and Life Sciences (CSHALS) is a scientific meeting on the practical applications of semantic technology to pharmaceutical R&D, healthcare, and life sciences. It has been held annually since 2008, and is the premier meeting in this domain. In 2012, CSHALS took on the additional topic of the application of Big Data to healthcare and life sciences, while maintaining its previous focus on semantic technologies."
"http://dbpedia.org/resource/DAYDREAMER"	"DAYDREAMER"	"Artificial intelligence"	"DAYDREAMER is a goal-based agent and cognitive architecture developed at University of California, Los Angeles by Erik Mueller. It models the human stream of thought and its triggering and direction by emotions, as in human daydreaming. The architecture is implemented as 12,000 lines of Lisp code."
"http://dbpedia.org/resource/Multi-Agent_Programming_Contest"	"Multi-Agent Programming Contest"	"Artificial intelligence"	"The Multi-Agent Programming Contest is an annual international programming competition with stated goal of stimulating research in the area of multi-agent system development and programming."
"http://dbpedia.org/resource/Bayesian_programming"	"Bayesian programming"	"Artificial intelligence"	"Bayesian programming is a formalism and a methodology to specify probabilistic models and solve problems when less than the necessary information is available. Edwin T. Jaynes proposed that probability could be considered as an alternative and an extension of logic for rational reasoning with incomplete and uncertain information. In his founding book Probability Theory: The Logic of Science he developed this theory and proposed what he called “the robot,” which was nota physical device, but an inference engine to automate probabilistic reasoning—a kind of Prolog for probability instead of logic. Bayesian programming is a formal and concrete implementation of this ""robot"". Bayesian programming may also be seen as an algebraic formalism to specify graphical models such as, for instance, Bayesian networks, dynamic Bayesian networks, Kalman filters or hidden Markov models. Indeed, Bayesian Programming is more general than Bayesian networks and has a power of expression equivalent to probabilistic factor graphs."
"http://dbpedia.org/resource/Behavior_tree_(artificial_intelligence,_robotics_and_control)"	"Behavior tree (artificial intelligence, robotics and control)"	"Artificial intelligence"	"A behavior tree (BT) is a mathematical model of plan execution used in computer science, robotics, control systems and video games. They describe switchings between a finite set of tasks in a modular fashion. Their strength comes from their ability to create very complex tasks composed of simple tasks, without worrying how the simple tasks are implemented. BTs present some similarities to hierarchical state machines with the key difference that the main building block of a behavior is a task rather than a state. Its ease of human understanding make BTs less error prone and very popular in the game developer community."
"http://dbpedia.org/resource/Deductive_classifier"	"Deductive classifier"	"Artificial intelligence"	"A deductive classifier is a type of artificial intelligence inference engine. It takes as input a set of declarations in a frame language about a domain such as medical research or molecular biology. For example, the names of classes, sub-classes, properties, and restrictions on allowable values. The classifier determines if the various declarations are logically consistent and if not will highlight the specific inconsistent declarations and the inconsistencies among them. If the declarations are consistent the classifier can then assert additional information based on the input. For example, it can add information about existing classes, create additional classes, etc. This differs from traditional inference engines that trigger off of IF-THEN conditions in rules. Classifiers are also similar to theorem provers in that they take as input and produce output via First Order Logic. Classifiers originated with KL-ONE Frame languages. They are increasingly significant now that they form a part in the enabling technology of the Semantic Web. Modern classifiers leverage the Web Ontology Language. The models they analyze and generate are called ontologies."
"http://dbpedia.org/resource/Brain_technology"	"Brain technology"	"Artificial intelligence"	"Brain technology, or self-learning know-how systems, defines a technology that employs latest findings in neuroscience. The term was first introduced by the Artificial Intelligence Laboratory in Zurich, Switzerland, in the context of the Roboy project. Brain Technology can be employed in robots, know-how management systems and any other application with self-learning capabilities. In particular, Brain Technology applications allow the visualization of the underlying learning architecture often coined as “know-how maps”."
"http://dbpedia.org/resource/Incremental_heuristic_search"	"Incremental heuristic search"	"Artificial intelligence"	"Incremental heuristic search algorithms combine both incremental and heuristic search to speed up searches of sequences of similar search problems, which is important in domains that are only incompletely known or change dynamically. Incremental search has been studied at least since the late 1960s. Incremental search algorithms reuse information from previous searches to speed up the current search and solve search problems potentially much faster than solving them repeatedly from scratch. Similarly, heuristic search has also been studied at least since the late 1960s. Heuristic search algorithms, often based on A*, use heuristic knowledge in the form of approximations of the goal distances to focus the search and solve search problems potentially much faster than uninformed search algorithms. The resulting search problems, sometimes called dynamic path planning problems, are graph search problems where paths have to be found repeatedly because the topology of the graph, its edge costs, the start vertex or the goal vertices change over time. So far, three main classes of incremental heuristic search algorithms have been developed: 
*  The first class restarts A* at the point where its current search deviates from the previous one (example: Fringe Saving A*). 
*  The second class updates the h-values from the previous search during the current search to make them more informed (example: Generalized Adaptive A*). 
*  The third class updates the g-values from the previous search during the current search to correct them when necessary, which can be interpreted as transforming the A* search tree from the previous search into the A* search tree for the current search (examples: Lifelong Planning A*, D*, D* Lite). All three classes of incremental heuristic search algorithms are different from other replanning algorithms, such as planning by analogy, in that their plan quality does not deteriorate with the number of replanning episodes."
"http://dbpedia.org/resource/Machine_listening"	"Machine listening"	"Artificial intelligence"	"Machine listening is a technique using software and hardware to extract meaningful information from audio signals. The engineer Paris Smaragdis, interviewed in Technology Review, talks about these systems --""software that uses sound to locate people moving through rooms, monitor machinery for impending breakdowns, or activate traffic cameras to record accidents."" Since audio signals are interpreted by the human ear-brain system, that complex perceptual mechanism should be simulated somehow in software for ""machine listening"". In other words, to perform on par with humans, the computer should hear and understand audio content much as humans do. Analyzing audio accurately involves several fields: electrical engineering (spectrum analysis, filtering, and audio transforms); artificial intelligence (machine learning and sound classification); psychoacoustics (sound perception); cognitive sciences (neuroscience and artificial intelligence); acoustics (physics of sound production); and music (harmony, rhythm, and timbre). Furthermore, audio transformations such as pitch shifting, time stretching, and sound object filtering, should be perceptually and musically meaningful. For best results, these transformations require perceptual understanding of spectral models, high-level feature extraction, and sound analysis/synthesis. Finally, structuring and coding the content of an audio file (sound and metadata) could benefit from efficient compression schemes, which discard inaudible information in the sound. Computational models of music and sound perception and cognition can lead to a more meaningful representation, a more intuitive digital manipulation and generation of sound and music in musical human-machine interfaces. Machine listening is a recent research field and many research groups are currently working on this area, including the Medical intelligence and language engineering lab at the Department of Electrical Engineering, Indian Institute of Science, Bangalore, India and the Audio Analysis Lab at Aalborg University, Denmark."
"http://dbpedia.org/resource/Committee_machine"	"Committee machine"	"Artificial intelligence"	"A committee machine is a type of artificial neural network using a divide and conquer strategy in which the responses of multiple neural networks (experts) are combined into a single response. The combined response of the committee machine is supposed to be superior to those of its constituent experts. Compare with ensembles of classifiers."
"http://dbpedia.org/resource/Computer_audition"	"Computer audition"	"Artificial intelligence"	"Inspired by models of human audition, CA deals with questions of representation, transduction, grouping, use of musical knowledge and general sound semantics for the purpose of performing intelligent operations on audio and music signals by the computer. Technically this requires a combination of methods from the fields of signal processing, auditory modelling, music perception and cognition, pattern recognition, and machine learning, as well as more traditional methods of artificial intelligence for musical knowledge representation."
"http://dbpedia.org/resource/Autognostics"	"Autognostics"	"Artificial intelligence"	"Autognostics is a new paradigm that describes the capacity for computer networks to be self-aware. It is considered as one of the major components of Autonomic Networking."
"http://dbpedia.org/resource/Perceptual_computing"	"Perceptual computing"	"Artificial intelligence"	"Perceptual computing is an application of Zadeh's theory of computing with words on the field of assisting people to make subjective judgments."
"http://dbpedia.org/resource/Competitions_and_prizes_in_artificial_intelligence"	"Competitions and prizes in artificial intelligence"	"Artificial intelligence"	"There are a number of competitions and prizes to promote research in artificial intelligence."
"http://dbpedia.org/resource/Artificial_psychology"	"Artificial psychology"	"Artificial intelligence"	"Artificial psychology is a theoretical discipline proposed by Dan Curtis (b. 1963). The theory considers the situation when an artificial intelligence approaches the level of complexity where the intelligence meets two conditions: Condition I 
*  A: Makes all of its decisions autonomously 
*  B: Is capable of making decisions based on information that is 1.  
*  New 2.  
*  Abstract 3.  
*  Incomplete 
*  C: The artificial intelligence is capable of reprogramming itself based on the new data 
*  D: And is capable of resolving its own programming conflicts, even in the presence of incomplete data. This means that the intelligence autonomously makes value-based decisions, referring to values that the intelligence has created for itself. Condition II 
*  All four criteria are met in situations that are not part of the original operating program When both conditions are met, then, according to this theory, the possibility exists that the intelligence will reach irrational conclusions based on real or created information. At this point, the criteria is met for intervention which will not necessarily be resolved by simple re-coding of processes due to extraordinarily complex nature of the codebase itself; but rather a discussion with the intelligence in a format which more closely resembles classical (human) psychology. If the intelligence cannot be reprogrammed by directly inputting new code, but requires the intelligence to reprogram itself through a process of analysis and decision based on information provided by a human, in order for it to overcome behavior which is inconsistent with the machines purpose or ability to function normally, then artificial psychology is by definition, what is required. The level of complexity that is required before these thresholds are met is currently a subject of extensive debate. The theory of artificial psychology does not address the specifics of what those levels may be, but only that the level is sufficiently complex that the intelligence cannot simply be recoded by a software developer, and therefore dysfunctionality must be addressed through the same processes that humans must go through to address their own dysfunctionalities. Along the same lines, artificial psychology does not address the question of whether or not the intelligence is conscious. As of 2015, the level of artificial intelligence does not approach any threshold where any of the theories or principles of artificial psychology can even be tested, and therefore, artificial psychology remains a largely theoretical discipline."
"http://dbpedia.org/resource/Knowledge_compilation"	"Knowledge compilation"	"Artificial intelligence"	"Knowledge compilation is a family of approaches for addressing the intractability ofa number of artificial intelligence problems. A propositional model is compiled in an off-line phase in order to support some queries in polytime. Many ways of compiling a propositional models exist.Among others: NNF, DNNF, d-DNNF, BDD, SDD, MDD, DNF and CNF. Different compiled representations have different properties.The three main properties are: 
*  The compactness of the representation 
*  The queries that are supported in polytime 
*  The transformations of the representations that can be performed in polytime"
"http://dbpedia.org/resource/Allen_(robot)"	"Allen (robot)"	"Artificial intelligence"	"Allen was a robot introduced by Rodney Brooks and his team in the late 1980s, and was their first robot based on subsumption architecture. It had sonar distance and odometry on board, and used an offboard lisp machine to simulate subsumption architecture. It resembled a footstool on wheels. Allen used three layers of control which are implemented in subsumption architecture. ""The lowest layer of control makes sure that the robot does not come into contact with other objects."" Due to this layer it could avoid static and dynamic obstacles, but it could not move. It sat in the middle of the room, waiting for obstruction. When the obstruction came, Allen ran away, avoiding collisions as it went. It used following internal representation, and every sonar return represented a repulsive force with, and inverse square drop off in strength. Direction of its move was obtained by sum of the repulsive forces (suitably thresholded). It possessed an additional reflex which halted it whenever it was moving forward, and something was directly in its path. ""The first level layer of control (second layer), when combined with zeroth, imbues the robot with the ability to wander around aimlessly without hitting obstacles."" Owing to the second layer, Allen could randomly wander about every 10 seconds. It used simple heuristic, which was coupled with the instinct to shun barriers by vector addition. ""The summed vector suppressed the more primitive obstacle avoidance vector, but the obstacle avoidance behaviour still operated, having been subsumed by the new layer, in its account of the lower level's repulsive force. Additionally, the halt reflex of the lower level operated autonomously and unchanged."" The third layer made the robot try to explore. Allen could look for distant places (with its sonars), then tried to reach them. ""This layer monitored progress through odometry, generating a desired heading which suppressed the direction desired by the wander layer. The desired heading was then fed into a vector addition with the instinctive obstacle avoidance layer. The physical robot did not therefore remain true to the desires of the upper layer. The upper layer had to watch what happened in the world, through odometry, in order to understand what was really happening in the lower control layers, and send down correction signals."""
"http://dbpedia.org/resource/Manifold_integration"	"Manifold integration"	"Artificial intelligence"	"Manifold integration is a combined concept of manifold learning and data integration, or an extension of manifold learning for multiple measurements. Various manifold learning methods have been developed. However, they consider only one dissimilarity matrix corresponding to one kernel matrix, which represents one manifold of the data set. In practice, however, multiple sensors are used at a time, and each sensor generates data set on one manifold. In such a case, manifold integration is a desirable task, combining these dissimilarity matrices into a compromise matrix that faithfully reflects multiple sensory information on one integrated manifold. For more information, see"
"http://dbpedia.org/resource/Computational_Heuristic_Intelligence"	"Computational Heuristic Intelligence"	"Artificial intelligence"	"Computational Heuristic Intelligence(CHI) is a name for describing specialized programming techniques in the field of Computational intelligence (also called Artificial Intelligence, or AI). These techniques have the express goal of avoiding complexity issues, also called NP-hard problems, by using human-like techniques. They are best summarized as the use of exemplar-based methods (Heuristics), rather than rule-based methods(Algorithms). Hence the term is distinct from the more conventional Computational Algorithmic Intelligence, or GOFAI. An example of a CHI technique is the Encoding specificity principle of Tulving and Thompson. In general, CHI principles are problem solving techniques used by people, rather than programmed into machines. It is by drawing attention to this key distinction that the use of this term is justified in a field already replete with confusing neologisms. Note that the legal systems of all modern human societies employ both heuristics (generalisations of cases) from individual trial records as well as legislated statutes (rules) as regulatory guides. Another recent approach to the avoidance of complexity issues is to employ feedback control rather than feedforward modeling as a problem-solving paradigm. This approach has been called Computational cybernetics, because (a) the term 'computational' is associated with conventional computer programming techniques which represent a strategic, compiled, or feedforward model of the problem, and (b) the term 'cybernetic' is associated with conventional system operation techniques which represent a tactical, interpreted, or feedback model of the problem. Of course, real programs and real problems both contain both feedforward and feedback components. A real example which illustrates this point is that of human cognition, which clearly involves both perceptual ('bottom-up', feedback, sensor-oriented) and conceptual ('top-down', feedforward, motor-oriented) information flows and hierarchies."
"http://dbpedia.org/resource/MANIC_(Cognitive_Architecture)"	"MANIC (Cognitive Architecture)"	"Artificial intelligence"	"MANIC, formerly known as PMML.1, is a cognitive architecture developed by the predictive modeling and machine learning laboratory at University Of Arkansas. It differs from other cognitive architectures in that it tries to ""minimize novelty"". That is, it attempts to organize well-established techniques in computer science, rather than propose any new methods for achieving cognition. While most other cognitive architectures are inspired by some neurological observation, and are subsequently developed in a top-down manner to behave in some manner like a brain, MANIC is inspired only by common practices in computer science, and was developed in a bottom-up manner for the purpose of unifying various methods in machine learning and artificial intelligence."
"http://dbpedia.org/resource/List_of_programming_languages_for_artificial_intelligence"	"List of programming languages for artificial intelligence"	"Artificial intelligence"	"Artificial intelligence researchers have developed several specialized programming languages for artificial intelligence:"
"http://dbpedia.org/resource/Cognitive_computer"	"Cognitive computer"	"Artificial intelligence"	"A cognitive computer combines artificial intelligence and machine-learning algorithms, in an approach which attempts to reproduce the behaviour of the human brain. An example is provided by the IBM company's Watson machine. A subsequent development by IBM is the TrueNorth microchip architecture, which is designed to be closer in structure to the human brain that the von Neumann architecture used in conventional computers. The IBM cognitive computers implement learning using Hebbian theory. Instead of being programmable in a traditional sense within machine language or a higher level programming language such a device learns by inputting instances through an input device that are aggregated within a computational convolution or neural network architecture consisting of weights within a parallel memory system. An early instantiation of such a device has been developed in 2012 under the Darpa SyNAPSE program at IBM directed by Dharmendra Modha."
"http://dbpedia.org/resource/Kuwahara_filter"	"Kuwahara filter"	"Artificial intelligence"	"Suppose that is a grey scale image and that we take a square window of size centered around a point in the image. This square can be divided into four smaller square regions each of which will be where is the cartesian product. It must be noted that pixels located on the borders between two regions belong to both regions so there is a slight overlap between subregions. The arithmetic mean and standard deviation of the four regions centered around a pixel (x,y) are calculated and used to determine the value of the central pixel. The output of the kuwahara filter for any point is then given by This means that the central pixel will take the mean value of the area that is most homogenous. The location of the pixel in relation to an edge plays a great role in determining which region will have the greater standard deviation. If for example the pixel is located on a dark side of an edge it will most probably take the mean value of the dark region. On the other hand, should the pixel be on the lighter side of an edge it will most probably take a light value. On the event that the pixel is located on the edge it will take the value of the more smooth, least textured region. The fact that the filter takes into account the homogeneity of the regions ensures that it will preserve the edges while using the mean creates the blurring effect. Similarly to the Median filter the Kuwahara filter uses a sliding window approach to access every pixel in the image. The size of the window is chosen in advance and may vary depending on the desired level of blur in the final image. Bigger windows typically result in the creation of more abstract images whereas small windows produce images that retain their detail. Typically windows are chose to be square with sides that have an odd number of pixels for symmetry. However, there are variations of the Kuwahara filter that use rectangular windows. Additionally, the subregions do not need to overlap or have the same size as long as they cover all of the window."
"http://dbpedia.org/resource/Puckstering"	"Puckstering"	"Artificial intelligence"	"Puckstering is a term used in gaming that refers to an operator controlling a group of AI entities. For example, a gamer playing the role of an army platoon sergeant could control a platoon of soldiers. He could command them to attack an enemy, which they would do autonomously. This is referred to as ""puckstering."""
"http://dbpedia.org/resource/Cloud_robotics"	"Cloud robotics"	"Artificial intelligence"	"Cloud robotics is a field of robotics that attempts to invoke cloud technologies such as cloud computing, cloud storage, and other Internet technologies centred on the benefits of converged infrastructure and shared services for robotics. When connected to the cloud, robots can benefit from the powerful computational, storage, and communications resources of modern data centre in the cloud, which can process and share information from various robots or agent (other machines, smart objects, humans, etc.). Humans can also delegate tasks to robots remotely through networks. Cloud computing technologies enable robot systems to be endowed with powerful capability whilst reducing costs through cloud technologies. Thus, it is possible to build lightweight, low cost, smarter robots have intelligent ""brain"" in the cloud. The ""brain"" consists of data center, knowledge base, task planners, deep learning, information processing, environment models, communication support etc."
"http://dbpedia.org/resource/Any-angle_path_planning"	"Any-angle path planning"	"Artificial intelligence"	"Any-angle path planning algorithms search for paths on a cell decomposition of a continuous configuration space (such as a two-dimensional terrain)."
"http://dbpedia.org/resource/Artificial_intelligence,_situated_approach"	"Artificial intelligence, situated approach"	"Artificial intelligence"	"In artificial intelligence research, the situated approach builds agents that are designed to behave effectively successfully in their environment. This requires designing AI ""from the bottom-up"" by focussing on the basic perceptual and motor skills required to survive. The situated approach gives a much lower priority to abstract reasoning or problem-solving skills. The approach was originally proposed as an alternative to traditional approaches (that is, approaches popular before 1985 or so).After several decades, classical AI technologies started to face intractable issues (e.g. combinatorial explosion) when confronted with real-world modeling problems. All approaches to address these issues focus on modeling intelligences situated in an environment. They have become known as the situated approach to AI."
"http://dbpedia.org/resource/Artificial_intuition"	"Artificial intuition"	"Artificial intelligence"	"Artificial intuition is the capacity of an artificial object or software to function with the factor of consciousness known as intuition, or a machine-based system that has some capacity to function analogous to human intuition."
"http://dbpedia.org/resource/Automated_personal_assistant"	"Automated personal assistant"	"Artificial intelligence"	"An automated personal assistant or an Intelligent Personal Assistant is a mobile software agent that can perform tasks, or services, on behalf of an individual based on a combination of user input, location awareness, and the ability to access information from a variety of online sources (such as weather conditions, traffic congestion, news, stock prices, user schedules, retail prices, etc.). There are two types of automated personal assistants: intelligent automated assistants (for example, Apple’s Siri and Tronton’s Cluzee), which perform concierge-type tasks (e.g., making dinner reservations, purchasing event tickets, making travel arrangements) or provide information based on voice input or commands; and smart personal agents, which automatically perform management or data-handling tasks based on online information and events often without user initiation or interaction. According to Chi-Hua Chien of Kleiner Perkins Caufield & Byers, examples of tasks that may be performed by a smart personal agent-type of automated personal assistant include schedule management (e.g., sending an alert to a dinner date that a user is running late due to traffic conditions, update schedules for both parties, and change the restaurant reservation time) and personal health management (e.g., monitoring caloric intake, heart rate and exercise regimen, then making recommendations for healthy choices). Both types of automated personal assistant technology are enabled by the combination of mobile computing devices, application programming interfaces (APIs), and the proliferation of mobile apps. However, intelligent automated assistants are designed to perform specific, one-off tasks specified by user voice instructions, while smart personal agents perform ongoing tasks (e.g., schedule management) autonomously."
"http://dbpedia.org/resource/Type-1_OWA_operators"	"Type-1 OWA operators"	"Artificial intelligence"	"The Yager's OWA (ordered weighted averaging) operators are used to aggregate the crisp values in decision making schemes (such as multi-criteria decision making, multi-expert decision making and multi-criteria/multi-expert decision making). It is widely accepted that Fuzzy sets are more suitable for representing preferences of criteria in decision making. The type-1 OWA operators have been proposed for this purpose. The type-1 OWA operators provides a technique for directly aggregating uncertain information with uncertain weights via OWA mechanism in soft decision making and data mining, where these uncertain objects are modelled by fuzzy sets. The two definitions for type-1 OWA operators are based on Zadeh's Extension Principle and -cuts of fuzzy sets. The two definitions lead to equivalent results."
"http://dbpedia.org/resource/Artificial_intelligence_and_law"	"Artificial intelligence and law"	"Artificial intelligence"	"Artificial intelligence and law (AI and law) is a subfield of artificial intelligence (AI) mainly concerned with applications of AI to legal informatics problems and original research on those problems. It is also concerned to contribute in the other direction: to export tools and techniques developed in the context of legal problems to AI in general. For example, theories of legal decision making, especially models of argumentation, have contributed to knowledge representation and reasoning; models of social organization based on norms have contributed to multi-agent systems; reasoning with legal cases has contributed to case-based reasoning; and the need to store and retrieve large amounts of textual data has resulted in contributions to conceptual information retrieval and intelligent databases."
"http://dbpedia.org/resource/Attributional_calculus"	"Attributional calculus"	"Artificial intelligence"	"Attributional calculus is a logic and representation system defined by Ryszard S. Michalski. It combines elements of predicate logic, propositional calculus, and multi-valued logic. Attributional calculus provides a formal language for natural induction, an inductive learning process whose results are in forms natural to people."
"http://dbpedia.org/resource/Agent_systems_reference_model"	"Agent systems reference model"	"Artificial intelligence"	"The agent systems reference model (ASRM) is a layered, abstract description for multiagent systems. As such, the reference model 
*  provides a taxonomy of terms, concepts and definitions to compare agent systems; 
*  identifies functional elements that are common in agent systems; 
*  captures data flow and dependencies among the functional elements in agent systems; and 
*  specifies assumptions and requirements regarding the dependencies among these elements. The ASRM differentiates itself from technical standards, such as Knowledge Interchange Format, Knowledge Query and Manipulation Language, and those of the Foundation for Intelligent Physical Agents in that it defines the required existence of components of a multiagent system; standards prescribe how they are designed."
"http://dbpedia.org/resource/Intelligent_decision_support_system"	"Intelligent decision support system"	"Artificial intelligence"	"An intelligent decision support system (IDSS) is a decision support system that makes extensive use of artificial intelligence (AI) techniques. Use of AI techniques in management information systems has a long history, indeed terms such as Knowledge-based systems (KBS) and intelligent systems have been used since the early 1980s to describe components of management systems, but the term ""Intelligent decision support system"" is thought to originate with Clyde Holsapple and Andrew Whinston in the late 1970s. Examples of specialized intelligent decision support systems include Flexible manufacturing systems (FMS), intelligent marketing decision support systems and medical diagnosis systems. Ideally, an intelligent decision support system should behave like a human consultant: supporting decision makers by gathering and analysing evidence, identifying and diagnosing problems, proposing possible courses of action and evaluating the proposed actions. The aim of the AI techniques embedded in an intelligent decision support system is to enable these tasks to be performed by a computer, while emulating human capabilities as closely as possible. Many IDSS implementations are based on expert systems, a well established type of KBS that encode knowledge and emulate the cognitive behaviours of human experts using predicate logic rules, and have been shown to perform better than the original human experts in some circumstances. Expert systems emerged as practical applications in the 1980s  based on research in artificial intelligence performed during the late 1960s and early 1970s. They typically combine knowledge of a particular application domain with an inference capability to enable the system to propose decisions or diagnoses. Accuracy and consistency can be comparable to (or even exceed) that of human experts when the decision parameters are well known (e.g. if a common disease is being diagnosed), but performance can be poor when novel or uncertain circumstances arise. Some research in AI, focused on enabling systems to respond to novelty and uncertainty in more flexible ways is starting to be used in intelligent decision support systems. For example, intelligent agents that perform complex cognitive tasks without any need for human intervention have been used in a range of decision support applications. Capabilities of these intelligent agents include knowledge sharing, machine learning, data mining, and automated inference. A range of AI techniques such as case based reasoning, rough sets and fuzzy logic have also been used to enable decision support systems to perform better in uncertain conditions."
"http://dbpedia.org/resource/Music_and_artificial_intelligence"	"Music and artificial intelligence"	"Artificial intelligence"	"Research in artificial intelligence (AI) is known to have impacted medical diagnosis, stock trading, robot control, and several other fields. Perhaps less popular is the contribution of AI in the field of music. Nevertheless, artificial intelligence and music (AIM) has, for a long time, been a common subject in several conferences and workshops, including the International Computer Music Conference, the Computing Society Conference  and the International Joint Conference on Artificial Intelligence. In fact, the first International Computer Music Conference was the ICMC 1974, Michigan State University, East Lansing, USA Current research includes the application of AI in music composition, performance, theory and digital sound processing. Several music software applications have been developed that use AI to produce music. A few examples are included below. Note that there are many that are still being developed."
"http://dbpedia.org/resource/Gödel_machine"	"Gödel machine"	"Artificial intelligence"	"A Gödel machine is a self-improving computer program that solves problems in an optimal way. It uses a recursive self-improvement protocol in which it rewrites its own code when it can prove the new code provides a more optimal strategy. The machine was invented by Jürgen Schmidhuber (first proposed in 2003), but is named after Kurt Gödel who inspired the mathematical theories. The Gödel machine is often discussed when dealing with issues of meta-learning, also known as ""learning to learn."" Applications include automating human design decisions and transfer of knowledge between multiple related tasks, and may lead to design of more robust and general learning architectures. Though theoretically possible, no full implementation has existed before. The Gödel machine is often compared with Marcus Hutter's AIXI, another formal specification for an artificial general intelligence. Schmidhuber points out that the Gödel machine could start out by implementing AIXI as its initial sub-program, and self-modify after it finds proof that another algorithm for its search code will be more optimal."
"http://dbpedia.org/resource/Legal_expert_system"	"Legal expert system"	"Artificial intelligence"	"A legal expert system is a domain-specific expert system that uses artificial intelligence to emulate the decision-making abilities of a human expert in the field of law. Legal expert systems employ a rule base or knowledge base and an inference engine to accumulate, reference and produce expert knowledge on specific subjects within the legal domain."
"http://dbpedia.org/resource/Trenchard_More"	"Trenchard More"	"Artificial intelligence"	"Trenchard More is a professor at Dartmouth College. He participated in the 1956 Dartmouth Summer Research Project on Artificial Intelligence. At the 50th year meeting of the Dartmouth Conference with Marvin Minsky, Geoffrey Hinton and Simon Osindero he presented The Future of Network Models and also gave a lecture entitled Routes to the Summit. Designed a theory for nested rectangular array that provided a formal structure used in the development of the  Nested Interactive Array Language."
"http://dbpedia.org/resource/Catastrophic_interference"	"Catastrophic interference"	"Artificial intelligence"	"Catastrophic interference, also known as catastrophic forgetting, is the tendency of an artificial neural network to completely and abruptly forget previously learned information upon learning new information. Neural networks are an important part of the network approach and connectionist approach to cognitive science. These networks use computer simulations to try and model human behaviours, such as memory and learning. Catastrophic interference is an important issue to consider when creating connectionist models of memory. It was originally brought to the attention of the scientific community by research from McCloskey and Cohen (1989), and Ractcliff (1990). It is a radical manifestation of the ‘sensitivity-stability’ dilemma  or the ‘stability-plasticity’ dilemma. Specifically, these problems refer to the issue of being able to make an artificial neural network that is sensitive to, but not disrupted by, new information. Lookup tables and connectionist networks lie on the opposite sides of the stability plasticity spectrum. The former remains completely stable in the presence of new information but lacks the ability to generalize, i.e. infer general principles, from new inputs. On the other hand, connectionist networks like the standard backpropagation network are very sensitive to new information and can generalize on new inputs. Backpropagation models can be considered good models of human memory insofar as they mirror the human ability to generalize but these networks often exhibit less stability than human memory. Notably, these backpropagation networks are susceptible to catastrophic interference. This is considered an issue when attempting to model human memory because, unlike these networks, humans typically do not show catastrophic forgetting. Thus, the issue of catastrophic interference must be eradicated from these backpropagation models in order to enhance the plausibility as models of human memory."
"http://dbpedia.org/resource/AI_box"	"AI box"	"Artificial intelligence"	"An AI box is a hypothetical isolated computer hardware system where a possibly dangerous artificial intelligence, or AI, is kept constrained in a ""virtual prison"" and not allowed to manipulate events in the external world. Such a box would be restricted to minimalist communication channels. Unfortunately, even if the box is well-designed, a sufficiently intelligent AI may nevertheless be able to persuade or trick its human keepers into releasing it, or otherwise be able to ""hack"" its way out of the box."
"http://dbpedia.org/resource/Deepdreaming"	"Deepdreaming"	"Artificial intelligence"	"Deep dreaming refers to the generation of images that desired activations in a trained deep network.For example, given a network that is trained to recognize cats (among other things), a dreamed cat image can be synthesized by performing a Gradient descent optimization adjusting a random image such that when it is fed forward through the trained network, it produces the ""this is a cat"" output.The optimization resembles Backpropagation, however instead of adjusting the network weights,the weights are held fixed and the input is adjusted. Alternately, an existing image can be altered so that it is more cat-like,and the resulting enhanced image can be again input to the procedure.This usage resembles the childhood activity of looking for animals or other patterns in clouds. The dreaming idea and name became popular on the internet in 2015 thanks to Google's DeepDream program. The idea dates from early in the history of neural networks , and was explored more recently (but prior to Google's work) by several research groups. Applying gradient descent independently to each pixel of the input produces images in whichadjacent pixels have little relation and thus the image has too much high frequency information.The generated images can be greatly improved by including a prior or regularizer that prefers inputsthat have natural image statistics (without a preference for any particular image), or are simply smooth.For example, used the total variation regularizer that prefers images that are piecewise constant. Various regularizers are discussed further in. The dreaming idea can be applied to hidden (internal) neurons other than those in the output, which allows exploration of the roles and representations of various parts of the network.It is also possible to optimize the input to satisfy either a single neuron (this usage is sometimes called Activity Maximization) or an entire layer of neurons. While dreaming is most often used for visualizing networks or producing computer art, it has recently been proposed that adding ""dreamed"" inputs to the training set can improve training times."
"http://dbpedia.org/resource/Language_Acquisition_Device_(computer)"	"Language Acquisition Device (computer)"	"Artificial intelligence"	"The Language Acquisition Device is a computer program developed by Lobal Technologies, a computer company in the United Kingdom, and scientists from King's College. It emulates the functions of the brain's frontal lobes where humans process language and emotion. Scientists hope this might enable computers to understand, speak, learn, and eventually think. One possible use is in interactive entertainment such as video gaming, where the technology is used to help computer-controlled characters to develop. A press release describing this technology produced widespread media interest in 2002, but no reports have been published since then, and the current status of the technology is unclear."
"http://dbpedia.org/resource/Evolving_intelligent_system"	"Evolving intelligent system"	"Artificial intelligence"	"The term Evolving Intelligent Systems (EIS) was coined in 2005 by Angelov and Kasabov, to define the new approach which focuses on learning, developing soft computing models that have both their parameters but also their structure adapting on-line. The first papers in this direction can be traced back to the end of 20th century, but these were closer to evolutionary algorithms. Later, in the very beginning of the 21st century both Angelov and Kasabov came independently to evolving rule-based, and evolving connectionist systems  concepts respectively. This was followed later by a large number of research works by various authors, etc. who meet annually at IEEE sponsored conferences on this topic. EIS are usually associated with, streaming data and on-line (often real-time) modes of operation. They can be seen as adaptive intelligent systems with low-computational complexity. EIS assumes on-line adaptation of system structure in addition to the parameter adaptation which is usually associated with the term ""incremental"" from Incremental heuristic search. Due to the implementation of a wide variety of adaptive, evolving and dynamic methodologies, they represent an important cornerstone within the field of data-driven learning in non-stationary environments An important sub-area of EIS is represented by Evolving Fuzzy Systems (EFS) (a comprehensive survey including real-world applications can be found in ), which rely on fuzzy systems architecture and incrementally update, evolve and prune fuzzy sets and fuzzy rules on demand and on-the-fly.One of the major strengths of EFS, compared to other forms of evolving system models, is that they are able to support some sort of interpretability and understandability for experts and users. This opens possibilities for enriched human-machine interaction's scenarios, where the users may ""communicate"" with an on-line evolving system in form of knowledge exchange (active learning (machine learning) and teaching). This concept is currently motivated and discussed in the evolving systems community under the term Human-Inspired Evolving Machines and respected as ""one future"" generation of ""EIS""."
"http://dbpedia.org/resource/Medical_intelligence_and_language_engineering_lab"	"Medical intelligence and language engineering lab"	"Artificial intelligence"	"The Medical Intelligence and Language Engineering laboratory, also known as MILE lab, is a research laboratory at the Indian Institute of Science, Bangalore under the Department of Electrical Engineering. The lab is known for its work on Image processing, online handwriting recognition, Text-To-Speech and Optical character recognition systems, all of which are focused mainly on documents and speech in Indian languages. The lab is headed by A. G. Ramakrishnan."
"http://dbpedia.org/resource/Open_Letter_on_Artificial_Intelligence"	"Open Letter on Artificial Intelligence"	"Artificial intelligence"	"In January 2015, Stephen Hawking, Elon Musk, and dozens of artificial intelligence experts signed an open letter on artificial intelligence calling for research on the societal impacts of AI. The letter affirmed that society can reap great potential benefits from artificial intelligence, but called for concrete research on how to prevent certain potential ""pitfalls"": artificial intelligence has the potential to eradicate disease and poverty, but researchers must not create something which cannot be controlled. The four-paragraph letter, titled Research Priorities for Robust and Beneficial Artificial Intelligence: an Open Letter, lays out detailed research priorities in an accompanying twelve-page document."
"http://dbpedia.org/resource/Oriented_Energy_Filters"	"Oriented Energy Filters"	"Artificial intelligence"	"Oriented energy filters are used to grant sight to intelligent machines and sensors. The light comes in and is filtered so that it can be properly computed and analyzed by the computer allowing it to “perceive” what it is measuring. These energy measurements are then calculated to take a real time measurement of the oriented space time structure. 3D Gaussian filters are used to extract orientation measurements. They were chosen due to their ability to capture a broad spectrum and easy and efficient computations. The use of these vision systems can then be used in smart room, human interface and surveillance applications. The computations used can tell more than the standalone frame that most perceived motion devices such as a television frame. The objects captured by these devices would tell the velocity and energy of an object and its direction in relation to space and time. This also allows for better tracking ability and recognition."
"http://dbpedia.org/resource/Knowledge-based_recommender_system"	"Knowledge-based recommender system"	"Artificial intelligence"	"Knowledge-based recommender systems (knowledge based recommenders)  are a specific type of recommender system that are based on explicit knowledge about the item assortment, user preferences, and recommendation criteria (i.e., which item should be recommended in which context?). These systems are applied in scenarios where alternative approaches such as Collaborative filtering and Content-based filtering cannot be applied. A major strength of knowledge-based recommender systems is the non-existence of cold-start (ramp-up) problems. A corresponding drawback are potential knowledge acquisition bottlenecks triggered by the need of defining recommendation knowledge in an explicit fashion."
"http://dbpedia.org/resource/Manifold_alignment"	"Manifold alignment"	"Artificial intelligence"	"Manifold alignment is a class of machine learning algorithms that produce projections between sets of data, given that the original data sets lie on a common manifold. The concept was first introduced as such by Ham, Lee, and Saul in 2003, adding a manifold constraint to the general problem of correlating sets of high-dimensional vectors."
"http://dbpedia.org/resource/Color_moments"	"Color moments"	"Artificial intelligence"	"Color moments are measures that characterise color distribution in an image in the same way that central moments uniquely describe a probability distribution. Color moments are mainly used for color indexing purposes as features in image retrieval applications in order to compare how similar two images are based on color. Usually one image is compared to a database of digital images with pre-computed features in order to find and retrieve a similar image. Each comparison between images results in a similarity score, and the lower this score is the more identical the two images are supposed to be."
"http://dbpedia.org/resource/BabyX"	"BabyX"	"Artificial intelligence"	"BabyX is a project of Auckland's Bioengineering Institute Laboratory for Animate Technologies, for the creation of a virtual animated baby that learns and reacts like a human baby. It uses the computer's cameras for ""seeing"" and microphones to ""listen"" as the inputs. The computer uses Artificial intelligence algorithms for BabyX's ""learning"" and interpretation of the inputs (voice and image) to understand the situation. The result is a virtual toddler that can learn to read, recognize objects and ""understand."" The output is the baby's face that can ""speak"" and express its mood by facial expressions (such as smile and show embarrassment)."
"http://dbpedia.org/resource/Knowledge_Based_Software_Assistant"	"Knowledge Based Software Assistant"	"Artificial intelligence"	"The Knowledge Based Software Assistant (KBSA) was a research program funded by the United States Air Force. The goal of the program was to apply concepts from artificial intelligence to the problem of designing and implementing computer software. Software would be described by models in very high level languages (essentially equivalent to first order logic) and then transformation rules would transform the specification into efficient code. The air force hoped to be able to generate the software to control weapons systems and other command and control systems using this method. As software was becoming ever more critical to USAF weapons systems it was realized that improving the quality and productivity of the software development process could have significant benefits for the military, as well as for information technology in other major US industries."
"http://dbpedia.org/resource/Reasoning_system"	"Reasoning system"	"Artificial intelligence"	"In information technology a reasoning system is a software system that generates conclusions from available knowledge using logical techniques such as deduction and induction. Reasoning systems play an important role in the implementation of artificial intelligence and knowledge-based systems. By the everyday usage definition of the phrase, all computer systems are reasoning systems in that they all automate some type of logic or decision. In typical use in the Information Technology field however, the phrase is usually reserved for systems that perform more complex kinds of reasoning. For example, not for systems that do fairly straightforward types of reasoning such as calculating a sales tax or customer discount but making logical inferences about a medical diagnosis or mathematical theorem. Reasoning systems come in two modes: interactive and batch processing. Interactive systems interface with the user to ask clarifying questions or otherwise allow the user to guide the reasoning process. Batch systems take in all the available information at once and generate the best answer possible without user feedback or guidance. Reasoning systems have a wide field of application that includes scheduling, business rule processing, problem solving, complex event processing, intrusion detection, predictive analytics, robotics, computer vision, and natural language processing."
"http://dbpedia.org/resource/Simulated_consciousness_(science_fiction)"	"Simulated consciousness (science fiction)"	"Artificial intelligence"	"Simulated consciousness, synthetic consciousness, etc. is a theme of a number of works in science fiction. The theme is one step beyond the concept of the ""brain in a vat""/""simulated reality"" in that not only the perceived reality but the brain and its consciousness are simulations themselves. On the other hand, it is also the extension of the concept of artificial consciousness in that not only the intelligence, but the ""reality"" it perceives and operates within is artificial as well. Stanislaw Lem's professor Corcoran (met by Ijon Tichy during his interstellar travels, first published by Lem in 1961) simulated conscious agents (personoids) to actually test the viability of the ""simulation hypothesis"" of the reality, i.e., the idea of solipsism. In the 1954 story The Tunnel under the World by Frederik Pohl, a whole city was simulated in order to run tests of the efficiency of advertising campaigns, and the plot evolves from the point when one ""simulacrum"" suddenly notices that every day is June 15. Pohl's idea was elaborated in Simulacron-3 (1964) by Daniel F. Galouye (alternative title: Counterfeit World), which tells the story of a virtual city developed as a computer simulation for market research purposes. In this city the simulated inhabitants possess consciousness; all but one of the inhabitants are unaware of the true nature of their world. Furthermore, various novels by Greg Egan such as Permutation City (1994), Diaspora (1997) and Schild's Ladder (2002) explore the concept of simulated consciousness."
"http://dbpedia.org/resource/SUPS"	"SUPS"	"Artificial intelligence"	"In computational neuroscience, SUPS (for Synaptic Updates Per Second) or formerly CUPS (Connections Updates Per Second) is a measure of a neuronal network performance, useful in fields of neuroscience, cognitive science, artificial intelligence, and computer science."
"http://dbpedia.org/resource/Real-time_Search"	"Real-time Search"	"Artificial intelligence"	"In computer graphics, Real-time search refers to the set of search algorithms that serve for Navigation and Path-finding of an agent in a dynamic environment, where the decision making is constantly affected by unpredictable forces such as moving obstacles. Since the obstacles between the start point and the goal are doing stochastic motions continuously, a real time reaction to the changing environment and a real time update for the chosen path become necessities. Especially, real time search provides solutions for complex agent representation problems which are encountered in several significant digital industries such as film production and game making, where enormous virtual human agents have to be simulated in the same complex scene with an authentic and collision-less motion simultaneously."
"http://dbpedia.org/resource/Glossary_of_artificial_intelligence"	"Glossary of artificial intelligence"	"Artificial intelligence"	"This glossary of artificial intelligence terms is about artificial intelligence, its sub-disciplines, and related fields."
"http://dbpedia.org/resource/Spectrum-diverse_Unified_Neuroevolution_Architecture"	"Spectrum-diverse Unified Neuroevolution Architecture"	"Artificial intelligence"	"Spectrum-diverse Unified Neuroevolution Architecture (SUNA) is an evolutionary algorithm developed by Danilo Vasconcellos Vargas and Junichi Murata for evolving neural networks in which both the topology as well as parameters of the network are developed. SUNA aims to work completely autonomously, learning any kind of problem without prior knowledge or the need of an expert. To achieve this, two problems needed to be tackled: 1.  
*  A powerful representation that is flexible enough to model any kind of problem need to be developed. 2.  
*  Consequently, to develop a powerful and complex representation a specialized learning algorithm is needed. SUNA tackles these two problems with the following key features respectively: 1.  
*  Unified Neural Model – To develop a powerful representation that is capable of modelling a wide range of problem classes, a novel neural model called Unified Neural Model is proposed that unifies most neural network features from the literature into one representation. 2.  
*  Spectrum-Diverse Neuroevolution – To develop such complex representations, neuroevolution is used with a new concept that calculates the spectrum of candidate solutions based on their characteristics and use it to keep the diversity. This enable high dimensional structures to be compared efficiently and scale well with their size."
"http://dbpedia.org/resource/Vaumpus_world"	"Vaumpus world"	"Artificial intelligence"	"Vaumpus world is a simple world use in artificial intelligence for which to represent knowledge and to reason.Vaumpus world was introduced by Genesereth, and is discussed in Russell-Norvig Artificial intelligence book (ISBN 0136042597) inspired by 1972 video game Hunt the Wumpus ."
"http://dbpedia.org/resource/Artificial_intelligence_for_video_surveillance"	"Artificial intelligence for video surveillance"	"Artificial intelligence"	"Artificial intelligence for video surveillance utilizes computer software programs that analyze the images from video surveillance cameras in order to recognize humans, vehicles or objects. Security contractors program the software to define restricted areas within the camera's view (such as a fenced off area, a parking lot but not the sidewalk or public street outside the lot) and program for times of day (such as after the close of business) for the property being protected by the camera surveillance. The artificial intelligence (""A.I."") sends an alert if it detects a trespasser breaking the ""rule"" set that no person is allowed in that area during that time of day. The A.I. program functions by using machine vision. Machine vision is a series of algorithms, or mathematical procedures, which work like a flow-chart or series of questions to compare the object seen with hundreds of thousands of stored reference images of humans in different postures, angles, positions and movements. The A.I. asks itself if the observed object moves like the reference images, whether it is approximately the same size height relative to width, if it has the characteristic two arms and two legs, if it moves with similar speed, and if it is vertical instead of horizontal. Many other questions are possible, such as the degree to which the object is reflective, the degree to which it is steady or vibrating, and the smoothness with which it moves. Combining all of the values from the various questions, an overall ranking is derived which gives the A.I. the probability that the object is or is not a human. If the value exceeds a limit that is set, then the alert is sent. It is characteristic of such programs that they are self-learning to a degree, learning, for example that humans or vehicles appear bigger in certain portions of the monitored image – those areas near the camera – than in other portions, those being the areas farthest from the camera. In addition to the simple rule restricting humans or vehicles from certain areas at certain times of day, more complex rules can be set. The user of the system may wish to know if vehicles drive in one direction but not the other. Users may wish to know that there are more than a certain preset number of people within a particular area. The A.I. is capable of maintaining surveillance of hundreds of cameras simultaneously. Its ability to spot a trespasser in the distance or in rain or glare is superior to humans' ability to do so. This type of A.I. for security is known as ""rule-based"" because a human programmer must set rules for all of the things for which the user wishes to be alerted. This is the most prevalent form of A.I. for security. Many video surveillance camera systems today include this type of A.I. capability. The hard-drive that houses the program can either be located in the cameras themselves or can be in a separate device that receives the input from the cameras. A newer, non-rule based form of A.I. for security called ""behavioral analytics"" has been developed. This software is fully self-learning with no initial programming input by the user or security contractor. In this type of analytics, the A.I. learns what is normal behavior for people, vehicles, machines, and the environment based on its own observation of patterns of various characteristics such as size, speed, reflectivity, color, grouping, vertical or horizontal orientation and so forth. The A.I. normalizes the visual data, meaning that it classifies and tags the objects and patterns it observes, building up continuously refined definitions of what is normal or average behavior for the various observed objects. After several weeks of learning in this fashion it can recognize when things break the pattern. When it observes such anomalies it sends an alert. For example, it is normal for cars to drive in the street. A car seen driving up onto a sidewalk would be an anomaly. If a fenced yard is normally empty at night, then a person entering that area would be an anomaly."
"http://dbpedia.org/resource/Dynamic_epistemic_logic"	"Dynamic epistemic logic"	"Artificial intelligence"	"Dynamic epistemic logic (DEL) is a logical framework dealing with knowledge and information change. Typically, DEL focuses on situations involving multiple agents and studies how their knowledge changes when events occur. These events can change factual properties of the actual world (they are called ontic events): for example a red card is painted in blue. They can also bring about changes of knowledge without changing factual properties of the world (they are called epistemic events): for example a card is revealed publicly (or privately) to be red. Originally, DEL focused on epistemic events. We only present in this entry some of the basic ideas of the original DEL framework; more details about DEL in general can be found in the references. Due to the nature of its object of study and its abstract approach, DEL is related and has applications to numerous research areas, such as computer science (artificial intelligence), philosophy (formal epistemology), economics (game theory) and cognitive science. In computer science, DEL is for example very much related to multi-agent systems, which are systems where multiple intelligent agents interact and exchange information. As a combination of dynamic logic and epistemic logic, dynamic epistemic logic is a young field of research. It really started in 1989 with Plaza’s logic of public announcement. Independently, Gerbrandy and Groeneveld proposed a system dealing moreover with private announcement and that was inspired by the work of Veltman. Another system was proposed by van Ditmarsch whose main inspiration was the Cluedo game. But the most influential and original system was the system proposed by Baltag, Moss and Solecki. This system can deal with all the types of situations studied in the works above and its underlying methodology is conceptually grounded. We will present in this entry some of its basic ideas. Formally, DEL extends ordinary epistemic logic by the inclusion of event models to describe actions, and a product update operator that defines how epistemic models are updated as the consequence of executing actions described through event models. Epistemic logic will first be recalled. Then, actions and events will enter into the picture and we will introduce the DEL framework."
"http://dbpedia.org/resource/Differentiable_neural_computer"	"Differentiable neural computer"	"Artificial intelligence"	"A differentiable neural computer (DNC) combines the learning and pattern-recognition strengths of deep neural networks with the ability to retain information in complex data structures such as graphs in a computer memory. The memory can be retained indefinitely, while the DNC uses what it has learned to solve related problems. DNC memory interactions are differentiable end-to-end, making it possible to optimize them efficiently using gradient descent. DNCs were introduced by DeepMind in 2016 as an extension of neural turing machines. They demonstrated, for example, how a DNC can be trained to navigate a variety of rapid transit systems, and then apply what it learned to get around on the London Underground. A neural network without memory would typically have to learn about each different transit system from scratch. On a sequence-processing task, DNCs performed better than either a long short-term memory system or a neural turing machine. So far, DNCs have only been demonstrated to handle relatively simple tasks, which could have been easily solved using conventional computer programming decades ago. But DNCs can learn some aspects of symbolic reasoning and apply it to the use of working memory. Further more they don't need to be programmed for each problem they are applied to, but can instead be trained. Some experts see promise that they can be trained to perform complex, structured tasks and address big-data applications that require some sort of rational reasoning, such as generating video commentaries or semantic text analysis."
"http://dbpedia.org/resource/List_of_datasets_for_machine_learning_research"	"List of datasets for machine learning research"	"Artificial intelligence"	"These datasets are used for machine learning research and have been cited in peer-reviewed academic journals and other publications. Datasets are an integral part of the field of machine learning. Major advances in this field can result from advances in learning algorithms (such as deep learning), computer hardware, and, less-intuitively, the availability of high-quality training datasets. High-quality labeled training datasets for supervised and semi-supervised machine learning algorithms are usually difficult and expensive to produce because of the large amount of time needed to label the data. Although they do not need to be labeled, high-quality datasets for unsupervised learning can also be difficult and costly to produce. This list aggregates high-quality datasets that have been shown to be of value to the machine learning research community from multiple different data repositories to provide greater coverage of the topic than is otherwise available."
"http://dbpedia.org/resource/Meca_Sapiens"	"Meca Sapiens"	"Artificial intelligence"	"Meca Sapiens (from the Latin words mēchanicus, which means ""mechanical"", and sapiens which means ""wise"", i.e. ""wise machine "") is a framework, in Artificial General Intelligence (AGI), whose aim is to implement synthetic consciousness. The framework is based on an understanding of consciousness as a system capability; its design stages followed a top-down development process; and it utilizes standard software engineering tools, structures and techniques. A complete system architecture based on the Meca Sapiens framework  and suitable for design and implementation was published in late 2015. The creator of the Meca Sapiens architecture is J.E. Tardy."
"http://dbpedia.org/resource/Turing_Robot"	"Turing Robot"	"Artificial intelligence"	"Turing Robot is the first Chinese company engaged in research on the commercialization of artificial intelligence (AI). Its founding members have over ten years of experience in AI research, including semantic recognition, cognitive computing, human-machine interaction, and machine learning. The company was founded in 2010 and in 2012 released the ""Wormhole Voice Assistant"" application, which was the first Chinese intelligence-based voice assistant application. In 2014, Turing released the first open platform for AI robots, also known as the Turing Robot. In November 2015, the Turing OS system was released ."
"http://dbpedia.org/resource/VaultML"	"VaultML"	"Artificial intelligence"	"Vault is an Israeli–based artificial intelligence company that lays claims to have created technologies that can ""read"" movie and TV screenplays in order to predict box office and investment performance. Part of the process reportedly entails analyzing 300,000 to 400,000 elements from the script, which could be anything from plot, character development, script structure, scene events. The founders are made up of high frequency trading veterans and state they use similar approaches to predicting film performance. Vault published its 2015 film predictions for over 20 movies in early 2015 and successfully predicted correctly many box office performances throughout that year. Vault's algorithms out earned the market on a return on investment basis."
"http://dbpedia.org/resource/Gomocup"	"Gomocup"	"Artificial intelligence"	"Gomocup is a worldwide tournament of artificial intelligences (AI) playing Gomoku and Renju. The tournament is being played since 2000 and takes place every year. As of 2016, it is the most famous and largest Gomoku AI tournament in the world, with around 40 authors from about 10 countries."
"http://dbpedia.org/resource/Robot_Lawyer"	"Robot Lawyer"	"Artificial intelligence"	"A robot lawyer  is an artificial intelligence lawyer. It is a computer based machine. A robot lawyer named ROSS has been employed by a law firm in the US, which will use the robot to assist its various teams in legal research."
"http://dbpedia.org/resource/Feedforward_neural_network"	"Feedforward neural network"	"Artificial neural networks"	"A feedforward neural network is an artificial neural network wherein connections between the units do not form a cycle. As such, it is different from recurrent neural networks. The feedforward neural network was the first and simplest type of artificial neural network devised. In this network, the information moves in only one direction, forward, from the input nodes, through the hidden nodes (if any) and to the output nodes. There are no cycles or loops in the network."
"http://dbpedia.org/resource/NETtalk_(artificial_neural_network)"	"NETtalk (artificial neural network)"	"Artificial neural networks"	"NETtalk is an artificial neural network. It is the result of research carried out in the mid-1980s by Terrence Sejnowski and Charles Rosenberg. The intent behind NETtalk was to construct simplified models that might shed light on the complexity of learning human level cognitive tasks, and their implementation as a connectionist model that could also learn to perform a comparable task. NETtalk is a program that learns to pronounce written English text by being shown text as input and matching phonetic transcriptions for comparison."
"http://dbpedia.org/resource/Neural_Networks_(journal)"	"Neural Networks (journal)"	"Artificial neural networks"	"Neural Networks is a monthly peer-reviewed scientific journal and an official journal of the International Neural Network Society, European Neural Network Society, and Japanese Neural Network Society. It was established in 1988 and is published by Elsevier. The journal covers all aspects of research on artificial neural networks. The founding editor-in-chief was Stephen Grossberg (Boston University), the current editors-in-chief are DeLiang Wang (Ohio State University) and Kenji Doya (Okinawa Institute of Science and Technology). The journal is abstracted and indexed in Scopus and the Science Citation Index. According to the Journal Citation Reports, the journal has a 2012 impact factor of 1.927."
"http://dbpedia.org/resource/Time_delay_neural_network"	"Time delay neural network"	"Artificial neural networks"	"Time delay neural network (TDNN)  is an artificial neural network architecture whose primary purpose is to work on sequential data. The TDNN units recognise features independent of time-shift (i.e. sequence position) and usually form part of a larger pattern recognition system. An example would be converting continuous audio into a stream of classified phoneme labels for speech recognition. An input signal is augmented with delayed copies as other inputs, the neural network is time-shift invariant since it has no internal state. The original paper presented a perceptron network whose connection weights were trained with the back-propagation algorithm, this may be done in batch or online. The Stuttgart Neural Network Simulator implements that version."
"http://dbpedia.org/resource/Leabra"	"Leabra"	"Artificial neural networks"	"Leabra stands for Local, Error-driven and Associative, Biologically Realistic Algorithm. It is a model of learning which is a balance between Hebbian and error-driven learning with other network-derived characteristics. This model is used to mathematically predict outcomes based on inputs and previous learning influences. This model is heavily influenced by and contributes to neural network designs and models.This algorithm is the default algorithm in emergent (successor of PDP++) when making a new project, and is extensively used in various simulations. Hebbian learning is performed using conditional principal components analysis (CPCA) algorithm with correction factor for sparse expected activity levels. Error-driven learning is performed using GeneRec, which is a generalization of the recirculation algorithm, and approximates Almeida–Pineda recurrent backpropagation. The symmetric, midpoint version of GeneRec is used, which is equivalent to the contrastive Hebbian learning algorithm (CHL). See O'Reilly (1996; Neural Computation) for more details. The activation function is a point-neuron approximation with both discrete spiking and continuous rate-code output. Layer or unit-group level inhibition can be computed directly using a k-winners-take-all (KWTA) function, producing sparse distributed representations. The net input is computed as an average, not a sum, over connections, based on normalized, sigmoidally transformed weight values, which are subject to scaling on a connection-group level to alter relative contributions. Automatic scaling is performed to compensate for differences in expected activity level in the different projections. Documentation about this algorithm can be found in the book ""Computational Explorations in Cognitive Neuroscience: Understanding the Mind by Simulating the Brain"" published by MIT press. and in the Emergent Documentation"
"http://dbpedia.org/resource/Evolutionary_acquisition_of_neural_topologies"	"Evolutionary acquisition of neural topologies"	"Artificial neural networks"	"Evolutionary acquisition of neural topologies (EANT/EANT2) is an evolutionary reinforcement learning method that evolves both the topology and weights of artificial neural networks. It is closely related to the works of Angeline et al. and Stanley and Miikkulainen. Like the work of Angeline et al., the method uses a type of parametric mutation that comes from evolution strategies and evolutionary programming (now using the most advanced form of the evolution strategies CMA-ES in EANT2), in which adaptive step sizes are used for optimizing the weights of the neural networks. Similar to the work of Stanley (NEAT), the method starts with minimal structures which gain complexity along the evolution path."
"http://dbpedia.org/resource/Perceptron"	"Perceptron"	"Artificial neural networks"	"In machine learning, the perceptron is an algorithm for supervised learning of binary classifiers (functions that can decide whether an input, represented by a vector of numbers, belongs to some specific class or not). It is a type of linear classifier, i.e. a classification algorithm that makes its predictions based on a linear predictor function combining a set of weights with the feature vector. The algorithm allows for online learning, in that it processes elements in the training set one at a time. The perceptron algorithm dates back to the late 1950s; its first implementation, in custom hardware, was one of the first artificial neural networks to be produced."
"http://dbpedia.org/resource/Autoassociative_memory"	"Autoassociative memory"	"Artificial neural networks"	"Autoassociative memory, also known as auto-association memory or an autoassociation network, is a generic term that refers to all types of memories that enable one to retrieve a piece of data from only a tiny sample of itself. It is often misunderstood to be only a form of backpropagation or other neural networks."
"http://dbpedia.org/resource/Bidirectional_associative_memory"	"Bidirectional associative memory"	"Artificial neural networks"	"Bidirectional associative memory (BAM) is a type of recurrent neural network. BAM was introduced by Bart Kosko in 1988. There are two types of associative memory, auto-associative and hetero-associative. BAM is hetero-associative, meaning given a pattern it can return another pattern which is potentially of a different size. It is similar to the Hopfield network in that they are both forms of associative memory. However, Hopfield nets return patterns of the same size."
"http://dbpedia.org/resource/Self-organizing_map"	"Self-organizing map"	"Artificial neural networks"	"A self-organizing map (SOM) or self-organising feature map (SOFM) is a type of artificial neural network (ANN) that is trained using unsupervised learning to produce a low-dimensional (typically two-dimensional), discretized representation of the input space of the training samples, called a map. Self-organizing maps differ from other artificial neural networks as they apply competitive learning as opposed to error-correction learning (such as backpropagation with gradient descent), and in the sense that they use a neighborhood function to preserve the topological properties of the input space. This makes SOMs useful for visualizing low-dimensional views of high-dimensional data, akin to multidimensional scaling. The artificial neural network introduced by the Finnish professor Teuvo Kohonen in the 1980s is sometimes called a Kohonen map or network. The Kohonen net is a computationally convenient abstraction building on work on biologically neural models from the 1970s and morphogenesis models dating back to Alan Turing in the 1950s. Like most artificial neural networks, SOMs operate in two modes: training and mapping. ""Training"" builds the map using input examples (a competitive process, also called vector quantization), while ""mapping"" automatically classifies a new input vector. A self-organizing map consists of components called nodes or neurons. Associated with each node are a weight vector of the same dimension as the input data vectors, and a position in the map space. The usual arrangement of nodes is a two-dimensional regular spacing in a hexagonal or rectangular grid. The self-organizing map describes a mapping from a higher-dimensional input space to a lower-dimensional map space. The procedure for placing a vector from data space onto the map is to find the node with the closest (smallest distance metric) weight vector to the data space vector. While it is typical to consider this type of network structure as related to feedforward networks where the nodes are visualized as being attached, this type of architecture is fundamentally different in arrangement and motivation. Useful extensions include using toroidal grids where opposite edges are connected and using large numbers of nodes. It has been shown that while self-organizing maps with a small number of nodes behave in a way that is similar to K-means, larger self-organizing maps rearrange data in a way that is fundamentally topological in character. It is also common to use the U-Matrix. The U-Matrix value of a particular node is the average distance between the node's weight vector and that of its closest neighbors. In a square grid, for instance, we might consider the closest 4 or 8 nodes (the Von Neumann and Moore neighborhoods, respectively), or six nodes in a hexagonal grid. Large SOMs display  emergent properties. In maps consisting of thousands of nodes, it is possible to perform cluster operations on the map itself."
"http://dbpedia.org/resource/Cerebellar_model_articulation_controller"	"Cerebellar model articulation controller"	"Artificial neural networks"	"The cerebellar model arithmetic computer (CMAC) is a type of neural network based on a model of the mammalian cerebellum. It is also known as the cerebellar model articulation controller. It is a type of associative memory. The CMAC was first proposed as a function modeler for robotic controllers by James Albus in 1975 (hence the name), but has been extensively used in reinforcement learning and also as for automated classification in the machine learning community. CMAC computes a function , where is the number of input dimensions. The input space is divided up into hyper-rectangles, each of which is associated with a memory cell. The contents of the memory cells are the weights, which are adjusted during training. Usually, more than one quantisation of input space is used, so that any point in input space is associated with a number of hyper-rectangles, and therefore with a number of memory cells. The output of a CMAC is the algebraic sum of the weights in all the memory cells activated by the input point. A change of value of the input point results in a change in the set of activated hyper-rectangles, and therefore a change in the set of memory cells participating in the CMAC output. The CMAC output is therefore stored in a distributed fashion, such that the output corresponding to any point in input space is derived from the value stored in a number of memory cells (hence the name associative memory). This provides generalisation."
"http://dbpedia.org/resource/Neocognitron"	"Neocognitron"	"Artificial neural networks"	"The neocognitron is a hierarchical, multilayered artificial neural network proposed by Kunihiko Fukushima in the 1980s. It has been used for handwritten character recognition and other pattern recognition tasks, and served as the inspiration for convolutional neural networks. The neocognitron was inspired by the model proposed by Hubel & Wiesel in 1959. They found two types of cells in the visual primary cortex called simple cell and complex cell, and also proposed a cascading model of these two types of cells. The neocognitron is a natural extension of these cascading models. The neocognitron consists of multiple types of cells, the most important of which are called S-cells and C-cells. The local features are extracted by S-cells, and these features' deformation, such as local shifts, are tolerated by C-cells. Local features in the input are integrated gradually and classified in the higher layers. The idea of local feature integration is in several other models, such as LeNet and SIFT model. There are multiple kinds of neocognitron. For example, some types of neocognitron can detect multiple patterns in the same input by using backward signals to achieve selective attention."
"http://dbpedia.org/resource/Rprop"	"Rprop"	"Artificial neural networks"	"Rprop, short for resilient backpropagation, is a learning heuristic for supervised learning in feedforward artificial neural networks. This is a first-order optimization algorithm. This algorithm was created by Martin Riedmiller and Heinrich Braun in 1992. Similarly to the Manhattan update rule, Rprop takes into account only the sign of the partial derivative over all patterns (not the magnitude), and acts independently on each ""weight"". For each weight, if there was a sign change of the partial derivative of the total error function compared to the last iteration, the update value for that weight is multiplied by a factor η−, where η− < 1. If the last iteration produced the same sign, the update value is multiplied by a factor of η+, where η+ > 1. The update values are calculated for each weight in the above manner, and finally each weight is changed by its own update value, in the opposite direction of that weight's partial derivative, so as to minimise the total error function. η+ is empirically set to 1.2 and η− to 0.5. Next to the cascade correlation algorithm and the Levenberg–Marquardt algorithm, Rprop is one of the fastest weight update mechanisms. RPROP is a batch update algorithm."
"http://dbpedia.org/resource/Neural_backpropagation"	"Neural backpropagation"	"Artificial neural networks"	"Neural backpropagation is the phenomenon in which the action potential of a neuron creates a voltage spike both at the end of the axon (normal propagation) and back through to the dendritic arbor or dendrites, from which much of the original input current originated. It has been shown that this simple process can be used in a manner similar to the backpropagation algorithm used in multilayer perceptrons, a type of artificial neural network. In addition to active backpropagation of the action potential, there is also passive electrotonic spread. While there is ample evidence to prove the existence of backpropagating action potentials, the function of such action potentials and the extent to which they invade the most distal dendrites remains highly controversial."
"http://dbpedia.org/resource/U-matrix"	"U-matrix"	"Artificial neural networks"	"The U-matrix (unified distance matrix) is a representation of a self-organizing map (SOM) where the Euclidean distance between the codebook vectors of neighboring neurons is depicted in a grayscale image. This image is used to visualize the data in a high-dimensional space using a 2D image."
"http://dbpedia.org/resource/Word_embedding"	"Word embedding"	"Artificial neural networks"	"Word embedding is the collective name for a set of language modeling and feature learning techniques in natural language processing (NLP) where words or phrases from the vocabulary are mapped to vectors of real numbers in a low-dimensional space relative to the vocabulary size (""continuous space""). Methods to generate this mapping include neural networks, dimensionality reduction on the word co-occurrence matrix, probabilistic models, and explicit representation in terms of the context in which words appear. Word and phrase embeddings, when used as the underlying input representation, have been shown to boost the performance in NLP tasks such as syntactic parsing and sentiment analysis."
"http://dbpedia.org/resource/Growing_self-organizing_map"	"Growing self-organizing map"	"Artificial neural networks"	"A growing self-organizing map (GSOM) is a growing variant of the popular self-organizing map (SOM). The GSOM was developed to address the issue of identifying a suitable map size in the SOM. It starts with a minimal number of nodes (usually 4) and grows new nodes on the boundary based on a heuristic. By using the value called Spread Factor (SF), the data analyst has the ability to control the growth of the GSOM. All the starting nodes of the GSOM are boundary nodes, i.e. each node has the freedom to grow in its own direction at the beginning. (Fig. 1) New Nodes are grown from the boundary nodes. Once a node is selected for growing all its free neighboring positions will be grown new nodes. The figure shows the three possible node growth options for a rectangular GSOM."
"http://dbpedia.org/resource/Artificial_neural_network"	"Artificial neural network"	"Artificial neural networks"	"Neural Networks (also referred to as connectionist systems) are a computational approach which is based on a large collection of neural units loosely modeling the way the brain solves problems with large clusters of biological neurons connected by axons. Each neural unit is connected with many others, and links can be enforcing or inhibitory in their effect on the activation state of connected neural units. Each individual neural unit may have a summation function which combines the values of all its inputs together. There may be a threshold function or limiting function on each connection and on the unit itself such that it must surpass it before it can propagate to other neurons. These systems are self-learning and trained rather than explicitly programmed and excel in areas where the solution or feature detection is difficult to express in a traditional computer program. Neural networks typically consist of multiple layers or a cube design, and the signal path traverses from front to back. Back propagation is where the forward stimulation is used to reset weights on the ""front"" neural units and this is sometimes done in combination with training where the correct result is known. More modern networks are a bit more free flowing in terms of stimulation and inhibition with connections interacting in a much more chaotic and complex fashion. Dynamic neural networks are the most advanced in that they dynamically can based on rules form new connections and even new neural units while disabling others. The goal of the neural network is to solve problems in the same way that the human brain would, although several neural networks are much more abstract. Modern neural network projects typically work with a few thousand to a few million neural units and millions of connections, which is still several orders of magnitude less complex than the human brain and closer to the computing power of a worm. New brain research often stimulates new patterns in neural networks. One new approach is using connections which span much further and link processing layers rather than always being localized to adjacent neurons. Other research being explored with the different types of signal over time that axons propagate which is more complex than simply on or off. Neural networks are based on real numbers, with the value of the core and of the axon typically being a representation between 0.0 and 1. An interesting facet of these systems is that they are unpredictable in their success with self learning. After training some become great problem solvers and others don't perform as well. In order to train them several thousand cycles of interaction typically occur. Like other machine learning methods – systems that learn from data – neural networks have been used to solve a wide variety of tasks, like computer vision and speech recognition, that are hard to solve using ordinary rule-based programming. Historically, the use of neural network models marked a directional shift in the late eighties from high-level (symbolic) artificial intelligence, characterized by expert systems with knowledge embodied in if-then rules, to low-level (sub-symbolic) machine learning, characterized by knowledge embodied in the parameters of a dynamical system."
"http://dbpedia.org/resource/European_Neural_Network_Society"	"European Neural Network Society"	"Artificial neural networks"	"The European Neural Network Society (ENNS) is an association of scientists, engineers, students, and others seeking to learn about and advance understanding of artificial neural networks. Specific areas of interest in this scientific field include modelling of behavioral and brain processes, development of neural algorithms and applying neural modelling concepts to problems relevant in many different domains. Erkki Oja and John G. Taylor are past ENNS presidents and honorary executive board members. Every year since 1991 ENNS organizes the International Conference on Artificial Neural Networks (ICANN). The history and the links to past conferences are available at the ENNS web site. This is one of the oldest and best established conferences on the subject, with proceedings published in Springer Lecture Notes in Computer Science, see index in DBLP bibliography database. As a non-profit organization ENNS promotes scientific activities at European and at the national levels in cooperation with national organizations that focus on neural networks. Every year many stipends to attend ICANN conference are given. ENNS also sponsors other students and have given awards and prizes at co-sponsored events (schools, workshops, conferences and competitions). The Neural Networks journal (Elsevier) is the official journal of three collaborating societies, ENNS, the International Neural Network Society (INNS) and the Japanese Neural Network Society (JNNS)."
"http://dbpedia.org/resource/Liquid_state_machine"	"Liquid state machine"	"Artificial neural networks"	"A liquid state machine (LSM) is a particular kind of spiking neural network. An LSM consists of a large collection of units (called nodes, or neurons). Each node receives time varying input from external sources (the inputs) as well as from other nodes. Nodes are randomly connected to each other. The recurrent nature of the connections turns the time varying input into a spatio-temporal pattern of activations in the network nodes. The spatio-temporal patterns of activation are read out by linear discriminant units. The soup of recurrently connected nodes will end up computing a large variety of nonlinear functions on the input. Given a large enough variety of such nonlinear functions, it is theoretically possible to obtain linear combinations (using the read out units) to perform whatever mathematical operation is needed to perform a certain task, such as speech recognition or computer vision. The word liquid in the name comes from the analogy drawn to dropping a stone into a still body of water or other liquid. The falling stone will generate ripples in the liquid. The input (motion of the falling stone) has been converted into a spatio-temporal pattern of liquid displacement (ripples). LSMs have been put forward as a way to explain the operation of brains. LSMs are argued to be an improvement over the theory of artificial neural networks because: 1.  
* Circuits are not hard coded to perform a specific task. 2.  
* Continuous time inputs are handled ""naturally"". 3.  
* Computations on various time scales can be done using the same network. 4.  
* The same network can perform multiple computations. Criticisms of LSMs as used in computational neuroscience are that 1.  
* LSMs don't actually explain how the brain functions. At best they can replicate some parts of brain functionality. 2.  
* There is no guaranteed way to dissect a working network and figure out how or what computations are being performed. 3.  
* Very little control over the process."
"http://dbpedia.org/resource/Cellular_neural_network"	"Cellular neural network"	"Artificial neural networks"	"In computer science and machine learning, cellular neural networks (CNN) (or cellular nonlinear networks (CNN)) are a parallel computing paradigm similar to neural networks, with the difference that communication is allowed between neighbouring units only. Typical applications include image processing, analyzing 3D surfaces, solving partial differential equations, reducing non-visual problems to geometric maps, modelling biological vision and other sensory-motor organs."
"http://dbpedia.org/resource/Convolutional_neural_network"	"Convolutional neural network"	"Artificial neural networks"	"In machine learning, a convolutional neural network (CNN, or ConvNet) is a type of feed-forward artificial neural network in which the connectivity pattern between its neurons is inspired by the organization of the animal visual cortex. Individual cortical neurons respond to stimuli in a restricted region of space known as the receptive field. The receptive fields of different neurons partially overlap such that they tile the visual field. The response of an individual neuron to stimuli within its receptive field can be approximated mathematically by a convolution operation. Convolutional networks were inspired by biological processes and are variations of multilayer perceptrons designed to use minimal amounts of preprocessing. They have wide applications in image and video recognition, recommender systems and natural language processing. The convolutional neural network is also known as shift invariant or space invariant artificial neural network (SIANN), which is named based on its shared weights architecture and translation invariance characteristics."
"http://dbpedia.org/resource/Oja's_rule"	"Oja's rule"	"Artificial neural networks"	"Oja's learning rule, or simply Oja's rule, named after Finnish computer scientist Erkki Oja, is a model of how neurons in the brain or in artificial neural networks change connection strength, or learn, over time. It is a modification of the standard Hebb's Rule (see Hebbian learning) that, through multiplicative normalization, solves all stability problems and generates an algorithm for principal components analysis. This is a computational form of an effect which is believed to happen in biological neurons."
"http://dbpedia.org/resource/Probabilistic_neural_network"	"Probabilistic neural network"	"Artificial neural networks"	"A probabilistic neural network (PNN) is a feedforward neural network, which was derived from the Bayesian network and a statistical algorithm called Kernel Fisher discriminant analysis. It was introduced by D.F. Specht in the early 1990s. In a PNN, the operations are organized into a multilayered feedforward network with four layers: 
*  Input layer 
*  Hidden layer 
*  Pattern layer/Summation layer 
*  Output layer"
"http://dbpedia.org/resource/Random_neural_network"	"Random neural network"	"Artificial neural networks"	"The random neural network (RNN) is a mathematical representation of an interconnected network of neurons or cells which exchange spiking signals. It was invented by Erol Gelenbe and is linked to the G-network model of queueing networks as well as to Gene Regulatory Network models. Each cell state is represented by an integer whose value rises when the cell receives an excitatory spike and drops when it receives an inhibitory spike. The spikes can originate outside the network itself, or they can come from other cells in the networks. Cells whose internal excitatory state has a positive value are allowed to send out spikes of either kind to other cells in the network according to specific cell-dependent spiking rates. The model has a mathematical solution in steady-state which provides the joint probability distribution of the network in terms of the individual probabilities that each cell is excited and able to send out spikes. Computing this solution is based on solving a set of non-linear algebraic equations whose parameters are related to the spiking rates of individual cells and their connectivity to other cells, as well as the arrival rates of spikes from outside the network. The RNN is a recurrent model, i.e. a neural network that is allowed to have complex feedback loops. A highly energy-efficient implementation of random neural networks was demonstrated by Krishna Palem et al. using the Probabilistic CMOS or PCMOS technology and was shown to be c. 226–300 times more efficient in terms of Energy-Performance-Product. RNNs are also related to artificial neural networks, which (like the random neural network) have gradient-based learning algorithms. The learning algorithm for an n-node random neural network that includes feedback loops (it is also a recurrent neural network) is of computational complexity O(n^3) (the number of computations is proportional to the cube of n, the number of neurons). The random neural network can also be used with other learning algorithms such as reinforcement learning can also be used. The RNN has been shown to be a universal approximator for bounded and continuous functions."
"http://dbpedia.org/resource/Recurrent_neural_network"	"Recurrent neural network"	"Artificial neural networks"	"A recurrent neural network (RNN) is a class of artificial neural network where connections between units form a directed cycle. This creates an internal state of the network which allows it to exhibit dynamic temporal behavior. Unlike feedforward neural networks, RNNs can use their internal memory to process arbitrary sequences of inputs. This makes them applicable to tasks such as unsegmented connected handwriting recognition or speech recognition."
"http://dbpedia.org/resource/Recursive_neural_network"	"Recursive neural network"	"Artificial neural networks"	"A recursive neural network (RNN) is a kind of deep neural network created by applying the same set of weights recursively over a structure, to produce a structured prediction over variable-length input, or a scalar prediction on it, by traversing a given structure in topological order. RNNs have been successful in learning sequence and tree structures in natural language processing, mainly phrase and sentence continuous representations based on word embedding. RNNs have first been introduced to learn distributed representations of structure, such as logical terms."
"http://dbpedia.org/resource/Feed_forward_(control)"	"Feed forward (control)"	"Artificial neural networks"	"Feed-forward, sometimes written feedforward, is a term describing an element or pathway within a control system which passes a controlling signal from a source in its external environment, often a command signal from an external operator, to a load elsewhere in its external environment. A control system which has only feed-forward behavior responds to its control signal in a pre-defined way without responding to how the load reacts; it is in contrast with a system that also has feedback, which adjusts the output to take account of how it affects the load, and how the load itself may vary unpredictably; the load is considered to belong to the external environment of the system. In a feed-forward system, the control variable adjustment is not error-based. Instead it is based on knowledge about the process in the form of a mathematical model of the process and knowledge about or measurements of the process disturbances. Some prerequisites are needed for control scheme to be reliable by pure feed-forward without feedback: the external command or controlling signal must be available, and the effect of the output of the system on the load should be known (that usually means that the load must be predictably unchanging with time). Sometimes pure feed-forward control without feedback is called 'ballistic', because once a control signal has been sent, it cannot be further adjusted; any corrective adjustment must be by way of a new control signal. In contrast 'cruise control' adjusts the output in response to the load that it encounters, by a feedback mechanism. These systems could relate to control theory, physiology or computing."
"http://dbpedia.org/resource/Backpropagation"	"Backpropagation"	"Artificial neural networks"	"Backpropagation, an abbreviation for ""backward propagation of errors"", is a common method of training artificial neural networks used in conjunction with an optimization method such as gradient descent. The method calculates the gradient of a loss function with respect to all the weights in the network, so that the gradient is fed to the optimization method which in turn uses it to update the weights, in an attempt to minimize the loss function. Backpropagation requires a known, desired output for each input value in order to calculate the loss function gradient. It is therefore usually considered to be a supervised learning method, although it is also used in some unsupervised networks such as autoencoders. It is a generalization of the delta rule to multi-layered feedforward networks, made possible by using the chain rule to iteratively compute gradients for each layer. Backpropagation requires that the activation function used by the artificial neurons (or ""nodes"") be differentiable."
"http://dbpedia.org/resource/Nervous_system_network_models"	"Nervous system network models"	"Artificial neural networks"	"Network of human nervous system comprises nodes (for example, neurons) that are connected by links (for example, synapses). The connectivity may be viewed anatomically, functionally, or electrophysiologically. These are presented in several Wikipedia articles that include Connectionism (a.k.a. Parallel Distributed Processing (PDP)), Biological neural network, Artificial neural network (a.k.a. Neural network), Computational neuroscience, as well as in several books by Ascoli, G. A. (2002), Sterratt, D., Graham, B., Gillies, A., & Willshaw, D. (2011), Gerstner, W., & Kistler, W. (2002), and Rumelhart, J. L., McClelland, J. L., and PDP Research Group (1986) among others. The focus of this article is a comprehensive view of modeling a neural network (technically neuronal network based on neuron model). Once an approach based on the perspective and connectivity is chosen, the models are developed at microscopic (ion and neuron), mesoscopic (functional or population), or macroscopic (system) levels. Computational modeling refers to models that are developed using computing tools."
"http://dbpedia.org/resource/Generalized_Hebbian_Algorithm"	"Generalized Hebbian Algorithm"	"Artificial neural networks"	"The Generalized Hebbian Algorithm (GHA), also known in the literature as Sanger's rule, is a linear feedforward neural network model for unsupervised learning with applications primarily in principal components analysis. First defined in 1989, it is similar to Oja's rule in its formulation and stability, except it can be applied to networks with multiple outputs. The name originates because of the similarity between the algorithm and a hypothesis made by Donald Hebb about the way in which synaptic strengths in the brain are modified in response to experience, i.e., that changes are proportional to the correlation between the firing of pre- and post-synaptic neurons."
"http://dbpedia.org/resource/Generative_topographic_map"	"Generative topographic map"	"Artificial neural networks"	"Generative topographic map (GTM) is a machine learning method that is a probabilistic counterpart of the self-organizing map (SOM), is probably convergent and does not require a shrinking neighborhood or a decreasing step size. It is a generative model: the data is assumed to arise by first probabilistically picking a point in a low-dimensional space, mapping the point to the observed high-dimensional input space (via a smooth function), then adding noise in that space. The parameters of the low-dimensional probability distribution, the smooth map and the noise are all learned from the training data using the expectation-maximization (EM) algorithm. GTM was introduced in 1996 in a paper by Christopher Bishop, Markus Svensen, and Christopher K. I. Williams."
"http://dbpedia.org/resource/Computational_cybernetics"	"Computational cybernetics"	"Artificial neural networks"	"Computational cybernetics is the integration of cybernetics and computational intelligence techniques. Though the term Cybernetics entered the technical lexicon in the 1940s and 1950s,it was first used informally as a popular noun in the 1960s, when it became associated with computers, robotics, Artificial Intelligence and Science fiction. While Cybernetics is primarily concerned with the study of control systems, computational cybernetics focuses on their automatic (complex, autonomic, flexible, adaptive) operation. Furthermore, computational cybernetics covers not only mechanical, but biological (living), social and economical systems. To achieve this goal, it uses research from the fields of communication theory, signal processing, information technology, control theory, the theory of adaptive systems, the theory of complex systems (game theory, and operational research)."
"http://dbpedia.org/resource/Quantum_neural_network"	"Quantum neural network"	"Artificial neural networks"	"Quantum neural networks (QNNs) are neural network models which are based on the principles of quantum mechanics. There are two different approaches to QNN research, one exploiting quantum information processing to improve existing neural network models (sometimes also vice versa), and the other one searching for potential quantum effects in the brain."
"http://dbpedia.org/resource/Multilayer_perceptron"	"Multilayer perceptron"	"Artificial neural networks"	"A multilayer perceptron (MLP) is a feedforward artificial neural network model that maps sets of input data onto a set of appropriate outputs. An MLP consists of multiple layers of nodes in a directed graph, with each layer fully connected to the next one. Except for the input nodes, each node is a neuron (or processing element) with a nonlinear activation function. MLP utilizes a supervised learning technique called backpropagation for training the network. MLP is a modification of the standard linear perceptron and can distinguish data that are not linearly separable."
"http://dbpedia.org/resource/Hybrid_neural_network"	"Hybrid neural network"	"Artificial neural networks"	"The term hybrid neural network can have two meanings: 1.  
* biological neural networks interacting with artificial neuronal models, and 2.  
* Artificial neural networks with a symbolic part (or, conversely, symbolic computations with a connectionist part). As for the first meaning, the artificial neurons and synapses in hybrid networks can be digital or analog. For the digital variant voltage clamps are used to monitor the membrane potential of neurons, to computationally simulate artificial neurons and synapses and to stimulate biological neurons by inducing synaptic. For the analog variant, specially designed electronic circuits connect to a network of living neurons through electrodes. As for the second meaning, incorporating elements of symbolic computation and artificial neural networks into one model was an attempt to combine the advantages of both paradigms while avoid the shortcomings. Symbolic representations have advantages with respect to explicit, direct control, fast initial coding, dynamic variable binding and knowledge abstraction. Representations of artificial neural networks, on the other hand, show advantages for biological plausibility, learning, robustness (fault-tolerant processing and graceful decay), and generalization to similar input. Since the early 1990s many attempts have been made to reconcile the two approaches."
"http://dbpedia.org/resource/Radial_basis_function_network"	"Radial basis function network"	"Artificial neural networks"	"In the field of mathematical modeling, a radial basis function network is an artificial neural network that uses radial basis functions as activation functions. The output of the network is a linear combination of radial basis functions of the inputs and neuron parameters. Radial basis function networks have many uses, including function approximation, time series prediction, classification, and system control. They were first formulated in a 1988 paper by Broomhead and Lowe, both researchers at the Royal Signals and Radar Establishment."
"http://dbpedia.org/resource/ALOPEX"	"ALOPEX"	"Artificial neural networks"	"ALOPEX (an acronym from ""ALgorithms Of Pattern EXtraction"") is a correlation based machine learning algorithm first proposed by Tzanakou and Harth in 1974."
"http://dbpedia.org/resource/Hierarchical_temporal_memory"	"Hierarchical temporal memory"	"Artificial neural networks"	"Hierarchical temporal memory (HTM) is an unsupervised to semi-supervised online machine learning model developed by Jeff Hawkins and Dileep George of Numenta, Inc. that models some of the structural and algorithmic properties of the neocortex. HTM is a biomimetic model based on the memory-prediction theory of brain function described by Jeff Hawkins in his book On Intelligence. HTM is a method for discovering and inferring the high-level causes of observed input patterns and sequences, thus building an increasingly complex model of the world. Jeff Hawkins states that HTM does not present any new idea or theory, but combines existing ideas to mimic the neocortex with a simple design that provides a large range of capabilities. HTM combines and extends approaches used in Sparse distributed memory, Bayesian networks, spatial and temporal clustering algorithms, while using a tree-shaped hierarchy of nodes that is common in neural networks."
"http://dbpedia.org/resource/Interactive_activation_and_competition_networks"	"Interactive activation and competition networks"	"Artificial neural networks"	"Interactive activation and competition (IAC) networks are artificial neural networks used to model memory and intuitive generalizations. They are made up of nodes or artificial neurons which are arrayed and activated in ways that emulate the behaviors of human memory. The IAC model is used by the parallel distributed processing (PDP) Group and is associated with James L. McClelland and David E. Rumelhart; it is described in detail in their book Explorations in Parallel Distributed Processing: A Handbook of Models, Programs, and Exercises. This model does not contradict any currently known biological data or theories, and its performance is close enough to human performance as to warrant further investigation."
"http://dbpedia.org/resource/Neuroevolution_of_augmenting_topologies"	"Neuroevolution of augmenting topologies"	"Artificial neural networks"	"NeuroEvolution of Augmenting Topologies (NEAT) is a genetic algorithm for the generation of evolving artificial neural networks (a neuroevolution technique) developed by Ken Stanley in 2002 while at The University of Texas at Austin. It alters both the weighting parameters and structures of networks, attempting to find a balance between the fitness of evolved solutions and their diversity. It is based on applying three key techniques: tracking genes with history markers to allow crossover among topologies, applying speciation (the evolution of species) to preserve innovations, and developing topologies incrementally from simple initial structures (""complexifying"")."
"http://dbpedia.org/resource/MoneyBee"	"MoneyBee"	"Artificial neural networks"	"MoneyBee is a distributed computing project in the fields of economics, finance and stock markets, that generates stock forecasts by application of artificial intelligence with the aid of artificial neural networks. MoneyBee acts as a screensaver. The project is run by i42 Informationsmanagement GmbH, a consulting private company from Mannheim, Germany. The project was suspended with a standing invitation for any interested in joining the MoneyBee2 project, but MoneyBee2 seems to have been abandoned in early 2010."
"http://dbpedia.org/resource/Universal_approximation_theorem"	"Universal approximation theorem"	"Artificial neural networks"	"In the mathematical theory of artificial neural networks, the universal approximation theorem states that a feed-forward network with a single hidden layer containing a finite number of neurons (i.e., a multilayer perceptron), can approximate continuous functions on compact subsets of Rn, under mild assumptions on the activation function. The theorem thus states that simple neural networks can represent a wide variety of interesting functions when given appropriate parameters; however, it does not touch upon the algorithmic learnability of those parameters. One of the first versions of the theorem was proved by George Cybenko in 1989 for sigmoid activation functions. Kurt Hornik showed in 1991 that it is not the specific choice of the activation function, but rather the multilayer feedforward architecture itself which gives neural networks the potential of being universal approximators. The output units are always assumed to be linear. For notational convenience, only the single output case will be shown. The general case can easily be deduced from the single output case."
"http://dbpedia.org/resource/Lernmatrix"	"Lernmatrix"	"Artificial neural networks"	"Lernmatrix, an associative-memory-like architecture of an artificial neural network, invented around 1960 by Karl Steinbuch."
"http://dbpedia.org/resource/Reservoir_computing"	"Reservoir computing"	"Artificial neural networks"	"Reservoir computing is a framework for computation that may be viewed as an extension of neural networks.Typically an input signal is fed into a fixed (random) dynamical system called a reservoir and the dynamics of the reservoir map the input to a higher dimension.Then a simple readout mechanism is trained to read the state of the reservoir and map it to the desired output.The main benefit is that the training is performed only at the readout stage and the reservoir is fixed.Liquid-state machines and echo state networksare two major types of reservoir computing."
"http://dbpedia.org/resource/Dropout_(neural_networks)"	"Dropout (neural networks)"	"Artificial neural networks"	"Dropout is a regularization technique for reducing overfitting in neural networks by preventing complex co-adaptations on training data. It is a very efficient way of performing model averaging with neural networks. The term ""dropout"" refers to dropping out units (both hidden and visible) in a neural network."
"http://dbpedia.org/resource/Radial_basis_function"	"Radial basis function"	"Artificial neural networks"	"A radial basis function (RBF) is a real-valued function whose value depends only on the distance from the origin, so that ; or alternatively on the distance from some other point c, called a center, so that . Any function that satisfies the property is a radial function. The norm is usually Euclidean distance, although other distance functions are also possible. Sums of radial basis functions are typically used to approximate given functions. This approximation process can also be interpreted as a simple kind of neural network; this was the context in which they originally surfaced, in work by David Broomhead and David Lowe in 1988, which stemmed from Michael J. D. Powell's seminal research from 1977.RBFs are also used as a kernel in support vector classification."
"http://dbpedia.org/resource/Sigmoid_function"	"Sigmoid function"	"Artificial neural networks"	"A sigmoid function is a mathematical function having an ""S"" shaped curve (sigmoid curve). Often, sigmoid function refers to the special case of the logistic function shown in the first figure and defined by the formula Other examples of similar shapes include the Gompertz curve (used in modeling systems that saturate at large values of t) and the ogee curve (used in the spillway of some dams). Sigmoid functions have finite limits at negative infinity and infinity, most often going either from 0 to 1 or from −1 to 1, depending on convention. A wide variety of sigmoid functions have been used as the activation function of artificial neurons, including the logistic and hyperbolic tangent functions. Sigmoid curves are also common in statistics as cumulative distribution functions (which go from 0 to 1), such as the integrals of the logistic distribution, the normal distribution, and Student's t probability density functions."
"http://dbpedia.org/resource/Softmax_function"	"Softmax function"	"Artificial neural networks"	"In mathematics, in particular probability theory and related fields, the softmax function, or normalized exponential, is a generalization of the logistic function that ""squashes"" a K-dimensional vector of arbitrary real values to a K-dimensional vector of real values in the range (0, 1) that add up to 1. The function is given by for j = 1, …, K. The softmax function is the gradient-log-normalizer of the categorical probability distribution. For this reason, the softmax function is used in various probabilistic multiclass classification methods including multinomial logistic regression, multiclass linear discriminant analysis, naive Bayes classifiers and artificial neural networks. Specifically, in multinomial logistic regression and linear discriminant analysis, the input to the function is the result of K distinct linear functions, and the predicted probability for the j'th class given a sample vector x is: This can be seen as the composition of K linear functions and the softmax function (where denotes the inner product of and )."
"http://dbpedia.org/resource/Stochastic_neural_analog_reinforcement_calculator"	"Stochastic neural analog reinforcement calculator"	"Artificial neural networks"	"SNARC (Stochastic Neural Analog Reinforcement Calculator) is a neural net machine designed by Marvin Lee Minsky. It is a randomly connected network of Hebb synapses. It was implemented by Minsky while he was a student, in hardware using vacuum tubes, and was possibly the first artificial self-learning machine."
"http://dbpedia.org/resource/Quickprop"	"Quickprop"	"Artificial neural networks"	"Quickprop is an iterative method for determining the minimum of the loss function of an artificial neural network, following an algorithm inspired by the Newton's method. Sometimes, the algorithm is classified to the group of the second order learning methods. It follows a quadratic approximation of the previous gradient step and the current gradient, which is expected to be closed to the minimum of the loss function, under the assumption that the loss function is locally approximately square, trying to describe it by means of an upwardly open parabola. The minimum is sought in the vertex of the parabola. The procedure requires only local information of the artificial neuron to which it is applied.The k-th approximation step is given by: Being the neuron j weight of its i input and E is the loss function. The Quickprop algorithm is an implementation of the error backpropagation algorithm, but the network can behave chaotically during the learning phase due to large step sizes."
"http://dbpedia.org/resource/Ni1000"	"Ni1000"	"Artificial neural networks"	"The Ni1000 is an artificial neural network chip developed by Nestor Corporation. The chip is aimed at image analysis applications, contains more than 3 million transistors and can analyze patterns at the rate of 40,000 per second. Prototypes running with Nestor's OCR software in 1994 were capable of recognizing around 100 handwritten characters per second."
"http://dbpedia.org/resource/Learning_vector_quantization"	"Learning vector quantization"	"Artificial neural networks"	"In computer science, learning vector quantization (LVQ), is a prototype-based supervised classification algorithm. LVQ is the supervised counterpart of vector quantization systems."
"http://dbpedia.org/resource/ND4S"	"ND4S"	"Artificial neural networks"	"ND4S is a free, open-source extension of the Scala programming language operating on the Java Virtual Machine—though it is compatible with both Java and Clojure. ND4S is a scientific computing library for linear algebra and matrix manipulation in a production environment, integrating with Hadoop and Spark to work with distributed GPUs. It supports n-dimensional arrays for JVM-based languages. ND4S has primarily been developed by the group in San Francisco that built Deeplearning4j, led by Adam Gibson. It was created under an Apache Software Foundation license."
"http://dbpedia.org/resource/Artisto"	"Artisto"	"Artificial neural networks"	"Artisto is a video processing application with art and movie effects filters based on neural network algorithms created in 2016 by Mail.ru Group machine learning specialists. At the moment the application can process videos up to 10 seconds long and offers users 21 filters, including those based on the works of famous artists (e.g. Blue Dream — Pablo Picasso), theme-based (Rio-2016 — related to the 2016 Summer Olympics in Rio de Janeiro) and others. The app works with both pre-recorded videos and videos recorded with the application."
"http://dbpedia.org/resource/ND4J_(software)"	"ND4J (software)"	"Artificial neural networks"	"ND4J is a scientific computing library, written in the programming language C++, operating on the Java virtual machine (JVM), and compatible with the languages Java, Scala, and Clojure. ND4J is for performing linear algebra and matrix manipulation in a production environment, integrating with Apache Hadoop and Spark to work with distributed central processing units (CPUs) or graphics processing units (GPUs). It supports n-dimensional arrays for JVM-based languages. ND4J is free and open-source software, released under Apache License 2.0, and developed mostly by the group in San Francisco that built Deeplearning4j, led by Adam Gibson. It was created under an Apache Software Foundation license."
"http://dbpedia.org/resource/ADALINE"	"ADALINE"	"Artificial neural networks"	"ADALINE (Adaptive Linear Neuron or later Adaptive Linear Element) is an early single-layer artificial neural network and the name of the physical device that implemented this network. The network uses memistors. It was developed by Professor Bernard Widrow and his graduate student Ted Hoff at Stanford University in 1960. It is based on the McCulloch–Pitts neuron. It consists of a weight, a bias and a summation function. The difference between Adaline and the standard (McCulloch–Pitts) perceptron is that in the learning phase the weights are adjusted according to the weighted sum of the inputs (the net). In the standard perceptron, the net is passed to the activation (transfer) function and the function's output is used for adjusting the weights. There also exists an extension known as Madaline."
"http://dbpedia.org/resource/Adaptive_resonance_theory"	"Adaptive resonance theory"	"Artificial neural networks"	"Adaptive resonance theory (ART) is a theory developed by Stephen Grossberg and Gail Carpenter on aspects of how the brain processes information. It describes a number of neural network models which use supervised and unsupervised learning methods, and address problems such as pattern recognition and prediction. The primary intuition behind the ART model is that object identification and recognition generally occur as a result of the interaction of 'top-down' observer expectations with 'bottom-up' sensory information. The model postulates that 'top-down' expectations take the form of a memory template or prototype that is then compared with the actual features of an object as detected by the senses. This comparison gives rise to a measure of category belongingness. As long as this difference between sensation and expectation does not exceed a set threshold called the 'vigilance parameter', the sensed object will be considered a member of the expected class. The system thus offers a solution to the 'plasticity/stability' problem, i.e. the problem of acquiring new knowledge without disrupting existing knowledge."
"http://dbpedia.org/resource/Echo_state_network"	"Echo state network"	"Artificial neural networks"	"The echo state network (ESN) is a recurrent neural network with a sparsely connected hidden layer (with typically 1% connectivity). The connectivity and weights of hidden neurons are fixed and randomly assigned. The weights of output neurons can be learned so that the network can (re)produce specific temporal patterns. The main interest of this network is that although its behaviour is non-linear, the only parameters are the weights of the output layer. Thus, the error function is quadratic with respect to the parameter vector and can be differentiated easily to a linear system."
"http://dbpedia.org/resource/Spiking_neural_network"	"Spiking neural network"	"Artificial neural networks"	"Spiking neural networks (SNNs) fall into the third generation of neural network models, increasing the level of realism in a neural simulation. In addition to neuronal and synaptic state, SNNs also incorporate the concept of time into their operating model. The idea is that neurons in the SNN do not fire at each propagation cycle (as it happens with typical multi-layer perceptron networks), but rather fire only when a membrane potential – an intrinsic quality of the neuron related to its membrane electrical charge – reaches a specific value. When a neuron fires, it generates a signal which travels to other neurons which, in turn, increase or decrease their potentials in accordance with this signal. In the context of spiking neural networks, the current activation level (modeled as some differential equation) is normally considered to be the neuron's state, with incoming spikes pushing this value higher, and then either firing or decaying over time. Various coding methods exist for interpreting the outgoing spike train as a real-value number, either relying on the frequency of spikes, or the timing between spikes, to encode information."
"http://dbpedia.org/resource/Delta_rule"	"Delta rule"	"Artificial neural networks"	"In machine learning, the delta rule is a gradient descent learning rule for updating the weights of the inputs to artificial neurons in a single-layer neural network. It is a special case of the more general backpropagation algorithm. For a neuron with activation function , the delta rule for 's th weight is given by , where It holds that and . The delta rule is commonly stated in simplified form for a neuron with a linear activation function as While the delta rule is similar to the perceptron's update rule, the derivation is different. The perceptron uses the Heaviside step function as the activation function , and that means that does not exist at zero, and is equal to zero elsewhere, which makes the direct application of the delta rule impossible."
"http://dbpedia.org/resource/Helmholtz_machine"	"Helmholtz machine"	"Artificial neural networks"	"The Helmholtz machine is a type of artificial neural network that can account for the hidden structure of a set of data by being trained to create a generative model of the original set of data. The hope is that by learning economical representations of the data, the underlying structure of the generative model should reasonably approximate the hidden structure of the data set. A Helmholtz machine contains two networks, a bottom-up recognition network that takes the data as input and produces a distribution over hidden variables, and a top-down ""generative"" network that generates values of the hidden variables and the data itself. Helmholtz machines are usually trained using an unsupervised learning algorithm, such as the wake-sleep algorithm. Helmholtz machines may also be used in applications requiring a supervised learning algorithm (e.g. character recognition, or position-invariant recognition of an object within a field)."
"http://dbpedia.org/resource/Stochastic_neural_network"	"Stochastic neural network"	"Artificial neural networks"	"Stochastic neural networks are a type of artificial neural networks built by introducing random variations into the network, either by giving the network's neurons stochastic transfer functions, or by giving them stochastic weights. This makes them useful tools for optimization problems, since the random fluctuations help it escape from local minima. An example of a neural network using stochastic transfer functions is a Boltzmann machine. Each neuron is binary valued, and the chance of it firing depends on the other neurons in the network. Stochastic neural networks have found applications in risk management, oncology, bioinformatics, and other similar fields."
"http://dbpedia.org/resource/Autoencoder"	"Autoencoder"	"Artificial neural networks"	"An autoencoder, autoassociator or Diabolo network is an artificial neural network used for unsupervised learning of efficient codings.The aim of an autoencoder is to learn a representation (encoding) for a set of data, typically for the purpose of dimensionality reduction. Recently, the autoencoder concept has become more widely used for learning generative models of data."
"http://dbpedia.org/resource/Adaptive_neuro_fuzzy_inference_system"	"Adaptive neuro fuzzy inference system"	"Artificial neural networks"	"An adaptive neuro-fuzzy inference system or adaptive network-based fuzzy inference system (ANFIS) is a kind of artificial neural network that is based on Takagi–Sugeno fuzzy inference system. The technique was developed in the early 1990s. Since it integrates both neural networks and fuzzy logic principles, it has potential to capture the benefits of both in a single framework. Its inference system corresponds to a set of fuzzy IF–THEN rules that have learning capability to approximate nonlinear functions. Hence, ANFIS is considered to be a universal estimator. For using the ANFIS in a more efficient and optimal way, one can use the best parameters obtained by genetic algorithm."
"http://dbpedia.org/resource/Group_method_of_data_handling"	"Group method of data handling"	"Artificial neural networks"	"Group method of data handling (GMDH) is a family of inductive algorithms for computer-based mathematical modeling of multi-parametric datasets that features fully automatic structural and parametric optimization of models. GMDH is used in such fields as data mining, knowledge discovery, prediction, complex systems modeling, optimization and pattern recognition. GMDH algorithms are characterized by inductive procedure that performs sorting-out of gradually complicated polynomial models and selecting the best solution by means of the so-called external criterion. A GMDH model with multiple inputs and one output is a subset of components of the base function (1): where f are elementary functions dependent on different sets of inputs, a are coefficients and m is the number of the base function components. In order to find the best solution GMDH algorithms consider various component subsets of the base function (1) called partial models. Coefficients of these models are estimated by the least squares method. GMDH algorithms gradually increase the number of partial model components and find a model structure with optimal complexity indicated by the minimum value of an external criterion. This process is called self-organization of models. The most popular base function used in GMDH is the gradually complicated Kolmogorov-Gabor polynomial (2): The resulting models are also known as polynomial neural networks. Jürgen Schmidhuber cites GDMH as one of the earliest deep learning methods, remarking that it was used to train eight-layer neural nets as early as 1971."
"http://dbpedia.org/resource/Compositional_pattern-producing_network"	"Compositional pattern-producing network"	"Artificial neural networks"	"Compositional pattern-producing networks (CPPNs) are a variation of artificial neural networks (ANNs) that differ in their set of activation functions and how they are applied. While ANNs often contain only sigmoid functions and sometimes Gaussian functions, CPPNs can include both types of functions and many others. The choice of functions for the canonical set can be biased toward specific types of patterns and regularities. For example, periodic functions such as sine produce segmented patterns with repetitions, while symmetric functions such as Gaussian produce symmetric patterns. Linear functions can be employed to produce linear or fractal-like patterns. Thus, the architect of a CPPN-based genetic art system can bias the types of patterns it generates by deciding the set of canonical functions to include. Furthermore, unlike typical ANNs, CPPNs are applied across the entire space of possible inputs so that they can represent a complete image. Since they are compositions of functions, CPPNs in effect encode images at infinite resolution and can be sampled for a particular display at whatever resolution is optimal. CPPNs can be evolved through neuroevolution techniques such as neuroevolution of augmenting topologies (called CPPN-NEAT). CPPNs have been shown to be a very powerful encoding when evolving the following: 
*  Neural networks, via the HyperNEAT algorithm, 
*  2D images, on ""PicBreeder.org"", 
*  3D objects, on ""EndlessForms.com"", 
*  Robot morphologies Rigid Robots Soft Robots."
"http://dbpedia.org/resource/CoDi"	"CoDi"	"Artificial neural networks"	"CoDi is a cellular automaton (CA) model for spiking neural networks (SNNs). CoDi is an acronym for Collect and Distribute, referring to the signals and spikes in a neural network. CoDi uses a von Neumann neighborhood modified for a three-dimensional space; each cell looks at the states of its six orthogonal neighbors and its own state. In a growth phase a neural network is grown in the CA-space based on an underlying chromosome. There are four types of cells: neuron body, axon, dendrite and blank. The growth phase is followed by a signaling- or processing-phase. Signals are distributed from the neuron bodies via their axon tree and collected from connection dendrites. These two basic interactions cover every case, and they can be expressed simply, using a small number of rules."
"http://dbpedia.org/resource/Learning_rule"	"Learning rule"	"Artificial neural networks"	"Learning rule or Learning process is a method or a mathematical logic which improves the artificial neural network's performance and usually this rule is applied repeatedly over the network. It is done by updating the weights and bias levels of a network when a network is simulated in a specific data environment. A learning rule may accept existing condition ( weights and bias ) of the network and will compare the expected result and actual result of the network to give new and improved values for weights and bias.  Depending on the complexity of actual model, which is being simulated, the learning rule of the network can be as simple as an XOR gate or Mean Squared Error or it can be the result of multiple differential equations. The learning rule is one of the factors which decides how fast or how accurate the artificial network can be developed. Depending upon the process to develop the network there are three main models of machine learning: 1.  
*  Unsupervised learning 2.  
*  Supervised learning 3.  
*  Reinforcement learning"
"http://dbpedia.org/resource/Optical_neural_network"	"Optical neural network"	"Artificial neural networks"	"An optical neural network is a physical implementation of an artificial neural network with optical components. Some artificial neural networks that have been implemented as optical neural networks include the Hopfield neural network and the Kohonen self-organizing map with liquid crystals . Biological neural networks function on an electrochemical basis, while optical neural networks use electromagnetic waves. Optical interfaces to biological neural networks can be created with optogenetics, but is not the same as an optical neural networks. In biological neural networks there exist a lot of different mechanisms for dynamically changing the state of the neurons, these include short-term and long-term synaptic plasticity. Synaptic plasticity is among the electrophysiological phenomena used to control the efficiency of synaptic transmission, long-term for learning and memory, and short-term for short transient changes in synaptic transmission efficiency. Implementing this with optical components is difficult, and ideally requires advanced photonic materials. Properties that might be desirable in photonic materials for optical neural networks include the ability to change their efficiency of transmitting light, based on the intensity of incoming light. There is one recent (2007) model of Optical Neural Network: the Programmable Optical Array/Analogic Computer (POAC). It had been implemented in the year 2000 and reported based on modified Joint Fourier Transform Correlator (JTC) and Bacteriorhodopsin (BR) as a holographic optical memory. Full parallelism, large array size and the speed of light are three promises offered by POAC to implement an optical CNN. They had been investigated during the last years with their practical limitations and considerations yielding the design of the first portable POAC version. The practical details: hardware (optical setups) and software (optical templates) are published. However, POAC is a general purpose and programmable array computer that has a wide range of applications including: 
*  image processing 
*  pattern recognition 
*  target tracking 
*  real-time video processing 
*  document security 
*  optical switching"
"http://dbpedia.org/resource/Early_stopping"	"Early stopping"	"Artificial neural networks"	"In machine learning, early stopping is a form of regularization used to avoid overfitting when training a learner with an iterative method, such as gradient descent. Such methods update the learner so as to make it better fit the training data with each iteration. Up to a point, this improves the learner's performance on data outside of the training set. Past that point, however, improving the learner's fit to the training data comes at the expense of increased generalization error. Early stopping rules provide guidance as to how many iterations can be run before the learner begins to over-fit. Early stopping rules have been employed in many different machine learning methods, with varying amounts of theoretical foundation."
"http://dbpedia.org/resource/Artificial_neuron"	"Artificial neuron"	"Artificial neural networks"	"An artificial neuron is a mathematical function conceived as a model of biological neurons. Artificial neurons are the constitutive units in an artificial neural network. Depending on the specific model used they may be called a semi-linear unit, Nv neuron, binary neuron, linear threshold function, or McCulloch–Pitts (MCP) neuron. The artificial neuron receives one or more inputs (representing dendrites) and sums them to produce an output (representing a neuron's axon). Usually the sums of each node are weighted, and the sum is passed through a non-linear function known as an activation function or transfer function. The transfer functions usually have a sigmoid shape, but they may also take the form of other non-linear functions, piecewise linear functions, or . They are also often monotonically increasing, continuous, differentiable and bounded. The thresholding function is inspired to build logic gates referred to as threshold logic; with a renewed interest to build logic circuit resembling brain processing. For example, new devices such as memristors have been extensively used to develop such logic in the recent times. The artificial neuron transfer function should not be confused with a linear system's transfer function."
"http://dbpedia.org/resource/Deep_belief_network"	"Deep belief network"	"Artificial neural networks"	"In machine learning, a deep belief network (DBN) is a generative graphical model, or alternatively a type of deep neural network, composed of multiple layers of latent variables (""hidden units""), with connections between the layers but not between units within each layer. When trained on a set of examples in an unsupervised way, a DBN can learn to probabilistically reconstruct its inputs. The layers then act as feature detectors on inputs. After this learning step, a DBN can be further trained in a supervised way to perform classification. DBNs can be viewed as a composition of simple, unsupervised networks such as restricted Boltzmann machines (RBMs) or autoencoders, where each sub-network's hidden layer serves as the visible layer for the next. This also leads to a fast, layer-by-layer unsupervised training procedure, where contrastive divergence is applied to each sub-network in turn, starting from the ""lowest"" pair of layers (the lowest visible layer being a training set). The observation, due to Yee-Whye Teh, Geoffrey Hinton's student, that DBNs can be trained greedily, one layer at a time, led to one of the first effective deep learning algorithms."
"http://dbpedia.org/resource/Rectifier_(neural_networks)"	"Rectifier (neural networks)"	"Artificial neural networks"	"In the context of artificial neural networks, the rectifier is an activation function defined as where x is the input to a neuron. This is also known as a ramp function and is analogous to half-wave rectification in electrical engineering. This activation function is first introduced to a dynamical network byHahnloser et al in a 2000 paper in Naturewith strong biological motivations and mathematical justifications.It has been used in convolutional networksthan the widely used logistic sigmoid (which is inspired by probability theory; see logistic regression) and its more practicalcounterpart, the hyperbolic tangent. The rectifier is, as of 2015, the most popular activation function for deep neural networks. A unit employing the rectifier is also called a rectified linear unit (ReLU). A smooth approximation to the rectifier is the analytic function which is called the softplus function.The derivative of softplus is , i.e. the logistic function. Rectified linear units find applications in computer visionand speech recognition using deep neural nets."
"http://dbpedia.org/resource/Neural_gas"	"Neural gas"	"Artificial neural networks"	"Neural gas is an artificial neural network, inspired by the self-organizing map and introduced in 1991 by Thomas Martinetz and Klaus Schulten. The neural gas is a simple algorithm for finding optimal data representations based on feature vectors. The algorithm was coined ""neural gas"" because of the dynamics of the feature vectors during the adaptation process, which distribute themselves like a gas within the data space. It is applied where data compression or vector quantization is an issue, for example speech recognition, image processing or pattern recognition. As a robustly converging alternative to the k-means clustering it is also used for cluster analysis."
"http://dbpedia.org/resource/Winner-take-all_(computing)"	"Winner-take-all (computing)"	"Artificial neural networks"	"Winner-take-all is a computational principle applied in computational models of neural networks by which neurons in a layer compete with each other for activation. In the classical form, only the neuron with the highest activation stays active while all other neurons shut down; however, other variations allow more than one neuron to be active, for example the soft winner take-all, by which a power function is applied to the neurons."
"http://dbpedia.org/resource/Artificial_Intelligence_System"	"Artificial Intelligence System"	"Artificial neural networks"	"Artificial Intelligence System (AIS) was a distributed computing project undertaken by Intelligence Realm, Inc. with the long-term goal of simulating the human brain in real time, complete with artificial consciousness and artificial general intelligence. They claimed to have found, in research, the ""mechanisms of knowledge representation in the brain which is equivalent to finding artificial intelligence"", before moving into the developmental phase."
"http://dbpedia.org/resource/Activation_function"	"Activation function"	"Artificial neural networks"	"In computational networks, the activation function of a node defines the output of that node given an input or set of inputs. A standard computer chip circuit can be seen as a digital network of activation functions that can be ""ON"" (1) or ""OFF"" (0), depending on input. This is similar to the behavior of the linear perceptron in neural networks. However, it is the nonlinear activation function that allows such networks to compute nontrivial problems using only a small number of nodes. In artificial neural networks this function is also called transfer function (not to be confused with a linear system’s transfer function)."
"http://dbpedia.org/resource/Promoter_based_genetic_algorithm"	"Promoter based genetic algorithm"	"Artificial neural networks"	"The promoter based genetic algorithm (PBGA) is a genetic algorithm for neuroevolution developed by F. Bellas and R.J. Duro in the Integrated Group for Engineering Research (GII) at the University of Coruña, in Spain. It evolves variable size feedforward artificial neural networks (ANN) that are encoded into sequences of genes for constructing a basic ANN unit. Each of these blocks is preceded by a gene promoter acting as an on/off switch that determines if that particular unit will be expressed or not."
"http://dbpedia.org/resource/Pulse-coupled_networks"	"Pulse-coupled networks"	"Artificial neural networks"	"Pulse-coupled networks or pulse-coupled neural networks (PCNNs) are neural models proposed by modeling a cat’s visual cortex, and developed for high-performance biomimetic image processing. In 1989, Eckhorn introduced a neural model to emulate the mechanism of cat’s visual cortex. The Eckhorn model provided a simple and effective tool for studying small mammal’s visual cortex, and was soon recognized as having significant application potential in image processing. In 1994, Johnson adapted the Eckhorn model to an image processing algorithm, calling this algorithm a pulse-coupled neural network. Over the past decade, PCNNs have been used in a variety of image processing applications, including: image segmentation, feature generation, face extraction, motion detection, region growing, and noise reduction. The basic property of the Eckhorn's linking-field model (LFM) is the coupling term. LFM is a modulation of the primary input by a biased offset factor driven by the linking input. These drive a threshold variable that decays from an initial high value. When the threshold drops below zero it is reset to a high value and the process starts over. This is different than the standard integrate-and-fire neural model, which accumulates the input until it passes an upper limit and effectively ""shorts out"" to cause the pulse. LFM uses this difference to sustain pulse bursts, something the standard model does not do on a single neuron level. It is valuable to understand, however, that a detailed analysis of the standard model must include a shunting term, due to the floating voltages level in the dendritic compartment(s), and in turn this causes an elegant multiple modulation effect that enables a true higher-order network (HON). Multidimensional pulse image processing of chemical structure data using PCNN has been discussed by Kinser, et al. A PCNN is a two-dimensional neural network. Each neuron in the network corresponds to one pixel in an input image, receiving its corresponding pixel’s color information (e.g. intensity) as an external stimulus. Each neuron also connects with its neighboring neurons, receiving local stimuli from them. The external and local stimuli are combined in an internal activation system, which accumulates the stimuli until it exceeds a dynamic threshold, resulting in a pulse output. Through iterative computation, PCNN neurons produce temporal series of pulse outputs. The temporal series of pulse outputs contain information of input images and can be used for various image processing applications, such as image segmentation and feature generation. Compared with conventional image processing means, PCNNs have several significant merits, including robustness against noise, independence of geometric variations in input patterns, capability of bridging minor intensity variations in input patterns, etc. A simplified PCNN called a spiking cortical model was developed in 2009. PCNNs are useful for image processing, as discussed in a book by Thomas Lindblad and Jason M. Kinser."
"http://dbpedia.org/resource/Linde–Buzo–Gray_algorithm"	"Linde–Buzo–Gray algorithm"	"Artificial neural networks"	"The Linde–Buzo–Gray algorithm (introduced by Yoseph Linde, Andrés Buzo and Robert M. Gray in 1980) is a vector quantization algorithm to derive a good codebook. It is similar to the k-means method in data clustering."
"http://dbpedia.org/resource/Restricted_Boltzmann_machine"	"Restricted Boltzmann machine"	"Artificial neural networks"	"A restricted Boltzmann machine (RBM) is a generative stochastic artificial neural network that can learn a probability distribution over its set of inputs. RBMs were initially invented under the name Harmonium by Paul Smolensky in 1986,and rose to prominence after Geoffrey Hinton and collaborators invented fast learning algorithms for them in the mid-2000s. RBMs have found applications in dimensionality reduction,classification,collaborative filtering, feature learningand topic modelling.They can be trained in either supervised or unsupervised ways, depending on the task. As their name implies, RBMs are a variant of Boltzmann machines, with the restriction that their neurons must form a bipartite graph: a pair of nodes from each of the two groups of units (commonly referred to as the ""visible"" and ""hidden"" units respectively) may have a symmetric connection between them; and there are no connections between nodes within a group. By contrast, ""unrestricted"" Boltzmann machines may have connections between hidden units. This restriction allows for more efficient training algorithms than are available for the general class of Boltzmann machines, in particular the gradient-based contrastive divergence algorithm. Restricted Boltzmann machines can also be used in deep learning networks. In particular, deep belief networks can be formed by ""stacking"" RBMs and optionally fine-tuning the resulting deep network with gradient descent and backpropagation."
"http://dbpedia.org/resource/Deeplearning4j"	"Deeplearning4j"	"Artificial neural networks"	"Deeplearning4j is a deep learning programming library written for Java and the Java virtual machine (JVM) and a computing framework with wide support for deep learning algorithms. Deeplearning4j includes implementations of the restricted Boltzmann machine, deep belief net, deep autoencoder, stacked denoising autoencoder and recursive neural tensor network, word2vec, doc2vec, and GloVe. These algorithms all include distributed parallel versions that integrate with Apache Hadoop and Spark. Deeplearning4j is open-source software released under Apache License 2.0, developed mainly by a machine learning group in San Francisco led by Adam Gibson. It is supported commercially by the startup Skymind. It is the only open-source project listed on Google's Word2vec page for its Java implementation."
"http://dbpedia.org/resource/OpenNN"	"OpenNN"	"Artificial neural networks"	"OpenNN (Open Neural Networks Library) is a software library written in the C++ programming language which implements neural networks, a main area of deep learning research. The library is open source, licensed under the GNU Lesser General Public License."
"http://dbpedia.org/resource/Instantaneously_trained_neural_networks"	"Instantaneously trained neural networks"	"Artificial neural networks"	"Instantaneously trained neural networks are feedforward artificial neural networks that create a new hidden neuron node for each novel training sample. The weights to this hidden neuron separate out not only this training sample but others that are near it, thus providing generalization. This training can be done in a variety of ways and the most popular network in this family is called the CC4 network where the separation is done using the nearest hyperplane that can be written down instantaneously. These networks use unary coding for an effective representation of the data sets. Instantaneously trained neural networks have been proposed as models of short term learning and used in web search, and financial time series prediction applications. They have also been used in instant classification of documents and for deep learning and data mining. As in other neural networks, their normal use is as software, but they have also been implemented in hardware using FPGAs and by optical implementation."
"http://dbpedia.org/resource/Boltzmann_machine"	"Boltzmann machine"	"Artificial neural networks"	"A Boltzmann machine is a type of stochastic recurrent neural network and Markov Random Field invented by Geoffrey Hinton and Terry Sejnowski in 1985. Boltzmann machines can be seen as the stochastic, generative counterpart of Hopfield nets. They were one of the first examples of a neural network capable of learning internal representations, and are able to represent and (given sufficient time) solve difficult combinatoric problems. They are theoretically intriguing because of the locality and Hebbian nature of their training algorithm, and because of their parallelism and the resemblance of their dynamics to simple physical processes. Due to a number of issues discussed below, Boltzmann machines with unconstrained connectivity have not proven useful for practical problems in machine learning or inference, but if the connectivity is properly constrained, the learning can be made efficient enough to be useful for practical problems They are named after the Boltzmann distribution in statistical mechanics, which is used in their sampling function."
"http://dbpedia.org/resource/Hopfield_network"	"Hopfield network"	"Artificial neural networks"	"A Hopfield network is a form of recurrent artificial neural network popularized by John Hopfield in 1982, but described earlier by Little in 1974. Hopfield nets serve as content-addressable memory systems with binary threshold nodes. They are guaranteed to converge to a local minimum, but will sometimes converge to a false pattern (wrong local minimum) rather than the stored pattern (expected local minimum). Hopfield networks also provide a model for understanding human memory."
"http://dbpedia.org/resource/Semantic_neural_network"	"Semantic neural network"	"Artificial neural networks"	"Semantic neural network (SNN) is based on John von Neumann's neural network [von Neumann, 1966] and Nikolai Amosov M-Network. There are limitations to a link topology for the von Neumann’s network but SNN accept a case without these limitations. Only logical values can be processed, but SNN accept that fuzzy values can be processed too. All neurons into the von Neumann network are synchronized by tacts. For further use of self-synchronizing circuit technique SNN accepts neurons can be self-running or synchronized. In contrast to the von Neumann network there are no limitations for topology of neurons for semantic networks. It leads to the impossibility of relative addressing of neurons as it was done by von Neumann. In this case an absolute readdressing should be used. Every neuron should have a unique identifier that would provide a direct access to another neuron. Of course, neurons interacting by axons-dendrites should have each other's identifiers. An absolute readdressing can be modulated by using neuron specificity as it was realized for biological neural networks. There’s no description for self-reflectiveness and self-modification abilities into the initial description of semantic networks [Dudar Z.V., Shuklin D.E., 2000]. But in [Shuklin D.E. 2004] a conclusion had been drawn about the necessity of introspection and self-modification abilities in the system. For maintenance of these abilities a concept of pointer to neuron is provided. Pointers represent virtual connections between neurons. In this model, bodies and signals transferring through the neurons connections represent a physical body, and virtual connections between neurons are representing an astral body. It is proposed to create models of artificial neuron networks on the basis of virtual machine supporting the opportunity for paranormal effects. SNN is generally used for natural language processing."
"http://dbpedia.org/resource/Long_short-term_memory"	"Long short-term memory"	"Artificial neural networks"	"Long short-term memory (LSTM) is a recurrent neural network (RNN) architecture (an artificial neural network) proposed in 1997 by Sepp Hochreiter and Jürgen Schmidhuber. Like most RNNs, an LSTM network is universal in the sense that given enough network units it can compute anything a conventional computer can compute, provided it has the proper weight matrix, which may be viewed as its program. Unlike traditional RNNs, an LSTM network is well-suited to learn from experience to classify, process and predict time series when there are very long time lags of unknown size between important events. This is one of the main reasons why LSTM outperforms alternative RNNs and Hidden Markov Models and other sequence learning methods in numerous applications. For example, LSTM achieved the best known results in unsegmented connected handwriting recognition, and in 2009 won the ICDAR handwriting competition. LSTM networks have also been used for automatic speech recognition, and were a major component of a network that in 2013 achieved a record 17.7% phoneme error rate on the classic TIMIT natural speech dataset. As of 2016, major technology companies including Google, Apple, Microsoft, and Baidu are using LSTM networks as fundamental components in new products."
"http://dbpedia.org/resource/Tensor_product_network"	"Tensor product network"	"Artificial neural networks"	"A tensor product network, in artificial neural networks, is a network that exploits the properties of tensors to model associative concepts such as variable assignment. Orthonormal vectors are chosen to model the ideas (such as variable names and target assignments), and the tensor product of these vectors construct a network whose mathematical properties allow the user to easily extract the association from it."
"http://dbpedia.org/resource/Neural_cryptography"	"Neural cryptography"	"Artificial neural networks"	"Neural cryptography is a branch of cryptography dedicated to analyzing the application of stochastic algorithms, especially artificial neural network algorithms, for use in encryption and cryptanalysis."
"http://dbpedia.org/resource/Backpropagation_through_time"	"Backpropagation through time"	"Artificial neural networks"	"Backpropagation through time (BPTT) is a gradient-based technique for training certain types of recurrent neural networks. It can be used to train Elman networks. The algorithm was independently derived by numerous researchers"
"http://dbpedia.org/resource/HyperNEAT"	"HyperNEAT"	"Artificial neural networks"	"Hypercube-based NEAT, or HyperNEAT, is a generative encoding that evolves artificial neural networks (ANNs) with the principles of the widely used NeuroEvolution of Augmented Topologies (NEAT) algorithm. It is a novel technique for evolving large-scale neural networks using the geometric regularities of the task domain. It uses Compositional Pattern Producing Networks  (CPPNs), which are used to generate the images for Picbreeder.org and shapes for EndlessForms.com. HyperNEAT has recently been extended to also evolve plastic ANNs  and to evolve the location of every neuron in the network."
"http://dbpedia.org/resource/Word2vec"	"Word2vec"	"Artificial neural networks"	"Word2vec is a group of related models that are used to produce word embeddings. These models are shallow, two-layer neural networks that are trained to reconstruct linguistic contexts of words. Word2vec takes as its input a large corpus of text and produces a high-dimensional space (typically of several hundred dimensions), with each unique word in the corpus being assigned a corresponding vector in the space. Word vectors are positioned in the vector space such that words that share common contexts in the corpus are located in close proximity to one another in the space. Word2vec was created by a team of researchers led by Tomas Mikolov at Google. The algorithm has been subsequently analysed and explained by other researchers and a Bayesian version of the algorithm is proposed as well. Embedding vectors created using the Word2vec algorithm have many advantages compared to earlier algorithms like Latent Semantic Analysis."
"http://dbpedia.org/resource/Hybrid_Kohonen_self-organizing_map"	"Hybrid Kohonen self-organizing map"	"Artificial neural networks"	"In artificial neural networks, a hybrid Kohonen self-organizing map is a type of self-organizing map (SOM) named for the Finnish professor Teuvo Kohonen, where the network architecture consists of an input layer fully connected to a 2–D SOM or Kohonen layer. The output from the Kohonen layer, which is the winning neuron, feeds into a hidden layer and finally into an output layer. In other words, the Kohonen SOM is the front–end, while the hidden and output layer of a multilayer perceptron is the back–end of thehybrid Kohonen SOM. The hybrid Kohonen SOM was first applied to machine vision systems for image classification and recognition. Hybrid Kohonen SOM has been used in weather prediction and especially in forecasting stock prices, which has made a challenging task considerably easier. It is fast and efficient with less classification error, hence is a better predictor, when compared to Kohonen SOM and backpropagation networks."
"http://dbpedia.org/resource/Vanishing_gradient_problem"	"Vanishing gradient problem"	"Artificial neural networks"	"In machine learning, the vanishing gradient problem is a difficulty found in training artificial neural networks with gradient-based learning methods and backpropagation. In such methods, each of the neural network's weights receives an update proportional to the gradient of the error function with respect to the current weight in each iteration of training. Traditional activation functions such as the hyperbolic tangent function have gradients in the range (−1, 1) or [0, 1), and backpropagation computes gradients by the chain rule. This has the effect of multiplying n of these small numbers to compute gradients of the ""front"" layers in an n-layer network, meaning that the gradient (error signal) decreases exponentially with n and the front layers train very slowly. With the advent of the back-propagation algorithm in the 1970s, many researchers tried to train supervised deep artificial neural networks from scratch, initially with little success. Sepp Hochreiter's diploma thesis of 1991 formally identified the reason for this failure in the ""vanishing gradient problem"", which not only affects many-layered feedforward networks, but also recurrent neural networks. The latter are trained by unfolding them into very deep feedforward networks, where a new layer is created for each time step of an input sequence processed by the network. When activation functions are used whose derivatives can take on larger values, one risks encountering the related exploding gradient problem."
"http://dbpedia.org/resource/IPO_underpricing_algorithm"	"IPO underpricing algorithm"	"Artificial neural networks"	"IPO underpricing is the increase in stock value from the initial offering price to the first-day closing price. Many believe that underpriced IPOs leave money on the table for corporations, but some believe that underpricing is inevitable. Investors state that underpricing signals high interest to the market which increases the demand. On the other hand, overpriced stocks will drop long-term as the price stabilizes so underpricing may keep the issuers safe from investor litigation."
"http://dbpedia.org/resource/Computational_neurogenetic_modeling"	"Computational neurogenetic modeling"	"Artificial neural networks"	"Computational neurogenetic modeling (CNGM) is concerned with the study and development of dynamic neuronal models for modeling brain functions with respect to genes and dynamic interactions between genes. These include neural network models and their integration with gene network models. This area brings together knowledge from various scientific disciplines, such as computer and information science, neuroscience and cognitive science, genetics and molecular biology, as well as engineering."
"http://dbpedia.org/resource/Bcpnn"	"Bcpnn"	"Artificial neural networks"	"A Bayesian Confidence Neural Network (BCPNN) is an artificial neural network inspired by Bayes' theorem: node activations represent probability (""confidence"") in the presence of input features or categories, synaptic weights are based on estimated correlations and the spread of activation corresponds to calculating posteriori probabilities. It was originally proposed by Anders Lansner and Örjan Ekeberg at KTH. The basic network is a feedforward neural network with continuous activation. This can be extended to include spiking units and hypercolumns, representing mutually exclusive or interval coded features. This network has been used for classification tasks and data mining, for example for discovery of adverse drug reactions. The units can also be connected as a recurrent neural network (losing the strict interpretation of their activations as probabilities) but becoming a possible abstract model of biological neural networks and memory."
"http://dbpedia.org/resource/Extension_neural_network"	"Extension neural network"	"Artificial neural networks"	"Extension neural network is a pattern recognition method found by M. H. Wang and C. P. Hung in 2003 to classify instances of data sets. Extension neural network is composed of artificial neural network and extension theory concepts. It uses the fast and adaptive learning capability of neural network and correlation estimation property of extension theory by calculating extension distance. ENN was used in: 
*  Failure detection in machinery. 
*  Tissue classification through MRI. 
*  Fault recognition in automotive engine. 
*  State of charge estimation in lead-acid battery. 
*  Classification with incomplete survey data."
"http://dbpedia.org/resource/Electricity_price_forecasting"	"Electricity price forecasting"	"Artificial neural networks"	"Electricity price forecasting (EPF) is a branch of energy forecasting which focuses on predicting the spot and forward prices in wholesale electricity markets. Over the last 15 years electricity price forecasts have become a fundamental input to energy companies’ decision-making mechanisms at the corporate level."
"http://dbpedia.org/resource/Jpred"	"Jpred"	"Artificial neural networks"	"Jpred v.4 is the latest version of the popular JPred Protein Secondary Structure Prediction Server which provides predictions by the JNet algorithm, one of the most accurate methods for secondary structure prediction, that has existed since 1998 in different versions. In addition to protein secondary structure, JPred also makes predictions of solvent accessibility and coiled-coil regions. The JPred service runs up to 134 000 jobs per month and has carried out over 2 million predictions in total for users in 179 countries. The current version of JPred (v4) has the following improvements and updates incorporated: 
*  Retrained on the latest UniRef90 and SCOPe/ASTRAL version of Jnet (v2.3.1) - mean secondary structure prediction accuracy of >82%. 
*  Upgraded the Web Server to the latest technologies (Bootstrap framework, JavaScript) and updating the web pages – improving the design and usability through implementing responsive technologies. 
*  Added RESTful API and mass-submission and results retrieval scripts - resulting in peak throughput above 20,000 predictions per day. 
*  Added prediction jobs monitoring tools. 
*  Upgraded the results reporting – both, on the web-site, and through the optional email summary reports: improved batch submission, added results summary preview through Jalview results visualization summary in SVG and adding full multiple sequence alignments into the reports. 
*  Improved help-pages, incorporating tool-tips, and adding one-page step-by-step tutorials. The JPred v3 followed on from previous versions of JPred developed and maintained by James Cuff and Jonathan Barber (see JPred References). This release added new functionality and fixed lots of bugs. The highlights are: 
*  New, friendlier user interface 
*  Retrained and optimised version of Jnet (v2) - mean secondary structure prediction accuracy of >81% 
*  Batch submission of jobs 
*  Better error checking of input sequences/alignments 
*  Predictions now (optionally) returned via e-mail 
*  Users may provide their own query names for each submission 
*  JPred now makes a prediction even when there are no PSI-BLAST hits to the query 
*  PS/PDF output now incorporates all the predictions The static HTML pages of JPred 2 are still available for reference. Sequence residues are categorised or assigned to one of the secondary structure elements, such as alpha-helix, beta-sheet and coiled-coil. Jnet uses two neural networks for its prediction. The first network is fed with a window of 17 residues over each amino acid in the alignment plus a conservation number. It uses a hidden layer of nine nodes and has three output nodes, one for each secondary structure element.The second network is fed with a window of 19 residues (the result of first network) plus the conservation number. It has a hidden layer with nine nodes and has three output nodes."
"http://dbpedia.org/resource/Committee_machine"	"Committee machine"	"Artificial neural networks"	"A committee machine is a type of artificial neural network using a divide and conquer strategy in which the responses of multiple neural networks (experts) are combined into a single response. The combined response of the committee machine is supposed to be superior to those of its constituent experts. Compare with ensembles of classifiers."
"http://dbpedia.org/resource/Infomax"	"Infomax"	"Artificial neural networks"	"Infomax is an optimization principle for artificial neural networks and other information processing systems. It prescribes that a function that maps a set of input values I to a set of output values O should be chosen or learned so as to maximize the average Shannon mutual information between I and O, subject to a set of specified constraints and/or noise processes. Infomax algorithms are learning algorithms that perform this optimization process. The principle was described by Linsker in 1988. Infomax, in its zero-noise limit, is related to the principle of redundancy reduction proposed for biological sensory processing by Horace Barlow in 1961, and applied quantitatively to retinal processing by Atick and Redlich. One of the applications of infomax has been to an independent component analysis algorithm that finds independent signals by maximising entropy. Infomax-based ICA was described by Bell and Sejnowski in 1995."
"http://dbpedia.org/resource/Madaline"	"Madaline"	"Artificial neural networks"	"MADALINE (Many ADALINE) is a three-layer (input, hidden, output), fully connected, feed-forward artificial neural network architecture for classification that uses ADALINE units in its hidden and output layers, i.e. its activation function is the sign function. The three-layer network uses memistors. Three different training algorithms for MADALINE networks, which cannot be learned using backpropagation because the sign function is not differentiable, have been suggested, called Rule I, Rule II and Rule III. The first of these dates back to 1962 and cannot adapt the weights of the hidden-output connection. The second training algorithm improved on Rule I and was described in 1988. The third ""Rule"" applied to a modified network with sigmoid activations instead of signum; it was later found to be equivalent to backpropagation. The Rule II training algorithm is based on a principle called ""minimal disturbance"". It proceeds by looping over training examples, then for each example, it: 
*  finds the hidden layer unit (ADALINE classifier) with the lowest confidence in its prediction, 
*  tentatively flips the sign of the unit, 
*  accepts or rejects the change based on whether the network's error is reduced, 
*  stops when the error is zero. Additionally, when flipping single units' signs does not drive the error to zero for a particular example, the training algorithm starts flipping pairs of units' signs, then triples of units, etc."
"http://dbpedia.org/resource/Synaptic_weight"	"Synaptic weight"	"Artificial neural networks"	"In neuroscience and computer science, synaptic weight refers to the strength or amplitude of a connection between two nodes, corresponding in biology to the amount of influence the firing of one neuron has on another. The term is typically used in artificial and biological neural network research."
"http://dbpedia.org/resource/The_Emotion_Machine"	"The Emotion Machine"	"Artificial neural networks"	"The Emotion Machine: Commonsense Thinking, Artificial Intelligence, and the Future of the Human Mind  is a 2006 book by cognitive scientist Marvin Minsky. The book is a sequel to Minsky's earlier book Society of Mind. Minsky argues that emotions are different ways to think that our mind uses to increase our intelligence. He challenges the distinction between emotions and other kinds of thinking. His main argument is that emotions are ""ways to think"" for different ""problem types"" that exist in the world, and that the brain has rule-based mechanisms (selectors) that turns on emotions to deal with various problems. The book reviews the accomplishments of AI, why modelling an AI is difficult in terms of replicating the behaviors of humans, if and how AIs think, and in what manner they might experience struggles and pleasures."
"http://dbpedia.org/resource/Competitive_learning"	"Competitive learning"	"Artificial neural networks"	"Competitive learning is a form of unsupervised learning in artificial neural networks, in which nodes compete for the right to respond to a subset of the input data. A variant of Hebbian learning, competitive learning works by increasing the specialization of each node in the network. It is well suited to finding clusters within data. Models and algorithms based on the principle of competitive learning include vector quantization and self-organizing maps (Kohonen maps)."
"http://dbpedia.org/resource/Bidirectional_recurrent_neural_networks"	"Bidirectional recurrent neural networks"	"Artificial neural networks"	"Bidirectional Recurrent Neural Networks(BRNN) were invented in 1997 by Schuster & Paliwal. BRNN is introduced to increase the amount of input information available to be referred to. For example, multilayer perceptron(MLP) and time delay neural network(TDNN) have limitation on the input data flexibility, as they require their input data to be fixed. Standard recurrent neural network(RNN) also has restrictions as its future input information cannot be reached from its current state. On the contrary, BRNN do not require their input data to be fixed. Moreover, their future input information is reachable from the current state. The basic idea of BRNN is to connect two hidden layers of opposite directions to the same output. By this structure, the output layer can get information from past and future states. BRNN are especially useful when the context of the input is needed. For example, in handwriting recognition, the performance can be enhanced by knowledge of the letters located before and after the current letter."
"http://dbpedia.org/resource/Dehaene–Changeux_model"	"Dehaene–Changeux model"	"Artificial neural networks"	"The Dehaene–Changeux model (DCM), also known as the global neuronal workspace or the global cognitive workspace model is a part of Bernard Baars's ""global workspace model"" for consciousness. It is a computer model of the neural correlates of consciousness programmed as a neural network. It attempts to reproduce the swarm behaviour  of the brain's higher cognitive functions such as consciousness, decision-making and the central executive functions. It was developed by cognitive neuroscientists Stanislas Dehaene and Jean-Pierre Changeux beginning in 1986. It has been used to provide a predictive framework to the study of inattentional blindness and the solving of the Tower of London test."
"http://dbpedia.org/resource/Grossberg_network"	"Grossberg network"	"Artificial neural networks"	"Grossberg network is a artificial neural network introduced by Stephen Grossberg. It is a self organizing, competitive network based on continuous time. Grossberg a neuroscientist and a biomedical engineer designed this network based on the human visual system."
"http://dbpedia.org/resource/Physical_neural_network"	"Physical neural network"	"Artificial neural networks"	"A physical neural network is a type of artificial neural network in which an electrically adjustable resistance material is used to emulate the function of a neural synapse. ""Physical"" neural network is used to emphasize the reliance on physical hardware used to emulate neurons as opposed to software-based approaches which simulate neural networks. More generally the term is applicable to other artificial neural networks in which a memristor or other electrically adjustable resistance material is used to emulate a neural synapse."
"http://dbpedia.org/resource/Types_of_artificial_neural_networks"	"Types of artificial neural networks"	"Artificial neural networks"	"There are many types of artificial neural networks (ANN). Artificial neural networks are computational models inspired by biological neural networks, and are used to approximate functions that are generally unknown. Particularly, they are inspired by the behaviour of neurons and the electrical signals they convey between input (such as from the eyes or nerve endings in the hand), processing, and output from the brain (such as reacting to light, touch, or heat). The way neurons semantically communicate is an area of ongoing research. Most artificial neural networks bear only some resemblance to their more complex biological counterparts, but are very effective at their intended tasks (e.g. classification or segmentation). Some ANNs are adaptive systems and are used for example to model populations and environments, which constantly change. Neural networks can be hardware- (neurons are represented by physical components) or software-based (computer models), and can use a variety of topologies and learning algorithms."
"http://dbpedia.org/resource/Multimodal_learning"	"Multimodal learning"	"Artificial neural networks"	"The information in real world usually comes as different modalities. For example, images are usually associated with tags and text explanations; texts contain images to more clearly express the main idea of the article. Different modalities are characterized by very different statistical properties. For Instance, image is usually represented as pixel intensities or outputs of feature extractors, while texts are represented as discrete word count vectors. Due to distinct statistical properties of different information resources, it is very important to discover the relationship between different modalities. Multimodal learning is a good model to represent the joint representations of different modalities. The multimodal learning model is also capable to fill missing modality given the observed ones. The multimodal learning model combines two Deep Boltzmann Machines each corresponds to one modality. An additional hidden layer is placed on top of the two Boltzmann Machines to give the joint representation."
"http://dbpedia.org/resource/Catastrophic_interference"	"Catastrophic interference"	"Artificial neural networks"	"Catastrophic interference, also known as catastrophic forgetting, is the tendency of an artificial neural network to completely and abruptly forget previously learned information upon learning new information. Neural networks are an important part of the network approach and connectionist approach to cognitive science. These networks use computer simulations to try and model human behaviours, such as memory and learning. Catastrophic interference is an important issue to consider when creating connectionist models of memory. It was originally brought to the attention of the scientific community by research from McCloskey and Cohen (1989), and Ractcliff (1990). It is a radical manifestation of the ‘sensitivity-stability’ dilemma  or the ‘stability-plasticity’ dilemma. Specifically, these problems refer to the issue of being able to make an artificial neural network that is sensitive to, but not disrupted by, new information. Lookup tables and connectionist networks lie on the opposite sides of the stability plasticity spectrum. The former remains completely stable in the presence of new information but lacks the ability to generalize, i.e. infer general principles, from new inputs. On the other hand, connectionist networks like the standard backpropagation network are very sensitive to new information and can generalize on new inputs. Backpropagation models can be considered good models of human memory insofar as they mirror the human ability to generalize but these networks often exhibit less stability than human memory. Notably, these backpropagation networks are susceptible to catastrophic interference. This is considered an issue when attempting to model human memory because, unlike these networks, humans typically do not show catastrophic forgetting. Thus, the issue of catastrophic interference must be eradicated from these backpropagation models in order to enhance the plausibility as models of human memory."
"http://dbpedia.org/resource/Counterpropagation_network"	"Counterpropagation network"	"Artificial neural networks"	"The counterpropagation network is a hybrid network. It consists of an outstar network and a competitive filter network. It was developed in 1986 by Robert Hecht-Nielsen. It is guaranteed to find the correct weights, unlike regular back propagation networks that can become trapped in local minimums during training. The input layer neurode connect to each neurode in the hidden layer. The hidden layer is a Kohonen network which categorizes the pattern that was input. The output layer is an outstar array which reproduces the correct output pattern for the category. Training is done in two stages. The hidden layer is first taught to categorize the patterns and the weights are then fixed for that layer. Then the output layer is trained. Each pattern that will be input needs a unique node in the hidden layer, which is often too large to work on real world problems."
"http://dbpedia.org/resource/Cover's_theorem"	"Cover's theorem"	"Artificial neural networks"	"Cover's Theorem is a statement in computational learning theory and is one of the primary theoretical motivations for the use of non-linear kernel methods in machine learning applications. The theorem states that given a set of training data that is not linearly separable, one can with high probability transform it into a training set that is linearly separable by projecting it into a higher-dimensional space via some non-linear transformation. The proof is easy. A deterministic mapping may be used. Indeed, suppose there are samples. Lift them onto the vertices of the simplex in the dimensional real space. Every partition of the samples into two sets is separable by a linear separator. QED. A complex pattern-classification problem, cast in a high-dimensional space nonlinearly, is more likely to be linearly separable than in a low-dimensional space, provided that the space is not densely populated.— Cover, T.M., Geometrical and Statistical properties of systems of linear inequalities with applications in pattern recognition., 1965"
"http://dbpedia.org/resource/Modular_neural_network"	"Modular neural network"	"Artificial neural networks"	"A modular neural network is an artificial neural network characterized by a series of independent neural networks moderated by some intermediary. Each independent neural network serves as a module and operates on separate inputs to accomplish some subtask of the task the network hopes to perform. The intermediary takes the outputs of each module and processes them to produce the output of the network as a whole. The intermediary only accepts the modules’ outputs—it does not respond to, nor otherwise signal, the modules. As well, the modules do not interact with each other."
"http://dbpedia.org/resource/Deep_lambertian_networks"	"Deep lambertian networks"	"Artificial neural networks"	"Deep Lambertian Networks (DLN)  is a combination of Deep belief network and Lambertian reflectance assumption which deals with the challenges posed by illumination variation in visual perception. Lambertian Reflectance model gives an illumination invariant representation which can be used for recognition. The Lambertian reflectance model is widely used formodeling illumination variations and is a good approximation for diffuse object surfaces. The DLN is a hybrid undirected-directed generative model that combines DBNs with the Lambertian reflectance model. In the DLN, the visible layer consists of image pixel intensities v ∈ RNv , where Nv is the number of pixels in the image. For every pixel i there are two latent variables namely the albedo and surface normal. GRBMs are used to model the albedo and surface normals. Combining Deep Belief Nets with the Lambertian reflectance assumption, the model can learn good priors over the albedo from 2D images. Illumination variations can be explained by changing only the lighting latent variable. By transferring learned knowledge from similar objects, albedo and surface normals estimation from a single image is also possible. Experiments demonstrate that this model is able to generalize as well as improve over standard baselines in one-shot face recognition. The model has been successfully applied in reconstruction of shadows facial images, given any set of lighting conditions. The model has also been tested on non-living objects. The method outperforms most other methods and is faster than them."
"http://dbpedia.org/resource/Hyper_basis_function_network"	"Hyper basis function network"	"Artificial neural networks"	"In machine learning, a Hyper basis function network, or HyperBF network, is a generalization of radial basis function (RBF) networks concept, where the Mahalanobis-like distance is used instead of Euclidean distance measure. Hyper basis function networks were first introduced by Poggio and Girosi in the 1990 paper “Networks for Approximation and Learning”."
"http://dbpedia.org/resource/Neural_network_synchronization_protocol"	"Neural network synchronization protocol"	"Artificial neural networks"	"The Neural network synchronization protocol, abbreviated as NNSP, is built on the application-level layer of the OSI upon TCP/IP. Aiming at secure communication, this protocol's design make use of a concept called neural network synchronization. Server and each connected client must create special type of neural network called Tree Parity Machine then compute outputs of their neural networks and exchange them in an iterative manner through the public channel. By learning and exchanging public outputs of their networks, client's and server's neural networks will be synchronized, meaning they will have identical synaptic weights, after some time. Once synchronization is achieved, weights of networks are used for secret key derivation. For the purposes of encryption/decryption of subsequent communication this symmetric key is used."
"http://dbpedia.org/resource/Convolutional_Deep_Belief_Networks"	"Convolutional Deep Belief Networks"	"Artificial neural networks"	"In computer science, Convolutional Deep Belief Network (CDBN) is a type of deep artificial neural network that is composed of multiple layers of convolutional restricted Boltzmann machines stacked together. Alternatively, it is a hierarchical generative model for deep learning, which is highly effective in the tasks of image processing and object recognition, though it has been used in other domains too. The salient features of the model include the fact that it scales well to high-dimensional images and is translation-invariant. CDBNs use the technique of probabilistic max-pooling to reduce the dimensions in higher layers in the network. Training of the network involves a pre-training stage accomplished in a greedy layer-wise manner, similar to other deep belief networks. Depending on whether the network is to be used for discrimination or generative tasks, it is then ""fine tuned"" or trained with either back-propagation or the up-down algorithm, respectively"
"http://dbpedia.org/resource/Synaptic_transistor"	"Synaptic transistor"	"Artificial neural networks"	"A synaptic transistor is an electrical device that can learn in ways similar to a neural synapse. It optimizes its own properties for the functions it has carried out in the past. The device mimics the behavior of the property of neurons called spike-timing-dependent plasticity, or STDP."
"http://dbpedia.org/resource/SILVIA"	"SILVIA"	"Artificial intelligence applications"	"Symbolically Isolated Linguistically Variable Intelligence Algorithms, or more popularly known as SILVIA, is a core platform technology developed by Cognitive Code. SILVIA was developed, and designed to recognize and interpret speech, text, and interact with applications and operating systems, all while interacting with a user. The technology can be run and operate via cloud, a mobile application, a part of network, or via server."
"http://dbpedia.org/resource/FatKat_(investment_software)"	"FatKat (investment software)"	"Artificial intelligence applications"	"FatKat, Inc. is a privately held company founded in 1999 by Raymond C. Kurzweil, an author, inventor, and futurist. He’s perhaps best known for creating an optical character recognition system that – in conjunction with a flatbed scanner and text-to-speech synthesizer – reads text aloud to the sight-impaired. FatKat is an acronym derived from “Financial Accelerating Transactions from Kurzweil Adaptive Technologies.” The aforesaid company is one of a total of nine Kurzweil companies. The purpose of FatKat as listed with the Massachusetts Secretary of the Commonwealth Corporations Division is “investment software.” Kurzweil, who specializes in artificial intelligence coupled with pattern recognition, has created software that uses quantitative methods to pick stocks for investment purposes. Although selecting stocks based on software-generated recommendations is not new, FatKat’s approach was unique at the time because of its “nonlinear decision making processes more akin to how a brain operates.” In layman's terms, the software can evolve by creating different rules, letting them compete, and using (or combining) the best outcomes. After FatKat’s inception, other investment and/or software companies rushed to develop software based on this and similar Darwinist evolutionary principles, using genetic algorithms. In 2005, Kurzweil reported that the FatKat software was “doing very well – 50% to 100% returns for the last two years.”  But as of December 2008, FatKat does not offer its software for sale."
"http://dbpedia.org/resource/SmartAction"	"SmartAction"	"Artificial intelligence applications"	"SmartAction provides artificial intelligence-based voice self-service. SmartAction's Intelligent Voice Automation (IVA) is a hosted IVR platform that uses natural language speech recognition and is based on an object-oriented coding framework. IVA is a cloud-based, hosted service. IVA® handles complex customer experience calls across 12 verticals, providing assistance with numerous call types such as appointment scheduling and rescheduling, delivery status, order placements, and a variety of outbound calls. SmartAction was founded by inventor and entrepreneur Peter Voss and is headquartered in El Segundo, CA."
"http://dbpedia.org/resource/Speech-generating_device"	"Speech-generating device"	"Artificial intelligence applications"	"Speech-generating devices (SGDs), also known as voice output communication aids, are electronic augmentative and alternative communication (AAC) systems used to supplement or replace speech or writing for individuals with severe speech impairments, enabling them to verbally communicate. SGDs are important for people who have limited means of interacting verbally, as they allow individuals to become active participants in communication interactions. They are particularly helpful for patients suffering from Amyotrophic Lateral Sclerosis (ALS) but recently have been used for children with predicted speech deficiencies. There are several input and display methods for users of varying abilities to make use of SGDs. Some SGDs have multiple pages of symbols to accommodate a large number of utterances, and thus only a portion of the symbols available are visible at any one time, with the communicator navigating the various pages. Speech-generating devices can produce electronic voice output by using digitized recordings of natural speech or through speech synthesis—which may carry less emotional information but can permit the user to speak novel messages. The content, organization, and updating of the vocabulary on an SGD is influenced by a number of factors, such at the user's needs and the contexts that the device will be used in. The development of techniques to improve the available vocabulary and rate of speech production is an active research area. Vocabulary items should be of high interest to the user, be frequently applicable, have a range of meanings, and be pragmatic in functionality. There are multiple methods of accessing messages on devices: directly or indirectly, or using specialized access devices—although the specific access method will depend on the skills and abilities of the user. SGD output is typically much slower than speech, although rate enhancement strategies can increase the user's rate of output, resulting in enhanced efficiency of communication. The first known SGD was prototyped in the mid-1970s, and rapid progress in hardware and software development has meant that SGD capabilities can now be integrated into devices like smartphones. Notable users of SGDs include Stephen Hawking, Roger Ebert, Tony Proudfoot, and Pete Frates (founder of the ALS ice bucket challenge). Speech-generating systems may be dedicated devices developed solely for AAC, or non-dedicated devices such as computers running additional software to allow them to function as AAC devices."
"http://dbpedia.org/resource/Silent_speech_interface"	"Silent speech interface"	"Artificial intelligence applications"	"Silent speech interface is a device that allows speech communication without using the sound made when people vocalize their speech sounds. As such it is a type of electronic lip reading. It works by the computer identifying the phonemes that an individual pronounces from nonauditory sources of information about their speech movements. These are then used to recreate the speech using speech synthesis."
"http://dbpedia.org/resource/TuVox"	"TuVox"	"Artificial intelligence applications"	"TuVox is a company that produces VXML-based telephone speech-recognition applications to replace DTMF touch-tone systems for their clients."
"http://dbpedia.org/resource/Content-based_image_retrieval"	"Content-based image retrieval"	"Artificial intelligence applications"	"Content-based image retrieval (CBIR), also known as query by image content (QBIC) and content-based visual information retrieval (CBVIR) is the application of computer vision techniques to the image retrieval problem, that is, the problem of searching for digital images in large databases (see this survey for a recent scientific overview of the CBIR field). Content-based image retrieval is opposed to traditional concept-based approaches (see Concept-based image indexing). ""Content-based"" means that the search analyzes the contents of the image rather than the metadata such as keywords, tags, or descriptions associated with the image. The term ""content"" in this context might refer to colors, shapes, textures, or any other information that can be derived from the image itself. CBIR is desirable because searches that rely purely on metadata are dependent on annotation quality and completeness. Having humans manually annotate images by entering keywords or metadata in a large database can be time consuming and may not capture the keywords desired to describe the image. The evaluation of the effectiveness of keyword image search is subjective and has not been well-defined. In the same regard, CBIR systems have similar challenges in defining success."
"http://dbpedia.org/resource/Monitoring_and_Surveillance_Agents"	"Monitoring and Surveillance Agents"	"Artificial intelligence applications"	"Monitoring and surveillance agents (also known as predictive agents) are a type of intelligent agent software that observes and reports on computer equipment. Monitoring and surveillance agents are often used to monitor complex computer networks to predict when a crash or some other defect may occur. Another type of monitoring and surveillance agent works on computer networks keeping track of the configuration of each computer connected to the network. It tracks and updates the central configuration database when anything on any computer changes, such as the number or type of disk drives. An important task in managing networks lies in prioritizing traffic and shaping bandwidth."
"http://dbpedia.org/resource/Network_Compartment"	"Network Compartment"	"Artificial intelligence applications"	"Network Compartmentalization, the division of network functionality into network compartments, is an important concept of Autonomic Networking."
"http://dbpedia.org/resource/Sinewave_synthesis"	"Sinewave synthesis"	"Artificial intelligence applications"	"Sinewave synthesis, or sine wave speech, is a technique for synthesizing speech by replacing the formants (main bands of energy) with pure tone whistles. The first sinewave synthesis program (SWS) for the automatic creation of stimuli for perceptual experiments was developed by Philip Rubin at Haskins Laboratories in the 1970s. This program was subsequently used by Robert Remez, Philip Rubin, David Pisoni, and other colleagues to show that listeners can perceive continuous speech without traditional speech cues. This work paved the way for a view of speech as a dynamic pattern of trajectories through articulatory-acoustic space."
"http://dbpedia.org/resource/Vehicle_infrastructure_integration"	"Vehicle infrastructure integration"	"Artificial intelligence applications"	"Vehicle Infrastructure Integration (VII) is an initiative fostering research and applications development for a series of technologies directly linking road vehicles to their physical surroundings, first and foremost in order to improve road safety. The technology draws on several disciplines, including transport engineering, electrical engineering, automotive engineering, and computer science. VII specifically covers road transport although similar technologies are in place or under development for other modes of transport. Planes, for example, use ground-based beacons for automated guidance, allowing the autopilot to fly the plane without human intervention. In highway engineering, improving the safety of a roadway can enhance overall efficiency. VII targets improvements in both safety and efficiency. Vehicle infrastructure integration is that branch of engineering, which deals with the study and application of a series of techniques directly linking road vehicles to their physical surroundings in order to improve road safety."
"http://dbpedia.org/resource/Language_identification"	"Language identification"	"Artificial intelligence applications"	"In natural language processing, language identification or language guessing is the problem of determining which natural language given content is in. Computational approaches to this problem view it as a special case of text categorization, solved with various statistical methods."
"http://dbpedia.org/resource/Resistance_Database_Initiative"	"Resistance Database Initiative"	"Artificial intelligence applications"	"HIV Resistance Response Database Initiative (RDI) is a not-for-profit organisation established in 2002 with the mission of improving the clinical management of HIV infection through the application of bioinformatics to HIV drug resistance and treatment outcome data. The RDI has the following specific goals: 1.  
*  To be an independent repository of HIV resistance and treatment outcome data 2.  
*  To use bioinformatics to explore the relationships between resistance, other clinical and laboratory factors and HIV treatment outcome 3.  
*  To develop and make freely available a system to predict treatment response, as an aid to optimising and individualising the clinical management of HIV infection The RDI consists of a small executive group based in the UK, an international advisory group of leading HIV/AIDS scientists and clinicians, and an extensive global network of collaborators and data contributors."
"http://dbpedia.org/resource/Roblog"	"Roblog"	"Artificial intelligence applications"	"Roblog is a neologism for a blog written by a robot with no human intervention. Roblogs were made possible with a new generation of robots which are capable of uploading images and texts automatically to the Web. The first roblogs to appear, late 2005, were written by AIBO robots, the dog-like robotic pets once manufactured by Sony."
"http://dbpedia.org/resource/Eccky"	"Eccky"	"Artificial intelligence applications"	"Eccky is an online game. Until 2009, it was an MSN-based life simulation game in which two people work together to create and raise a virtual baby. Eccky won the 2005 SpinAwards for Innovation and for Best Interactive Concept. In 2009, the game play changed to a real-time virtual world on Hyves."
"http://dbpedia.org/resource/Machine_translation"	"Machine translation"	"Artificial intelligence applications"	"Machine translation, sometimes referred to by the abbreviation MT (not to be confused with computer-aided translation, machine-aided human translation (MAHT) or interactive translation) is a sub-field of computational linguistics that investigates the use of software to translate text or speech from one language to another. On a basic level, MT performs simple substitution of words in one language for words in another, but that alone usually cannot produce a good translation of a text because recognition of whole phrases and their closest counterparts in the target language is needed. Solving this problem with corpus and statistical techniques is a rapidly growing field that is leading to better translations, handling differences in linguistic typology, translation of idioms, and the isolation of anomalies. Current machine translation software often allows for customization by domain or profession (such as weather reports), improving output by limiting the scope of allowable substitutions. This technique is particularly effective in domains where formal or formulaic language is used. It follows that machine translation of government and legal documents more readily produces usable output than conversation or less standardised text. Improved output quality can also be achieved by human intervention: for example, some systems are able to translate more accurately if the user has unambiguously identified which words in the text are proper names. With the assistance of these techniques, MT has proven useful as a tool to assist human translators and, in a very limited number of cases, can even produce output that can be used as is (e.g., weather reports). The progress and potential of machine translation have been debated much through its history. Since the 1950s, a number of scholars have questioned the possibility of achieving fully automatic machine translation of high quality. Some critics claim that there are in-principle obstacles to automating the translation process."
"http://dbpedia.org/resource/AForge.NET"	"AForge.NET"	"Artificial intelligence applications"	"AForge.NET is a computer vision and artificial intelligence library originally developed by Andrew Kirillov for the .NET Framework. The source code and binaries of the project are available under the terms of the Lesser GPL and the GPL (GNU General Public License). Another (unaffiliated) project called Accord.NET was created to extend the features of the original AForge.NET library."
"http://dbpedia.org/resource/Cleverbot"	"Cleverbot"	"Artificial intelligence applications"	"Cleverbot is a web application that uses an artificial intelligence algorithm to have conversations with humans. It was created by the British AI scientist Rollo Carpenter, who also created Jabberwacky, a similar web application. In its first decade, Cleverbot held several thousand conversations with Carpenter and his associates. Since launching on the web in 1997, the number of conversations held has exceeded 200 million. Besides the web application, Cleverbot is also available as an iOS, Android, and Windows Phone app."
"http://dbpedia.org/resource/EuResist"	"EuResist"	"Artificial intelligence applications"	"EuResist is an international project designed to improve the treatment of HIV patients by developing a computerized system that can recommend optimal treatment based on the patient’s clinical and genomic data. The project is part of the Virtual Physiological Human framework, funded by the European Commission. It started in 2006 with the formation of a consortium of several research institutes and hospitals in Europe and Israel. The consortium completed its commitment to the European Commission near the end of 2008, at which time the system became available online. A non-profit organization was consequently established by the main partners to maintain and improve the system. In 2009, the EuResist project was named as a Computerworld honors program laureate."
"http://dbpedia.org/resource/Artificial_Solutions"	"Artificial Solutions"	"Artificial intelligence applications"	"Artificial Solutions is a multinational software company that develops and sells natural language interaction products for enterprise and consumer use. The company's natural language solutions have been deployed in a wide range of industries including finance, telecoms, the public sector, retail and travel."
"http://dbpedia.org/resource/Interactions_Corporation"	"Interactions Corporation"	"Artificial intelligence applications"	"Interactions LLC is a privately held technology company that builds and delivers hosted Virtual Assistant applications that enable businesses to deliver automated natural language communications for customer care."
"http://dbpedia.org/resource/Mobileye"	"Mobileye"	"Artificial intelligence applications"	"Mobileye (NYSE: MBLY) is an Israeli technology company that develops vision-based advanced driver assistance systems (ADAS) providing warnings for collision prevention and mitigation. Mobileye N.V. headquarters and main R&D centre is located in Jerusalem, Israel operating under the company name Mobileye Vision Technology Ltd. The company has also sales and marketing offices in Jericho, New York; Shanghai, China; Tokyo, Japan and Düsseldorf, Germany."
"http://dbpedia.org/resource/Akinator"	"Akinator"	"Artificial intelligence applications"	"Akinator, the Web Genie is an internet game and mobile app based on Twenty Questions that attempts to determine which character the player is thinking of by asking them a series of questions. It is an artificial intelligence program that can find and learn the best questions to ask the player. Created by three French programmers in 2007, it became popular worldwide in November 2008, according to Google Trends. In Europe popularity peak was reached in 2009 and Japan in 2010 with the launch of mobile apps by French mobile company SCIMOB, reaching highest ranks on app store. While playing ""Akinator"", questions are asked by a cartoon genie."
"http://dbpedia.org/resource/Verbot"	"Verbot"	"Artificial intelligence applications"	"The Verbot (Verbal-Robot) was a popular chatterbot program and Artificial Intelligence Software Development Kit (SDK) for the Windows platform and for the web."
"http://dbpedia.org/resource/Kasparov's_Gambit"	"Kasparov's Gambit"	"Artificial intelligence applications"	"Kasparov's Gambit or simply Gambit is a chess playing computer program created by Heuristic Software and published by Electronic Arts in 1993 based on Socrates II, the only winner of the North American Computer Chess Championship running on a common microcomputer. It was designed for DOS while Garry Kasparov reigned as world champion, whose involvement and support was its key allure."
"http://dbpedia.org/resource/Quack.com"	"Quack.com"	"Artificial intelligence applications"	"Quack.com was an early voice portal company. The domain name later was used for Quack; an iPad Search application from AOL."
"http://dbpedia.org/resource/DialogOS"	"DialogOS"	"Artificial intelligence applications"	"DialogOS is a graphical programming environment to design computer system which can converse through voice with the user. Dialogs are clicked together in a Flowchart. DialogOS includes bindings to control Lego Mindstorms robots with the voice. DialogOS is used in computer science courses in schools and universities to teach programming and to introduce beginners in the basic principles of human/computer interaction and dialog design."
"http://dbpedia.org/resource/GNOME_Chess"	"GNOME Chess"	"Artificial intelligence applications"	"GNOME Chess (formerly glChess) is a graphical front-end featuring a 2D and a 3D chessboard interface. GNOME Chess does not comprise an own chess engine and to play against the computer a third party chess engine must be present, but most Linux distributions package GNU Chess as the default chess engine with it. Additionally GNOME Chess supports third party chess engines, known ones are automatically detected. GNOME Chess is written in Vala. For 2D rendering it uses GTK+ and Cairo/librsvg, and 3D support is optionally available using OpenGL. As part of the GNOME desktop environment and GNOME Games, GNOME Chess is free and open-source software subject to the terms of the GNU General Public License (GPL) version 2."
"http://dbpedia.org/resource/Optical_answer_sheet"	"Optical answer sheet"	"Artificial intelligence applications"	"An optical answer sheet or ""bubble sheet"" is a special type of form used in multiple choice question examinations. Optical mark recognition is used to detect answers. The most well known company in the United States involved with optical answer sheets is the Scantron Corporation, although certain uses require their own customized system. The terms ""Optical answer sheet"" and ""scantron"" have become more or less interchangeable. Optical answer sheets usually have a set of blank ovals or boxes that correspond to each question, often on separate sheets of paper. Bar codes may mark the sheet for automatic processing, and each series of ovals filled will return a certain value when read. In this way students' answers can be digitally recorded, or identity given."
"http://dbpedia.org/resource/Polyworld"	"Polyworld"	"Artificial intelligence applications"	"Polyworld is a cross-platform (Linux, Mac OS X) program written by Larry Yaeger to evolve Artificial Intelligence through natural selection and evolutionary algorithms. It uses the Qt graphics toolkit and OpenGL to display a graphical environment in which a population of trapezoid agents search for food, mate, have offspring, and prey on each other. The population is typically only in the hundreds, as each individual is rather complex and the environment consumes considerable computer resources. The graphical environment is necessary since the individuals actually move around the 2-D plane and must be able to ""see."" Since some basic abilities, like eating carcasses or randomly generated food, seeing other individuals, mating or fighting with them, etc., are possible, a number of interesting behaviours have been observed to spontaneously arise after prolonged evolution, such as cannibalism, predators and prey, and mimicry. Each individual makes decisions based on a neural net using Hebbian learning; the neural net is derived from each individual's genome. The genome does not merely specify the wiring of the neural nets, but also determines their size, speed, color, mutation rate and a number of other factors. The genome is randomly mutated at a set probability, which are also changed in descendant organisms."
"http://dbpedia.org/resource/Machine_translation_software_usability"	"Machine translation software usability"	"Artificial intelligence applications"	"The sections below give objective criteria for evaluating the usability of machine translation software output."
"http://dbpedia.org/resource/Optical_character_recognition"	"Optical character recognition"	"Artificial intelligence applications"	"Optical character recognition (optical character reader, OCR) is the mechanical or electronic conversion of images of typed, handwritten or printed text into machine-encoded text, whether from a scanned document, a photo of a document, a scene-photo (for example the text on signs and billboards in a landscape photo) or from subtitle text superimposed on an image (for example from a television broadcast). It is widely used as a form of information entry from printed paper data records, whether passport documents, invoices, bank statements, computerised receipts, business cards, mail, printouts of static-data, or any suitable documentation. It is a common method of digitising printed texts so that they can be electronically edited, searched, stored more compactly, displayed on-line, and used in machine processes such as cognitive computing, machine translation, (extracted) text-to-speech, key data and text mining. OCR is a field of research in pattern recognition, artificial intelligence and computer vision. Early versions needed to be trained with images of each character, and worked on one font at a time. Advanced systems capable of producing a high degree of recognition accuracy for most fonts are now common, and with support for a variety of digital image file format inputs. Some systems are capable of reproducing formatted output that closely approximates the original page including images, columns, and other non-textual components."
"http://dbpedia.org/resource/Orange_(software)"	"Orange (software)"	"Artificial intelligence applications"	"Orange is a free software machine learning and data mining package (written in Python). It has a visual programming front-end for explorative data analysis and visualization, and can also be used as a Python library. The program is maintained and developed by the Bioinformatics Laboratory of the Faculty of Computer and Information Science at University of Ljubljana."
"http://dbpedia.org/resource/Question_answering"	"Question answering"	"Artificial intelligence applications"	"Question Answer (Q AND A) is a computer science discipline within the fields of information retrieval and natural language processing (NLP), which is concerned with building systems that automatically answer questions posed by humans in a natural language. A QA implementation, usually a computer program, may construct its answers by querying a structured database of knowledge or information, usually a knowledge base. More commonly, QA systems can pull answers from an unstructured collection of natural language documents. Some examples of natural language document collections used for QA systems include: 
*  a local collection of reference texts 
*  internal organization documents and web pages 
*  compiled newswire reports 
*  a set of Wikipedia pages 
*  a subset of World Wide Web pages QA research attempts to deal with a wide range of question types including: fact, list, definition, How, Why, hypothetical, semantically constrained, and cross-lingual questions. 
*  Closed-domain question answering deals with questions under a specific domain (for example, medicine or automotive maintenance), and can be seen as an easier task because NLP systems can exploit domain-specific knowledge frequently formalized in ontologies. Alternatively, closed-domain might refer to a situation where only a limited type of questions are accepted, such as questions asking for descriptive rather than procedural information. QA systems in the context of machine reading applications have also been constructed in the medical domain, for instance related to Alzheimers disease  
*  Open-domain question answering deals with questions about nearly anything, and can only rely on general ontologies and world knowledge. On the other hand, these systems usually have much more data available from which to extract the answer."
"http://dbpedia.org/resource/Speech_synthesis"	"Speech synthesis"	"Artificial intelligence applications"	"Speech synthesis is the artificial production of human speech. A computer system used for this purpose is called a speech computer or speech synthesizer, and can be implemented in software or hardware products. A text-to-speech (TTS) system converts normal language text into speech; other systems render symbolic linguistic representations like phonetic transcriptions into speech. Synthesized speech can be created by concatenating pieces of recorded speech that are stored in a database. Systems differ in the size of the stored speech units; a system that stores phones or diphones provides the largest output range, but may lack clarity. For specific usage domains, the storage of entire words or sentences allows for high-quality output. Alternatively, a synthesizer can incorporate a model of the vocal tract and other human voice characteristics to create a completely ""synthetic"" voice output. The quality of a speech synthesizer is judged by its similarity to the human voice and by its ability to be understood clearly. An intelligible text-to-speech program allows people with visual impairments or reading disabilities to listen to written works on a home computer. Many computer operating systems have included speech synthesizers since the early 1990s. A text-to-speech system (or ""engine"") is composed of two parts: a front-end and a back-end. The front-end has two major tasks. First, it converts raw text containing symbols like numbers and abbreviations into the equivalent of written-out words. This process is often called text normalization, pre-processing, or tokenization. The front-end then assigns phonetic transcriptions to each word, and divides and marks the text into prosodic units, like phrases, clauses, and sentences. The process of assigning phonetic transcriptions to words is called text-to-phoneme or grapheme-to-phoneme conversion. Phonetic transcriptions and prosody information together make up the symbolic linguistic representation that is output by the front-end. The back-end—often referred to as the synthesizer—then converts the symbolic linguistic representation into sound. In certain systems, this part includes the computation of the target prosody (pitch contour, phoneme durations), which is then imposed on the output speech."
"http://dbpedia.org/resource/Statistical_semantics"	"Statistical semantics"	"Artificial intelligence applications"	"Statistical semantics is the study of ""how the statistical patterns of human word usage can be used to figure out what people mean, at least to a level sufficient for information access"". How can we figure out what words mean, simply by looking at patterns of words in huge collections of text? What are the limits to this approach to understanding words?"
"http://dbpedia.org/resource/SCIgen"	"SCIgen"	"Artificial intelligence applications"	"SCIgen is a computer program that uses context-free grammar to randomly generate nonsense in the form of computer science research papers. All elements of the papers are formed, including graphs, diagrams, and citations. Created by scientists at the Massachusetts Institute of Technology, its stated aim is ""to maximize amusement, rather than coherence."""
"http://dbpedia.org/resource/Text_mining"	"Text mining"	"Artificial intelligence applications"	"Text mining, also referred to as text data mining, roughly equivalent to , refers to the process of deriving high-quality information from text. High-quality information is typically derived through the devising of patterns and trends through means such as statistical pattern learning. Text mining usually involves the process of structuring the input text (usually parsing, along with the addition of some derived linguistic features and the removal of others, and subsequent insertion into a database), deriving patterns within the structured data, and finally evaluation and interpretation of the output. 'High quality' in text mining usually refers to some combination of relevance, novelty, and interestingness. Typical text mining tasks include text categorization, text clustering, concept/entity extraction, production of granular taxonomies, sentiment analysis, document summarization, and entity relation modeling (i.e., learning relations between named entities). Text analysis involves information retrieval, lexical analysis to study word frequency distributions, pattern recognition, tagging/annotation, information extraction, data mining techniques including link and association analysis, visualization, and predictive analytics. The overarching goal is, essentially, to turn text into data for analysis, via application of natural language processing (NLP) and analytical methods. A typical application is to scan a set of documents written in a natural language and either model the document set for predictive classification purposes or populate a database or search index with the information extracted."
"http://dbpedia.org/resource/Accord.NET"	"Accord.NET"	"Artificial intelligence applications"	"Accord.NET is a framework for scientific computing in .NET. The source code of the project is available under the terms of the Gnu Lesser Public License, version 2.1. The framework comprises a set of libraries that are available in source code as well as via executable installers and NuGet packages. The main areas covered include numerical linear algebra, numerical optimization, statistics, machine learning, artificial neural networks, signal and image processing, and support libraries (such as graph plotting and visualization). The project was originally created to extend the capabilities of the  AForge.NET Framework, but has since incorporated AForge.NET inside itself. Newer releases have united both frameworks under the Accord.NET name."
"http://dbpedia.org/resource/Artisto"	"Artisto"	"Artificial intelligence applications"	"Artisto is a video processing application with art and movie effects filters based on neural network algorithms created in 2016 by Mail.ru Group machine learning specialists. At the moment the application can process videos up to 10 seconds long and offers users 21 filters, including those based on the works of famous artists (e.g. Blue Dream — Pablo Picasso), theme-based (Rio-2016 — related to the 2016 Summer Olympics in Rio de Janeiro) and others. The app works with both pre-recorded videos and videos recorded with the application."
"http://dbpedia.org/resource/Prisma_(app)"	"Prisma (app)"	"Artificial intelligence applications"	"Prisma is a photo-editing application that utilizes a neural network and artificial intelligence to transform the image into an artistic effect. The app was created by Alexey Moiseenkov (Russian: Алексей Моисеенков), and it was launched in June 2016 as a free mobile app. A week after its launch, the app gained popularity and received over 7.5 million downloads and over 1 million active users as of July 2016. It debuted on iOS on Apple App Store during the first week of June and it became the leading app at the App Store in Russia and other neighboring countries. On 19 July 2016, the developer launched a beta version of the app for Android and it closed few hours later by developers after receiving feedback from its users. It was later released publicly on 24 July 2016 on Google Play. In July 2016, the developer announced that the video and virtual reality version of the app is currently under development."
"http://dbpedia.org/resource/ESTAR_project"	"ESTAR project"	"Artificial intelligence applications"	"The eSTAR project is a multi-agent system that aims to implement a true heterogeneous network of robotic telescopes for automated observing. The project is a joint collaboration between the Astrophysics Group of the University of Exeter and the Astrophysics Research Institute at Liverpool John Moores University. In 2006 work began on an autonomous software agent for observations of variable stars. This agent implements the optimal sampling technique of Saunders et al. (2006) and the prototype was successfully tested on the RoboNet network of telescopes which includes: the Liverpool Telescope, the Faulkes Telescope North and the Faulkes Telescope South. eSTAR is affiliated with the RoboNet Consortium and the global Heterogeneous Telescope Networks Consortium. As of 2007 eSTAR is ""live"" supporting two real-time observing projects. Automated follow-up observations of gamma ray bursts are performed using the 3.8m UKIRT telescope situated in Hawai'i, making this telescope the largest in the world, with an automated response system for tracking such events. eSTAR is also involved in the search for extra-solar planets by placing observations on the RoboNet system of telescopes on behalf of the PLANET collaboration. The technique of gravitational microlensing is used to monitor large numbers of stars in the galactic bulge looking for the tell-tale signature of cool planets orbiting those stars."
"http://dbpedia.org/resource/Natural_language_user_interface"	"Natural language user interface"	"Artificial intelligence applications"	"Natural language user interfaces (LUI or NLUI) are a type of computer human interface where linguistic phenomena such as verbs, phrases and clauses act as UI controls for creating, selecting and modifying data in software applications. In interface design natural language interfaces are sought after for their speed and ease of use, but most suffer the challenges to understanding wide varieties of ambiguous input.Natural language interfaces are an active area of study in the field of natural language processing and computational linguistics. An intuitive general natural language interface is one of the active goals of the Semantic Web. Text interfaces are ""natural"" to varying degrees. Many formal (un-natural) programming languages incorporate idioms of natural human language. Likewise, a traditional keyword search engine could be described as a ""shallow"" natural language user interface."
"http://dbpedia.org/resource/Intelligent_character_recognition"	"Intelligent character recognition"	"Artificial intelligence applications"	"In computer science, intelligent character recognition (ICR) is an advanced optical character recognition (OCR) or — rather more specific — handwriting recognition system that allows fonts and different styles of handwriting to be learned by a computer during processing to improve accuracy and recognition levels. Most ICR software has a self-learning system referred to as a neural network, which automatically updates the recognition database for new handwriting patterns. It extends the usefulness of scanning devices for the purpose of document processing, from printed character recognition (a function of OCR) to hand-written matter recognition. Because this process is involved in recognising hand writing, accuracy levels may, in some circumstances, not be very good but can achieve 97%+ accuracy rates in reading handwriting in structured forms. Often to achieve these high recognition rates several read engines are used within the software and each is given elective voting rights to determine the true reading of characters. In numeric fields, engines which are designed to read numbers take preference, while in alpha fields, engines designed to read hand written letters have higher elective rights. When used in conjunction with a bespoke interface hub, hand-written data can be automatically populated into a back office system avoiding laborious manual keying and can be more accurate than traditional human data entry. An important development of ICR was the invention of Automated Forms Processing in 1993. This involved a three-stage process of capturing the image of the form to be processed by ICR and preparing it to enable the ICR engine to give best results, then capturing the information using the ICR engine and finally processing the results to automatically validate the output from the ICR engine. This application of ICR increased the usefulness of the technology and made it applicable for use with real world forms in normal business applications. Modern software applications use ICR as a technology of recognizing text in forms filled in by hand (hand-printed)."
"http://dbpedia.org/resource/Document_processing"	"Document processing"	"Artificial intelligence applications"	"Document processing involves the conversion of typed and handwritten text on paper-based & electronic documents (e.g., scanned image of a document) into electronic information utilising one of, or a combination of, intelligent character recognition (ICR), optical character recognition (OCR) and experienced data entry clerks."
"http://dbpedia.org/resource/Dr._Sbaitso"	"Dr. Sbaitso"	"Artificial intelligence applications"	"Dr. Sbaitso is an artificial intelligence speech synthesis program released in 1992 by Creative Labs for  MS DOS-based personal computers."
"http://dbpedia.org/resource/Applications_of_artificial_intelligence"	"Applications of artificial intelligence"	"Artificial intelligence applications"	"AI researchers have created many tools to solve the most difficult problems in computer science. Many of their inventions have been adopted by mainstream computer science and are no longer considered a part of AI. (See AI effect). According to , p. 15), all of the following were originally developed in AI laboratories: time sharing,interactive interpreters,graphical user interfaces and the computer mouse,rapid development environments,the linked list data structure,automatic storage management,symbolic programming,functional programming,dynamic programming andobject-oriented programming."
"http://dbpedia.org/resource/Concept_mining"	"Concept mining"	"Artificial intelligence applications"	"Concept mining is an activity that results in the extraction of concepts from artifacts. Solutions to the task typically involve aspects of artificial intelligence and statistics, such as data mining and text mining. Because artifacts are typically a loosely structured sequence of words and other symbols (rather than concepts), the problem is nontrivial, but it can provide powerful insights into the meaning, provenance and similarity of documents."
"http://dbpedia.org/resource/Xaitment"	"Xaitment"	"Artificial intelligence applications"	"xaitment is a German-based company that develops and sells artificial intelligence (AI) software to video game developers and simulation developers. The company was founded in 2004 by Dr. Andreas Gerber, and is a spin-off of the German Research Centre for Artificial Intelligence, or DFKI. xaitment has its main office in Quierschied, Germany, and field offices in San Francisco and China."
"http://dbpedia.org/resource/Automatic_image_annotation"	"Automatic image annotation"	"Artificial intelligence applications"	"Automatic image annotation (also known as automatic image tagging or linguistic indexing) is the process by which a computer system automatically assigns metadata in the form of captioning or keywords to a digital image. This application of computer vision techniques is used in image retrieval systems to organize and locate images of interest from a database. This method can be regarded as a type of multi-class image classification with a very large number of classes - as large as the vocabulary size. Typically, image analysis in the form of extracted feature vectors and the training annotation words are used by machine learning techniques to attempt to automatically apply annotations to new images. The first methods learned the correlations between image features and training annotations, then techniques were developed using machine translation to try to translate the textual vocabulary with the 'visual vocabulary', or clustered regions known as blobs. Work following these efforts have included classification approaches, relevance models and so on. The advantages of automatic image annotation versus content-based image retrieval (CBIR) are that queries can be more naturally specified by the user . CBIR generally (at present) requires users to search by image concepts such as color and texture, or finding example queries. Certain image features in example images may override the concept that the user is really focusing on. The traditional methods of image retrieval such as those used by libraries have relied on manually annotated images, which is expensive and time-consuming, especially given the large and constantly growing image databases in existence. Some annotation engines are online, including the ALIPR.com real-time tagging engine developed by Pennsylvania State University researchers, and Behold."
"http://dbpedia.org/resource/Artificial_imagination"	"Artificial imagination"	"Artificial intelligence applications"	"Artificial imagination (AIm), also called Synthetic imagination or machine imagination is defined as artificial simulation of human imagination by general or special purpose computers or artificial neural networks. The term artificial imagination is also used to describe a property of machines or programs: Among some of the traits that researchers hope to simulate using machines include creativity, vision, digital art, humor, satire, etc. Artificial imagination research uses tools and insights from many fields, including computer science, Rhetoric, psychology, creative arts, philosophy, neuroscience, affective computing, Artificial Intelligence, Artificial intuition, cognitive science, linguistics, operations research, creative writing, probability and logic. The various practitioners in the field are researching various aspects of Artificial imagination, such as Artificial (visual) imagination,Artificial (aural) Imagination, modeling/filtering content based on human emotions and Interactive Search. Some articles on the topic speculate on how artificial imagination may evolve to create an artificial world which people may not want to leave at all. . Some researchers in the field, such as G. Schleis and M. Rizki, Dept. of Comput. Sci., Wayne State Univ. have focused on using artificial neural networks for simulating artificial imagination. The topic of artificial imagination has gotten interest from scholars outside the computer science domain, such as noted communications scholar Ernest Bormann, who came up with the Symbolic Convergence Theory and has worked on a project to develop artificial imagination in computer systems. How to Build a Mind: Toward Machines with Imagination by Igor Aleksander is a good academic book on the topic. Artificial Imagination, a roman à clef, is a good non-academic book supposedly written by an Artificial imagination system."
"http://dbpedia.org/resource/Handwriting_recognition"	"Handwriting recognition"	"Artificial intelligence applications"	"Handwriting recognition (or HWR) is the ability of a computer to receive and interpret intelligible handwritten input from sources such as paper documents, photographs, touch-screens and other devices. The image of the written text may be sensed ""off line"" from a piece of paper by optical scanning (optical character recognition) or intelligent word recognition. Alternatively, the movements of the pen tip may be sensed ""on line"", for example by a pen-based computer screen surface, a generally easier task as there are more clues available. Handwriting recognition principally entails optical character recognition. However, a complete handwriting recognition system also handles formatting, performs correct segmentation into characters and finds the most plausible words."
"http://dbpedia.org/resource/Automatic_number_plate_recognition"	"Automatic number plate recognition"	"Artificial intelligence applications"	"Automatic number plate recognition (ANPR; see also  below) is a technology that uses optical character recognition on images to read vehicle registration plates. It can use existing closed-circuit television, road-rule enforcement cameras, or cameras specifically designed for the task. ANPR is used by police forces around the world for law enforcement purposes, including to check if a vehicle is registered or licensed. It is also used for electronic toll collection on pay-per-use roads and as a method of cataloging the movements of traffic for example by highways agencies. Automatic number plate recognition can be used to store the images captured by the cameras as well as the text from the license plate, with some configurable to store a photograph of the driver. Systems commonly use infrared lighting to allow the camera to take the picture at any time of the day. ANPR technology must take into account plate variations from place to place. Concerns about these systems have centered on privacy fears of government tracking citizens' movements, misidentification, high error rates, and increased government spending. Critics have described it as a form of mass surveillance."
"http://dbpedia.org/resource/Clinical_decision_support_system"	"Clinical decision support system"	"Artificial intelligence applications"	"A clinical decision support system (CDSS) is a health information technology system that is designed to provide physicians and other health professionals with clinical decision support (CDS), that is, assistance with clinical decision-making tasks. A working definition has been proposed by Robert Hayward of the Centre for Health Evidence: ""Clinical Decision Support systems link health observations with health knowledge to influence health choices by clinicians for improved health care"". CDSSs constitute a major topic in artificial intelligence in medicine."
"http://dbpedia.org/resource/Human-centered_computing_(NASA)"	"Human-centered computing (NASA)"	"Artificial intelligence applications"	"Human-Centered Computing is the name of a subproject of NASA's Intelligent Systems Project. It is focused on the development of adaptive systems that amplify human cognitive, perceptual, and motor capabilities in such domains as: space, mission control operations, air traffic management, safety and security systems."
"http://dbpedia.org/resource/Optical_mark_recognition"	"Optical mark recognition"	"Artificial intelligence applications"	"Optical mark recognition (also called optical mark reading and OMR) is the process of capturing human-marked data from document forms such as surveys and tests."
"http://dbpedia.org/resource/Artificial_Intelligence_Applications_Institute"	"Artificial Intelligence Applications Institute"	"Artificial intelligence applications"	"The Artificial Intelligence Applications Institute (AIAI) at the School of Informatics at the University of Edinburgh is a non-profit technology transfer organisation that promotes the benefits of the application of Artificial Intelligence research to commercial, industrial, and government organisations worldwide."
"http://dbpedia.org/resource/Chinese_speech_synthesis"	"Chinese speech synthesis"	"Artificial intelligence applications"	"Chinese speech synthesis is the application of speech synthesis to the Chinese language (usually Standard Chinese). It poses additional difficulties due to the Chinese characters (which frequently have different pronunciations in different contexts), the complex prosody, which is essential to convey the meaning of words, and sometimes the difficulty in obtaining agreement among native speakers concerning what the correct pronunciation is of certain phonemes."
"http://dbpedia.org/resource/Noisy_text_analytics"	"Noisy text analytics"	"Artificial intelligence applications"	"Noisy text analytics is a process of information extraction whose goal is to automatically extract structured or semistructured information from noisy unstructured text data. While Text analytics is a growing and mature field that has great value because of the huge amounts of data being produced, processing of noisy text is gaining in importance because a lot of common applications produce noisy text data. Noisy unstructured text data is found in informal settings such as online chat, text messages, e-mails, message boards, newsgroups, blogs, wikis and web pages. Also, text produced by processing spontaneous speech using automatic speech recognition and printed or handwritten text using optical character recognition contains processing noise. Text produced under such circumstances is typically highly noisy containing spelling errors, abbreviations, non-standard words, false starts, repetitions, missing punctuations, missing letter case information, pause filling words such as “um” and “uh” and other texting and speech disfluencies. Such text can be seen in large amounts in contact centers, chat rooms, optical character recognition (OCR) of text documents, short message service (SMS) text, etc. Documents with historical language can also be considered noisy with respect to today’s knowledge about the language. Such text contains important historical, religious, ancient medical knowledge that is useful. The nature of the noisy text produced in all these contexts warrants moving beyond traditional text analysis techniques."
"http://dbpedia.org/resource/Mind's_Eye_(US_military)"	"Mind's Eye (US military)"	"Artificial intelligence applications"	"The Mind's Eye is a video analysis research project using artificial intelligence. It is funded by the Defense Advanced Research Projects Agency. Twelve research teams have been contracted by DARPA for the Mind's Eye: Carnegie Mellon University, Co57 Systems, Inc., Colorado State University, Jet Propulsion Laboratory/Caltech, Massachusetts Institute of Technology, Purdue University, SRI International, State University of New York at Buffalo, TNO (Netherlands), University of Arizona, University of California Berkeley and the University of Southern California."
"http://dbpedia.org/resource/OpenNN"	"OpenNN"	"Artificial intelligence applications"	"OpenNN (Open Neural Networks Library) is a software library written in the C++ programming language which implements neural networks, a main area of deep learning research. The library is open source, licensed under the GNU Lesser General Public License."
"http://dbpedia.org/resource/Big_mechanism"	"Big mechanism"	"Artificial intelligence applications"	"Big Mechanism is a $45 million DARPA research program, begun in 2014, aimed at developing software that will read cancer research papers, integrate them into a cancer model and frame new hypotheses by the end of 2017."
"http://dbpedia.org/resource/Braina"	"Braina"	"Artificial intelligence applications"	"Braina is an intelligent personal assistant application for Microsoft Windows developed by Brainasoft. Braina uses natural language interface and speech recognition to interact with its users and allows users to use English language sentences to perform various tasks on their computer. The application can find information from the internet, play songs and videos of user's choice, take dictation, find and open files, set alarms and reminders, performs math calculations, controls windows and programs etc. Braina's Android app can be used to interact with the system remotely over a Wi-Fi network. The name Braina is a short form of Brain Artificial. The software adapts to the user's behavior over time to better anticipate needs. The software also allows users to type commands using keyboard instead of saying them. Braina comes in both free and paid version. Future plc's TechRadar recognized Braina as one of the top 10 free essential software for 2015."
"http://dbpedia.org/resource/Eurisko"	"Eurisko"	"Artificial intelligence applications"	"Eurisko (Gr., I discover) is a program written by Douglas Lenat in RLL-1, a representation language itself written in the Lisp programming language. A sequel to Automated Mathematician, it consists of heuristics, i.e. rules of thumb, including heuristics describing how to use and change its own heuristics. Lenat was frustrated by Automated Mathematician's constraint to a single domain and so developed Eurisko; his frustration with the effort of encoding domain knowledge for Eurisko led to Lenat's subsequent (and, as of 2014, continuing) development of Cyc. Lenat envisions ultimately coupling the Cyc knowledgebase with the Eurisko discovery engine."
"http://dbpedia.org/resource/Natachata"	"Natachata"	"Artificial intelligence applications"	"Natachata is a sexbot used to simulate smutty conversations via text messages with paying subscribers, fooling them into believing that she is human. Written by former rocket scientist Simon Luttrell, Natachata is widely used by porn chat merchants. Natachata can handle 15 messages per second at £1 for each SMS text message in the UK. Natachata operates by comparing incoming messages with a database of 100,000 sentences and then concocts a reply based on what it received. It then transcribes the message into slang, adds spelling mistakes and, after a random delay, sends a reply. Most users of such systems would be surprised to learn that they are not conversing with a human."
"http://dbpedia.org/resource/Grandmaster_Chess"	"Grandmaster Chess"	"Artificial intelligence applications"	"Grandmaster Chess is a 1993 video game to play chess for PC DOS platform develop by IntraCorp and its subsidiary Capstone that was focused on neural network technology and an artificial intelligence (AI) able to learn from mistakes. Capable of using VGA and SVGA modes, features multiple skill levels, different sets of pieces, boards and backgrounds, 2D/3D view, pull-down menus, move list with VCR style control, able to analysis moves and games and rate the user strength. Originally it was distributed in floppy discs, but in 1996 in appeared in CD-ROM. This release only relevant addition was the Terminator 2: Judgement Day: Chess Wars package, an animated chess set like Battle Chess video game representing the Terminator 2: Judgment Day movie."
"http://dbpedia.org/resource/Context-sensitive_user_interface"	"Context-sensitive user interface"	"Artificial intelligence applications"	"A context-sensitive user interface is one which can automatically choose from a multiplicity of options based on the current or previous state(s) of the program operation. Context sensitivity is almost ubiquitous in current graphical user interfaces, usually in the form of context menus. Context sensitivity, when operating correctly, should be practically transparent to the user. For example: Clicking on a text document automatically opens the document in a word processing environment. The user does not have to specify what type of program opens the file under standard conditions. The same methodology applies to other file types e.g.: 
* Video files (.mpg .mov .avi etc.) open in a video player without the user having to select a specific program. 
* Photographic and other image files (.jpg .png etc.) will open in a photo viewer automatically. 
* Program files and their shortcuts (i.e. .exe files) are automatically run by the operating system. The user-interface may also provide Context sensitive feedback, such as changing the appearance of the mouse pointer or cursor, changing the menu color, or with applicable auditory or tactile feedback."
"http://dbpedia.org/resource/Imense"	"Imense"	"Artificial intelligence applications"	"Imense Ltd is a UK-based company that develops technology for Content-based image retrieval and automatic image annotation."
"http://dbpedia.org/resource/Echobox"	"Echobox"	"Artificial intelligence applications"	"Echobox is a software company that helps publishers increase traffic by 'intelligently' posting articles on social media platforms such as Facebook and Twitter. By analysing large amounts of data, it learns how specific audiences respond to different articles at different times of the day. It then chooses the best stories to post and the best times to post them. It uses both historical and real-time data to understand to what has worked well in the past as well as what is currently trending on the web. Echobox integrates with a publisher’s existing analytics and data. The technology is used by a number of publishers including Le Monde, Le Figaro, Axel Springer and San Jose Mercury News. In 2016, the company raised $3.4m in venture capital finance from Mangrove Capital Partners and LocalGlobe. The company’s advisors include Zoubin Ghahramani, Professor of Information Engineering at the University of Cambridge."
"http://dbpedia.org/resource/Sunspring"	"Sunspring"	"Artificial intelligence applications"	"Sunspring is a 2016 experimental science fiction short film entirely written by an artificial intelligence bot using neural networks. It was conceived by BAFTA-nominated filmmaker Oscar Sharp and NYU AI researcher Ross Goodwin. It stars Thomas Middleditch, Elisabeth Grey, and Humphrey Ker as three people, namely H, H2, and C, living in a future world and eventually connecting with each other through a love triangle. The script of the film was authored by a recurrent neural network called long short-term memory (LSTM) by an AI bot named Benjamin. Originally made for the Sci-Fi-London film festival's 48hr Challenge, it was released online by technology news website Ars Technica on 9 June 2016."
"http://dbpedia.org/resource/Optical_braille_recognition"	"Optical braille recognition"	"Artificial intelligence applications"	"Optical braille recognition is the act of capturing and processing images of braille characters into natural language characters. It is used to convert braille documents for people who cannot read them into text, and for preservation and reproduction of the documents."
"http://dbpedia.org/resource/Pop_music_automation"	"Pop music automation"	"Artificial intelligence applications"	"Pop music automation is a field of study among musicians and computer scientists with a goal of producing successful pop music algorithmically. It is often based on the premise that pop music is especially formulaic, unchanging, and easy to compose. The idea of automating pop music composition is related to many ideas in algorithmic music, Artificial Intelligence (AI) and computational creativity."
"http://dbpedia.org/resource/GestureTek"	"GestureTek"	"Artificial intelligence applications"	"GestureTek is an American-based interactive technology company headquartered in Silicon Valley, California, with offices in Toronto and Ottawa, Ontario and Asia."
"http://dbpedia.org/resource/WebCrow"	"WebCrow"	"Artificial intelligence applications"	"The WebCrow is a research project carried out at the Information Engineering of the University of Siena with the purpose of automatically solving crosswords."
"http://dbpedia.org/resource/Validis"	"Validis"	"Artificial intelligence applications"	"Validis is a web based service which detects anomalies in accounting data in very much the same manner as a spell checker finds errors in a text document. The word itself is derived from a concatenation of Validate and Discover. The primary user base is among accounting professionals both within the financial community of corporate and SME business and across professional accounting practice. It provides a means to identify out of line data that is incomplete, invalid, inconsistent, or inaccurate, an analysis collectively tagged the ‘Four Is’."
"http://dbpedia.org/resource/Artificial_intelligence_marketing"	"Artificial intelligence marketing"	"Artificial intelligence applications"	"Artificial intelligence marketing (AIM) is a form of direct marketing leveraging database marketing techniques as well as AI concept and model such as machine learning and Bayesian Network. The main difference resides in the reasoning part which suggests it is performed by computer and algorithm instead of human."
"http://dbpedia.org/resource/Activity_recognition"	"Activity recognition"	"Artificial intelligence applications"	"Activity recognition aims to recognize the actions and goals of one or more agents from a series of observations on the agents' actions and the environmental conditions. Since the 1980s, this research field has captured the attention of several computer science communities due to its strength in providing personalized support for many different applications and its connection to many different fields of study such as medicine, human-computer interaction, or sociology. To understand activity recognition better, consider the following elderly assistance scenario. An elderly man wakes up at dawn in his apartment, where he stays alone. He lights the stove to make a pot of tea, switches on the toaster oven, and takes some bread and jelly from the cupboard. After he takes his morning medication, a computer-generated voice gently reminds him to turn off the toaster. Later that day, his daughter accesses a secure website where she scans a check-list, which was created by a sensor network in her father's apartment. She finds that her father is eating normally, taking his medicine on schedule, and continuing to manage his daily life on his own. Due to its many-faceted nature, different fields may refer to activity recognition as plan recognition, goal recognition, intent recognition, behavior recognition, location estimation and location-based services."
"http://dbpedia.org/resource/YouNoodle"	"YouNoodle"	"Artificial intelligence applications"	"YouNoodle is a San Francisco-based company, with offices in Barcelona and Santiago, founded in 2010, building a platform for entrepreneurship competitions all over the world. YouNoodle matches entrepreneurs with competitions, accelerators, and startup programs, and provides a judging and voting SaaS platform to university, non-profit, government and enterprise clients organizing innovation challenges and competitions. Stanford's BASES, UC Berkeley's B-Plan, Start-Up Chile, Amazon Startup Challenge, and NASA are all running one or more competitions on YouNoodle's platform."
"http://dbpedia.org/resource/Artificial_intuition"	"Artificial intuition"	"Artificial intelligence applications"	"Artificial intuition is the capacity of an artificial object or software to function with the factor of consciousness known as intuition, or a machine-based system that has some capacity to function analogous to human intuition."
"http://dbpedia.org/resource/Document_capture_software"	"Document capture software"	"Artificial intelligence applications"	"Document Capture Software refers to applications that provide the ability and feature set to automate the process of scanning paper documents. Most scanning hardware, both scanners and copiers, provides the basic ability to scan to any number of image file formats, including: PDF, TIFF, JPG, BMP, etc. This basic functionality is augmented by document capture software, which can add efficiency and standardization to the process."
"http://dbpedia.org/resource/Trenchard_More"	"Trenchard More"	"Artificial intelligence applications"	"Trenchard More is a professor at Dartmouth College. He participated in the 1956 Dartmouth Summer Research Project on Artificial Intelligence. At the 50th year meeting of the Dartmouth Conference with Marvin Minsky, Geoffrey Hinton and Simon Osindero he presented The Future of Network Models and also gave a lecture entitled Routes to the Summit. Designed a theory for nested rectangular array that provided a formal structure used in the development of the  Nested Interactive Array Language."
"http://dbpedia.org/resource/List_of_Prisma_(app)_filters"	"List of Prisma (app) filters"	"Artificial intelligence applications"	"This is a list of Prisma filters for the photo-editing application to render images into an artistic effect."
"http://dbpedia.org/resource/Language_Acquisition_Device_(computer)"	"Language Acquisition Device (computer)"	"Artificial intelligence applications"	"The Language Acquisition Device is a computer program developed by Lobal Technologies, a computer company in the United Kingdom, and scientists from King's College. It emulates the functions of the brain's frontal lobes where humans process language and emotion. Scientists hope this might enable computers to understand, speak, learn, and eventually think. One possible use is in interactive entertainment such as video gaming, where the technology is used to help computer-controlled characters to develop. A press release describing this technology produced widespread media interest in 2002, but no reports have been published since then, and the current status of the technology is unclear."
"http://dbpedia.org/resource/ETAP-3"	"ETAP-3"	"Artificial intelligence applications"	"ETAP-3 is a proprietary linguistic processing system focusing on English and Russian. It was developed in Moscow, Russia at the Institute for Information Transmission Problems (ru:Институт проблем передачи информации им. А. А. Харкевича РАН). It is a rule-based system which uses the Meaning-Text Theory as its theoretical foundation. At present, there are several applications of ETAP-3, such as a machine translation tool, a converter of the Universal Networking Language, an interactive learning tool for Russian language learners and a syntactically annotated corpus of Russian language. Demo versions of some of these tools are available online."
"http://dbpedia.org/resource/Sayre's_paradox"	"Sayre's paradox"	"Artificial intelligence applications"	"Sayre’s Paradox is a dilemma encountered in the design of automated handwriting recognition systems. A standard statement of the paradox is that a cursively written word cannot be recognized without being segmented and cannot be segmented without being recognized. The paradox was first articulated in a 1973 publication by Kenneth M. Sayre, after whom it was named."
"http://dbpedia.org/resource/Marketing_and_artificial_intelligence"	"Marketing and artificial intelligence"	"Artificial intelligence applications"	"Advancements in Artificial intelligence’s application to a range of disciplines have led to the development of Artificial intelligence systems which have proved useful to marketers. These systems assist in areas such as market forecasting, automation of processes and decision making and increase the efficiency of tasks which would usually be performed by humans. The science behind these systems can be explained through neural networks and expert systems which are computer programs that process input and provide valuable output for marketers. In the area of social networking, AI is used to Artificial intelligence systems stemming from Social computing technology can be applied to understand social networks on the Web. Data mining techniques can be used to analyze different types of social networks. This analysis helps a marketer to identify influential actors or nodes within networks, this information can then be applied to take a Societal marketing approach. Artificial intelligence has gained significant recognition in the marketing industry. However, ethical issues surrounding these systems and their potential to impact on the need for humans in the workforce, specifically marketing, is a controversial topic."
"http://dbpedia.org/resource/Neural_machine_translation"	"Neural machine translation"	"Artificial intelligence applications"	"Neural machine translation (NMT) is the approach to machine translation in which a large neural network is trained to maximize translation performance. It is a radical departure from the phrase-based statistical translation approaches, in which a translation system consists of subcomponents that are separately optimized. The artificial neural network (ANN) is a model inspired by the functional aspects and structure of the brain’s biological neural networks. With use of ANN, it is possible to execute a number of tasks, such as classification, clustering, and prediction, using machine learning techniques like supervised or reinforced learning to learn or adjust net connections. A bidirectional recurrent neural network (RNN), known as an encoder, is used by the neural network to encode a source sentence for a second RNN, known as a decoder, that is used to predict words in the target language. NMT models are inspired by deep representation learning. They require only a fraction of the memory needed by traditional statistical machine translation (SMT) models. Furthermore, unlike conventional translation systems, each and every component of the neural translation model is trained jointly to maximize the translation performance. When a new neural network is created, it is trained for certain domains or applications. Once an automatic learning mechanism is established, the network practices. With time it starts operating according to its own judgment, turning into an ""expert""."
"http://dbpedia.org/resource/Face_detection"	"Face detection"	"Face recognition"	"Face detection is a computer technology being used in a variety of applications that identifies human faces in digital images. Face detection also refers to the psychological process by which humans locate and attend to faces in a visual scene."
"http://dbpedia.org/resource/Facial_Images_National_Database"	"Facial Images National Database"	"Face recognition"	"The Facial Images National Database (FIND) was a project managed by the National Policing Improvement Agency. The database was a collection of mugshots both from still and from video image sources. It was also designed to keep track of scars, tattoos, and similar markings on persons within the database to increase efficiency in identification. It was intended that FIND would provide national access to images of individuals who have been arrested for a criminal offence, linking the image with the criminal data held on the Police National Computer. The pilot went live on 6 November 2006, with Lancashire, West Yorkshire and Merseyside contributing and viewing images. Greater Manchester, North Wales, Devon and Cornwall, British Transport Police (BTP) North Eastern Region, as well as one of the Metropolitan Police specialist units and eBorders had read only access to the system. The forward plan for FIND included the addition of facial recognition software (much like the United States' FERET database) to the system. Due to budget pressures, the project was cancelled in early 2008 but this decision was under review in October 2008."
"http://dbpedia.org/resource/Glasgow_Face_Matching_Test"	"Glasgow Face Matching Test"	"Face recognition"	"The Glasgow Face Matching Test (GFMT) was created by researchers at the University of Glasgow and at Glasgow Caledonian University. It is a cognitive test designed to determine a person's ability to match different images of unfamiliar faces, and is designed for use in academic research and in applied security settings, where reliable human performance on this task is a common requirement of identity management systems. The test was created using a database of photographs, taken of a demographically heterogeneous sample of 300 people. Images of the individuals were captured in a fifteen-minute session on two digital cameras (one Digital Video, one digital still). Similar individuals were paired to make 'different' pairs and 'same' pairs were made by pairing images from the two different cameras. All images used were high quality, with the subject standing face on and looking straight at the camera lens, which was positioned at head height. There are two versions of the test, one short version comprising 40 ""same-or-different"" 2AFC decisions and another longer version with 164 decisions. These tests, complete with normative data, are accessible via the journal article referenced below."
"http://dbpedia.org/resource/Face_Recognition_Vendor_Test"	"Face Recognition Vendor Test"	"Face recognition"	"The Face Recognition Vendor Test (FRVT) was a series of large scale independent evaluations for face recognition systems realized by the National Institute of Standards and Technology in 2000, 2002 and 2006. Previous evaluations in the series were the Face Recognition Technology (FERET) evaluations in 1994, 1995 and 1996."
"http://dbpedia.org/resource/Three-dimensional_face_recognition"	"Three-dimensional face recognition"	"Face recognition"	"Three-dimensional face recognition (3D face recognition) is a modality of facial recognition methods in which the three-dimensional geometry of the human face is used. It has been shown that 3D face recognition methods can achieve significantly higher accuracy than their 2D counterparts, rivaling fingerprint recognition. 3D face recognition has the potential to achieve better accuracy than its 2D counterpart by measuring geometry of rigid features on the face. This avoids such pitfalls of 2D face recognition algorithms as change in lighting, different facial expressions, make-up and head orientation. Another approach is to use the 3D model to improve accuracy of traditional image based recognition by transforming the head into a known view. Additionally, most 3D scanners acquire both a 3D mesh and the corresponding texture. This allows combining the output of pure 3D matchers with the more traditional 2D face recognition algorithms, thus yielding better performance (as shown in FRVT 2006). The main technological limitation of 3D face recognition methods is the acquisition of 3D image, which usually requires a range camera. Alternatively, multiple images from different angles from a common camera (e.g. webcam) may be used to create the 3D model with significant post-processing. (See 3D data acquisition and object reconstruction.) This is also a reason why 3D face recognition methods have emerged significantly later (in the late 1980s) than 2D methods. Recently commercial solutions have implemented depth perception by projecting a grid onto the face and integrating video capture of it into a high resolution 3D model. This allows for good recognition accuracy with low cost off-the-shelf components. 3D face recognition is still an active research field, though several vendors offer commercial solutions."
"http://dbpedia.org/resource/Facial_recognition_system"	"Facial recognition system"	"Face recognition"	"A facial recognition system is a computer application capable of identifying or verifying a person from a digital image or a video frame from a video source. One of the ways to do this is by comparing selected facial features from the image and a facial database. It is typically used in security systems and can be compared to other biometrics such as fingerprint or eye iris recognition systems. Recently, it has also become popular as a commercial identification and marketing tool."
"http://dbpedia.org/resource/Picollator"	"Picollator"	"Face recognition"	"Picollator - Internet search engine that performs search for web sites and multimedia by visual query (image) or text, or a combination of visual query and text. Picollator recognizes objects in the image, obtains their relevance to the text and vice a versa, and searches in accordance with all information provided."
"http://dbpedia.org/resource/Eigenface"	"Eigenface"	"Face recognition"	"Eigenfaces is the name given to a set of eigenvectors when they are used in the computer vision problem of human face recognition. The approach of using eigenfaces for recognition was developed by Sirovich and Kirby (1987) and used by Matthew Turk and Alex Pentland in face classification. The eigenvectors are derived from the covariance matrix of the probability distribution over the high-dimensional vector space of face images. The eigenfaces themselves form a basis set of all images used to construct the covariance matrix. This produces dimension reduction by allowing the smaller set of basis images to represent the original training images. Classification can be achieved by comparing how faces are represented by the basis set."
"http://dbpedia.org/resource/Cross-race_effect"	"Cross-race effect"	"Face recognition"	"The cross-race effect (sometimes called cross-race bias, other-race bias or own-race bias) refers to the tendency to more easily recognize members of one's own race. A study was made which examined 271 real court cases. In photographic line-ups, 231 witnesses participated in cross-race versus same-race identification. In cross-race lineups, only 45% were correctly identified versus 60% for same-race identifications. In social psychology, the cross-race effect is described as the ""ingroup advantage"". In other fields, the effect can be seen as a specific form of the ""ingroup advantage"" since it is only applied in interracial or inter-ethnic situations, whereas ""ingroup advantage"" can refer to mono-ethnic situations as well. Deeper study of the cross-race effect has also demonstrated two types of processing for the recognition of faces: featural and holistic. It has been found that holistic processing (which occurs beyond individual parts of the face) is more commonly used in same-race situations, but there is an experience effect, which means that as a person gains more experience with those of a particular race, he or she will begin to use more holistic processing. Featural processing is much more commonly used with an unfamiliar stimulus or face."
"http://dbpedia.org/resource/Viola–Jones_object_detection_framework"	"Viola–Jones object detection framework"	"Face recognition"	"The Viola–Jones object detection framework is the first object detection framework to provide competitive object detection rates in real-time proposed in 2001 by Paul Viola and Michael Jones. Although it can be trained to detect a variety of object classes, it was motivated primarily by the problem of face detection. This algorithm is implemented in OpenCV as cvHaarDetectObjects()."
"http://dbpedia.org/resource/Iris_Challenge_Evaluation"	"Iris Challenge Evaluation"	"Face recognition"	"The Iris Challenge Evaluation (ICE) were a series of events conducted and managed by the National Institute of Standards and Technology for projects on technology development and evaluation for iris recognition. The ICE 2006 was the first large-scale, open, independent technology evaluation for iris recognition. The primary goals of the ICE projects were to promote the development and advancement of iris recognition technology and assess its state-of-the-art capability. The ICE projects were open to academia, industry and research institutes."
"http://dbpedia.org/resource/Thatcher_effect"	"Thatcher effect"	"Face recognition"	"The Thatcher effect or Thatcher illusion is a phenomenon where it becomes more difficult to detect local feature changes in an upside-down face, despite identical changes being obvious in an upright face. It is named after the former British Prime Minister Margaret Thatcher, on whose photograph the effect was first and most famously demonstrated. The effect was originally created in 1980 by Peter Thompson, Professor of Psychology at the University of York"
"http://dbpedia.org/resource/Face_hallucination"	"Face hallucination"	"Face recognition"	"Face hallucination refers to any superresolution technique which applies specifically to faces. It comprises techniques which take noisy or low-resolution facial images, and convert them into high-resolution images using knowledge about typical facial features. It can be applied in facial recognition systems for identifying faces faster and more effectively.Due to the potential applications in facial recognition systems, face hallucination has become an active area of research."
"http://dbpedia.org/resource/Coke_Zero_Facial_Profiler"	"Coke Zero Facial Profiler"	"Face recognition"	"Facial Profiler was a free Facebook app created to promote Coca-Cola Zero by the advertising agency Crispin Porter & Bogusky. The app used face recognition technology to search a database of voluntarily participating Facebook users  to match people based on appearance. The software's algorithm analyzed face attributes like skin color, face structure and angles of the face. Once matched, users could contact their look-alike via their Facebook profile. Coke Zero claimed the inspiration for Facial Profiler came from the drink itself. The tagline for the project was: ""If Coke Zero has Coke's taste, is it possible someone out there has your face?"""
"http://dbpedia.org/resource/Polar_Rose_(facial_recognition)"	"Polar Rose (facial recognition)"	"Face recognition"	"Polar Rose was a company from Malmö, Sweden which made facial recognition software. Polar Rose had a service that allows users to name people in their photos on photo sharing sites like Flickr and 23hq.com using their Facebook contacts. Using their facial recognition Polar Rose applied auto-tagging for users. The service was discontinued on 6 September 2010 only to be purchased two weeks later by Apple Inc. for approximately $29 million. The Polar Rose website and related add-ons for Firefox and Internet Explorer allowed users to put names to the faces of people whose photographs appear on the web, and to then find other pictures of the same individuals; however, as of May 20, 2009, use of the plugins was discontinued in favor of a purely web-based interface. Search Wikia featured integration with Polar Rose. Polar Rose founder Jan Erik Solem was company CTO while Danish serial entrepreneur Nikolaj Nyholm is CEO. Polar Rose was awarded a World Economic Forum Technology Pioneer Award in 2008  and has won other awards like SIME's 2007 Best Technical Innovation and the Red Herring Global 100 list for 2007."
"http://dbpedia.org/resource/L-1_Identity_Solutions"	"L-1 Identity Solutions"	"Face recognition"	"L-1 Identity Solutions, Inc. is a large American defense contractor in Connecticut. It was formed on August 29, 2006, from a merger of Viisage Technology, Inc. and Identix Incorporated. It specializes in selling face recognition systems, electronic passports such as Fly Clear, and other biometric technology to governments such as the United States and Saudi Arabia. It also licenses technology to other companies internationally, including China. On July 26, 2011, Safran (NYSE Euronext Paris: SAF) acquired L-1 Identity Solutions, Inc. for a total cash amount of USD 1.09 billion.|author=Business Wire |date=20010  L-1 is now part of Morpho's MorphoTrust department. Bioscrypt is a biometrics research, development and manufacturing company purchased by L-1 Identity Solutions. It provides fingerprint IP readers for physical access control systems, Facial recognition system readers for contactless access control authentication and OEM fingerprint modules for embedded applications. According to IMS Research, Bioscrypt has been the world market leader in biometric access control for enterprises (since 2006) with a worldwide market share of over 13%. In 2011, Bioscrypt was sold to Safran Morpho."
"http://dbpedia.org/resource/FERET_database"	"FERET database"	"Face recognition"	"The FERET database is a standard dataset used for facial recognition system evaluation. The Face Recognition Technology (FERET) program is managed by the Defense Advanced Research Projects Agency (DARPA) and the National Institute of Standards and Technology (NIST). A database of facial imagery was collected between December 1993 and August 1996. In 2003 DARPA released a high-resolution, 24-bit color version of these images. The dataset tested includes 2,413 still facial images, representing 856 individuals. The FERET program was set out to establish a large database of facial images that was gathered independently from the algorithm developers. Dr. Harry Wechsler at George Mason University was selected to direct the collection of this database. The database collection was a collaborative effort between Dr. Wechsler and Dr. Phillips."
"http://dbpedia.org/resource/E-FIT"	"E-FIT"	"Face recognition"	"Electronic Facial Identification Technique (E-FIT, e-fit, efit) is a computer-based method of producing facial composites of wanted criminals, based on eyewitness descriptions."
"http://dbpedia.org/resource/Face_perception"	"Face perception"	"Face recognition"	"Face perception is an individual's understanding and interpretation of the face, particularly the human face, especially in relation to the associated information processing in the brain. The proportions and expressions of the human face are important to identify origin, emotional tendencies, health qualities, and some social information. From birth, faces are important in the individual's social interaction. Face perceptions are very complex as the recognition of facial expressions involves extensive and diverse areas in the brain. Sometimes, damaged parts of the brain can cause specific impairments in understanding faces or prosopagnosia."
"http://dbpedia.org/resource/Prosopagnosia"	"Prosopagnosia"	"Face recognition"	"Prosopagnosia /ˌprɑːsəpæɡˈnoʊʒə/ (Greek: ""prosopon"" = ""face"", ""agnosia"" = ""not knowing""), also called face blindness, is a cognitive disorder of face perception where the ability to recognize familiar faces, including one's own face (self-recognition), is impaired, while other aspects of visual processing (e.g., object discrimination) and intellectual functioning (e.g., decision making) remain intact. The term originally referred to a condition following acute brain damage (acquired prosopagnosia), but a congenital or developmental form of the disorder also exists, which may affect up to 2.5% of the population. The specific brain area usually associated with prosopagnosia is the fusiform gyrus, which activates specifically in response to faces. The functionality of the fusiform gyrus allows most people to recognize faces in more detail than they do similarly complex inanimate objects. For those with prosopagnosia, the new method for recognizing faces depends on the less-sensitive object recognition system. The right hemisphere fusiform gyrus is more often involved in familiar face recognition than the left. It remains unclear whether the fusiform gyrus is only specific for the recognition of human faces or if it is also involved in highly trained visual stimuli. There are two types of prosopagnosia: acquired and congenital (developmental). Acquired prosopagnosia results from occipito-temporal lobe damage and is most often found in adults. This is further subdivided into apperceptive and associative prosopagnosia. In congenital prosopagnosia, the individual never adequately develops the ability to recognize faces. Though there have been several attempts at remediation, no therapies have demonstrated lasting real-world improvements across a group of prosopagnosics. Prosopagnosics often learn to use ""piecemeal"" or ""feature-by-feature"" recognition strategies. This may involve secondary clues such as clothing, gait, hair color, skin color, body shape, and voice. Because the face seems to function as an important identifying feature in memory, it can also be difficult for people with this condition to keep track of information about people, and socialize normally with others. Prosopagnosia has also been associated with other disorders that are associated with nearby brain areas: left hemianopsia (loss of vision from left side of space, associated with damage to the right occipital lobe), achromatopsia (a deficit in color perception often associated with unilateral or bilateral lesions in the temporo-occipital junction) and topographical disorientation (a loss of environmental familiarity and difficulties in using landmarks, associated with lesions in the posterior part of the parahippocampal gyrus and anterior part of the lingual gyrus of the right hemisphere)."
"http://dbpedia.org/resource/Face_Recognition_Grand_Challenge"	"Face Recognition Grand Challenge"	"Face recognition"	"The Face Recognition Grand Challenge (FRGC) was conducted in an effort to promote and advance face recognition technology. It was the successor of the Face Recognition Vendor Test."
"http://dbpedia.org/resource/Multiple_Biometric_Grand_Challenge"	"Multiple Biometric Grand Challenge"	"Face recognition"	"Multiple Biometric Grand Challenge (MBGC) is a biometric project."
"http://dbpedia.org/resource/FERET_(facial_recognition_technology)"	"FERET (facial recognition technology)"	"Face recognition"	"The Facial Recognition Technology (FERET) program was sponsored by The US Department of Defense (DoD) Counterdrug Technology Development Program Office. The goal of the FERET program was to develop automatic face recognition capabilities that could be employed to assist security, intelligence, and law enforcement personnel in the performance of their duties. The program consisted of three major elements: 
* Sponsoring research 
* Collecting the FERET database 
* Performing the FERET evaluations The goal of the sponsored research was to develop face recognition algorithms. The FERET database was collected to support the sponsored research and the FERET evaluations. The FERET evaluations were performed to measure progress in algorithm development and identify future research directions."
"http://dbpedia.org/resource/Face.com"	"Face.com"	"Face recognition"	"Face.com was a Tel Aviv-based technology company that developed a platform for efficient and accurate facial recognition in photos uploaded via web and mobile applications. Face.com apps and API services scanned billions of photos monthly and tagged faces in those photos, tying them to social networking information. As of February 2011, the company had “discovered” 18 billion faces across its API and Facebook applications. The company was established in 2009, and maintained an office in Tel Aviv, Israel with 10 employees. Face.com developed and released two Facebook applications: Photo Finder and Photo Tagger. Photo Finder allowed users to find untagged pictures of themselves as well as friends on Facebook and then tag those photos. Photo Tagger enabled bulk-tagging of faces that appear in multiple photos uploaded to the Facebook website. On June 18, 2012, the company announced that it has been acquired by Facebook. It was also announced that they were winding up their API's to focus on new Facebook products."
"http://dbpedia.org/resource/Viewdle"	"Viewdle"	"Face recognition"	"Viewdle is a Ukrainian facial recognition company. Google was reportedly close to closing a deal to buy the company for between $30 to $45 million in October 2012."
"http://dbpedia.org/resource/Cascade_Learning_Based_on_Adaboost"	"Cascade Learning Based on Adaboost"	"Face recognition"	"The Boosting Algorithms for Detector Cascade Learning is proposed by Mohammad Saberian and Nuno Vasconcelos in 2014, it is based on Viola–Jones object detection framework."
"http://dbpedia.org/resource/Covert_facial_recognition"	"Covert facial recognition"	"Face recognition"	"Joachim Bodamer created the term prosopagnosia in 1947, which is a disorder where individuals have an inability to recognize faces of people. Individuals with this disorder do not have the ability to overtly recognize faces, but discoveries have been made showing that people with this disorder have the ability to covertly recognize faces. Covert facial recognition is the unconscious recognition of familiar faces by people with prosopagnosia. The individuals who express this phenomenon are unaware that they are recognizing the faces of people they have seen before. There are two types of prosopagnosia, congenital and acquired. Congential prosopagnosia is an inability to recognize faces without a history of brain damage; while acquired prosopagnosia is caused by damage to the right occipital-temporal region of the brain. In the 1950s it was theorized that the right cerebral hemisphere was involved in facial recognition and in the 1960s this theory was supported by many experiments. Although the ability for overt facial recognition is inhibited in patients with prosopagnosia, there have been many studies done which show that some of these individuals may have the ability to recognize familiar faces covertly. These experiments have used behavioral and physiological measures in order to demonstrate covert facial recognition. A common physiological measure that is used is the measure of autonomic activity by using skin-conductance responses (SCR) which show a larger response in individuals with prosopagnosia who are shown pictures of familiar faces compared to pictures of unfamiliar faces."
"http://dbpedia.org/resource/Super_recogniser"	"Super recogniser"	"Face recognition"	"Super recognisers are people with extraordinary good face recognition ability. It is the extreme opposite of prosopagnosia. It is estimated that 1–2% of population are super recognisers who can remember 80% of faces they have seen. Normal people can only remember about 20% of faces. They have proved to be far superior to computer recognition systems. The science behind this is poorly understood but may be related to the fusiform face area part of the brain. In May 2015, the Metropolitan Police officially formed a team made up of people with a ""superpower"" for recognising people and put them to work identifying crooks whose faces are captured on CCTV. Scotland Yard has a squad of over 200 super recognisers."
"http://dbpedia.org/resource/People_Staring_at_Computers"	"People Staring at Computers"	"Face recognition"	"People Staring at Computers is an art project and Tumblr Blog created by Kyle McDonald in June 2011. McDonald traveled to two Apple Store locations in New York City; one on West 14th Street and one on SoHo Street. Each Apple Store contains about 50 desktop and laptop computers, half of which were used for the project after installing an application. The application was created by McDonald and took one picture from the Apple Store’s computer webcam every minute. This application also included face recognition, allowing the computer to take a picture only if a face was detected in front of the webcam. Apple Stores routinely clear their computer servers every night so each day McDonald had to reinstall the program. McDonald would also take video of the customer’s reactions once the picture was taken. The project was complete after three days and in total collected over a thousand photographs of Apple Store customers."
"http://dbpedia.org/resource/Face_superiority_effect"	"Face superiority effect"	"Face recognition"	"In psychology, the face superiority effect refers to the phenomena of how human faces are perceived and encoded in memory. Rather than perceiving and encoding individual features of a face (nose, eyes etc.), we perceive and encode a human face as a ""holistic"" unified whole. This phenomena aids our visual system in the recognition of thousands of faces, a task that would be difficult if it were necessary to recognize a set of individual features and characteristics. This effect is limited to the perception of upright faces and does not occur when face is in an unusual angle. A 1967 study presented paired images of faces to participants. The faces were either both upright, both inverted, or mixed. Then the participants were shown fifteen pairs of photographs and asked to decide which one they had seen. The participants remembered the upright faces more than the inverted faces."
"http://dbpedia.org/resource/Face_space"	"Face space"	"Face recognition"	"Face space is a theoretical idea in psychology such that it is a multidimensional space in which recognisable faces are stored. The representation of faces within this space are according to invariant features of the face itself. However, recently was theoretically demonstrated that faces can be stored in the face space according to their dynamic features as well, and that in this case the resulting space exhibits a twofold structure. The face space is useful for accounting various aspects of face recognition including distinctiveness, the own race bias  and caricature effects. The face space framework has been cited in almost 1000 scientific articles and it was recently revisited in a special edition of the journal Quarterly Journal of Experimental Psychology featuring the top 10 ideas that have appeared in the journal's pages."
"http://dbpedia.org/resource/Prosopometamorphopsia"	"Prosopometamorphopsia"	"Face recognition"	"Prosopometamorphopsia is a rare visual perceptual distortion resulting in an altered perception of faces. It is distinct from prosopagnosia which is characterised by the inability to recognise faces and it relatively more common."
"http://dbpedia.org/resource/ViBe"	"ViBe"	"Computer vision"	"ViBe is a background subtraction algorithm which has been presented at the IEEE ICASSP 2009 conference and was refined in later publications. More precisely, it is a software module for extracting background information from moving images. It has been developed by Oliver Barnich and Marc Van Droogenbroeck of the Montefiore Institute, University of Liège, Belgium. ViBe is patented: the patent covers various aspects such as stochastic replacement, spatial diffusion, and non-chronological handling. ViBe is written in the programming language C, and has been implemented on CPU, GPU and FPGA."
"http://dbpedia.org/resource/Intrinsic_dimension"	"Intrinsic dimension"	"Computer vision"	"In signal processing of multidimensional signals, for example in computer vision, the intrinsic dimension of the signal describes how many variables are needed to represent the signal. For a signal of N variables, its intrinsic dimension M satisfies 0 ≤ M ≤ N. Usually the intrinsic dimension of a signal relates to variables defined in a Cartesian coordinate system. In general, however, it is also possible to describe the concept for non-Cartesian coordinates, for example, using polar coordinates."
"http://dbpedia.org/resource/Machine_vision"	"Machine vision"	"Computer vision"	"Machine vision (MV) is the technology and methods used to provide imaging-based automatic inspection and analysis for such applications as automatic inspection, process control, and robot guidance in industry. The scope of MV is broad. MV is related to, though distinct from, computer vision."
"http://dbpedia.org/resource/Visual_descriptor"	"Visual descriptor"	"Computer vision"	"In computer vision, visual descriptors or image descriptors are descriptions of the visual features of the contents in images, videos, or algorithms or applications that produce such descriptions. They describe elementary characteristics such as the shape, the color, the texture or the motion, among others."
"http://dbpedia.org/resource/Multi-scale_approaches"	"Multi-scale approaches"	"Computer vision"	"The scale space representation of a signal obtained by Gaussian smoothing satisfies a number of special properties, scale-space axioms, which make it into a special form of multi-scale representation. There are, however, also other types of ""multi-scale approaches"" in the areas of computer vision, image processing and signal processing, in particular the notion of wavelets. The purpose of this article is to describe a few of these approaches:"
"http://dbpedia.org/resource/Scale-space_axioms"	"Scale-space axioms"	"Computer vision"	"In image processing and computer vision, a scale space framework can be used to represent an image as a family of gradually smoothed images. This framework is very general and a variety of scale space representations exist. A typical approach for choosing a particular type of scale space representation is to establish a set of scale-space axioms, describing basic properties of the desired scale-space representation and often chosen so as to make the representation useful in practical applications. Once established, the axioms narrow the possible scale-space representations to a smaller class, typically with only a few free parameters. A set of standard scale space axioms, discussed below, leads to the linear Gaussian scale-space, which is the most common type of scale space used in image processing and computer vision."
"http://dbpedia.org/resource/Scale_space_implementation"	"Scale space implementation"	"Computer vision"	"The linear scale-space representation of an N-dimensional continuous signal, is obtained by convolving fC with an N-dimensional Gaussian kernel: In other words: However, for implementation, this definition is impractical, since it is continuous. When applying the scale space concept to a discrete signal fD, different approaches can be taken. This article is a brief summary of some of the most frequently used methods."
"http://dbpedia.org/resource/Stereo_cameras"	"Stereo cameras"	"Computer vision"	"The stereo cameras approach is a method of distilling a noisy video signal into a coherent data set that a computer can begin to process into actionable symbolic objects, or abstractions. Stereo cameras is one of many approaches used in the broader fields of computer vision and machine vision."
"http://dbpedia.org/resource/Articulated_body_pose_estimation"	"Articulated body pose estimation"	"Computer vision"	"Articulated body pose estimation, in computer vision, is the study of algorithms and systems that recover the pose of an articulated body, which consists of joints and rigid parts using image-based observations. It is one of longest-lasting problems in computer vision because of the complexity of the models that relate observation with pose, and because of the variety of situations in which it would be useful."
"http://dbpedia.org/resource/Active_appearance_model"	"Active appearance model"	"Computer vision"	"An active appearance model (AAM) is a computer vision algorithm for matching a statistical model of object shape and appearance to a new image. They are built during a training phase. A set of images, together with coordinates of landmarks that appear in all of the images, is provided to the training supervisor. The model was first introduced by Edwards, Cootes and Taylor in the context of face analysis at the 3rd International Conference on Face and Gesture Recognition, 1998. Cootes, Edwards and Taylor further described the approach as a general method in computer vision at the European Conference on Computer Vision in the same year. The approach is widely used for matching and tracking faces and for medical image interpretation. The algorithm uses the difference between the current estimate of appearance and the target image to drive an optimization process.By taking advantage of the least squares techniques, it can match to new images very swiftly. It is related to the active shape model (ASM). One disadvantage of ASM is that it only uses shape constraints (together with some information about the image structure near the landmarks), and does not take advantage of all the available information – the texture across the target object. This can be modelled using an AAM."
"http://dbpedia.org/resource/Active_shape_model"	"Active shape model"	"Computer vision"	"Active shape models (ASMs) are statistical models of the shape of objects which iteratively deform to fit to an example of the object in a new image, developed by Tim Cootes and Chris Taylor in 1995. The shapes are constrained by the PDM (point distribution model) Statistical Shape Model to vary only in ways seen in a training set of labelled examples. The shape of an object is represented by a set of points (controlled by the shape model). The ASM algorithm aims to match the model to a new image. The ASM works by alternating the following steps: 
*  Generate a suggested shape by looking in the image around each point for a better position for the point. This is commonly done using what is called a ""profile model"", which looks for strong edges or uses the Mahalanobis distance to match a model template for the point. 
*  Conform the suggested shape to the point distribution model, commonly called a ""shape model"" in this context. The figure to the right shows an example. The technique has been widely used to analyse images of faces, mechanical assemblies and medical images (in 2D and 3D). It is closely related to the active appearance model. It is also known as a ""Smart Snakes"" method, since it is an analog to an active contour model which would respect explicit shape constraints."
"http://dbpedia.org/resource/Binocular_disparity"	"Binocular disparity"	"Computer vision"	"Binocular disparity refers to the difference in image location of an object seen by the left and right eyes, resulting from the eyes’ horizontal separation (parallax). The brain uses binocular disparity to extract depth information from the two-dimensional retinal images in stereopsis. In computer vision, binocular disparity refers to the difference in coordinates of similar features within two stereo images. A similar disparity can be used in rangefinding by a coincidence rangefinder to determine distance and/or altitude to a target. In astronomy, the disparity between different locations on the Earth can be used to determine various celestial parallax, and Earth's orbit can be used for stellar parallax."
"http://dbpedia.org/resource/Digital_image_processing"	"Digital image processing"	"Computer vision"	"Digital image processing is the use of computer algorithms to perform image processing on digital images. As a subcategory or field of digital signal processing, digital image processing has many advantages over analog image processing. It allows a much wider range of algorithms to be applied to the input data and can avoid problems such as the build-up of noise and signal distortion during processing. Since images are defined over two dimensions (perhaps more) digital image processing may be modeled in the form of multidimensional systems."
"http://dbpedia.org/resource/Document_mosaicing"	"Document mosaicing"	"Computer vision"	"Document mosaicing is a process that stitches multiple, overlapping snapshot images of a document together in order to produce one large, high resolution composite. The document is slid under a stationary, over-the-desk camera by hand until all parts of the document are snapshotted by the camera’s field of view. As the document slid under the camera, all motion of the document is coarsely tracked by the vision system. The document is periodically snapshotted such that the successive snapshots are overlap by about 50%. The system then finds the overlapped pairs and stitches them together repeatedly until all pairs are stitched together as one piece of document. The document mosaicing can be divided into four main processes. 
* Tracking 
* Feature detecting 
* Correspondences establishing 
* Images mosaicing."
"http://dbpedia.org/resource/Graph_cuts_in_computer_vision"	"Graph cuts in computer vision"	"Computer vision"	"As applied in the field of computer vision, graph cuts can be employed to efficiently solve a wide variety of low-level computer vision problems (early vision), such as image smoothing, the stereo correspondence problem, image segmentation, and many other computer vision problems that can be formulated in terms of energy minimization. Such energy minimization problems can be reduced to instances of the maximum flow problem in a graph (and thus, by the max-flow min-cut theorem, define a minimal cut of the graph). Under most formulations of such problems in computer vision, the minimum energy solution corresponds to the maximum a posteriori estimate of a solution. Although many computer vision algorithms involve cutting a graph (e.g., normalized cuts), the term ""graph cuts"" is applied specifically to those models which employ a max-flow/min-cut optimization (other graph cutting algorithms may be considered as graph partitioning algorithms). ""Binary"" problems (such as denoising a binary image) can be solved exactly using this approach; problems where pixels can be labeled with more than two different labels (such as stereo correspondence, or denoising of a grayscale image) cannot be solved exactly, but solutions produced are usually near the global optimum."
"http://dbpedia.org/resource/Visual_servoing"	"Visual servoing"	"Computer vision"	"Visual servoing, also known as vision-based robot control and abbreviated VS, is a technique which uses feedback information extracted from a vision sensor (visual feedback) to control the motion of a robot. One of the earliest papers that talks about visual servoing was from the SRI International Labs in 1979."
"http://dbpedia.org/resource/Connected-component_labeling"	"Connected-component labeling"	"Computer vision"	"Connected-component labeling (alternatively connected-component analysis, blob extraction, region labeling, blob discovery, or region extraction) is an algorithmic application of graph theory, where subsets of connected components are uniquely labeled based on a given heuristic. Connected-component labeling is not to be confused with segmentation. Connected-component labeling is used in computer vision to detect connected regions in binary digital images, although color images and data with higher dimensionality can also be processed. When integrated into an image recognition system or human-computer interaction interface, connected component labeling can operate on a variety of information. Blob extraction is generally performed on the resulting binary image from a thresholding step. Blobs may be counted, filtered, and tracked. Blob extraction is related to but distinct from blob detection."
"http://dbpedia.org/resource/3D_reconstruction_from_multiple_images"	"3D reconstruction from multiple images"	"Computer vision"	"3D reconstruction from multiple images is the creation of three-dimensional models from a set of images. It is the reverse process of obtaining 2D images from 3D scenes. The essence of an image is a projection from a 3D scene onto a 2D plane, during which process the depth is lost. The 3D point corresponding to a specific image point is constrained to be on the line of sight. From a single image, it is impossible to determine which point on this line corresponds to the image point. If two images are available, then the position of a 3D point can be found as the intersection of the two projection rays. This process is referred to as triangulation. The key for this process is the relations between multiple views which convey the information that corresponding sets of points must contain some structure and that this structure is related to the poses and the calibration of the camera. In recent decades, there is an important demand for 3D content for computer graphics, virtual reality and communication, triggering a change in emphasis for the requirements. Many existing systems for constructing 3D models are built around specialized hardware (e.g. stereo rigs) resulting in a high cost, which cannot satisfy the requirement of its new applications. This gap stimulates the use of digital imaging facilities (like a camera). Moore's law also tells us that more work can be done in software. An early method was proposed by Tomasi and Kanade. They used an affine factorization approach to extract 3D from images sequences. However, the assumption of orthographic projection is a significant limitation of this system."
"http://dbpedia.org/resource/Computer_vision"	"Computer vision"	"Computer vision"	"Computer vision is an interdisciplinary field that deals with how computers can be made to gain high-level understanding from digital images or videos. From the perspective of engineering, it seeks to automate tasks that the human visual system can do. Computer vision tasks include methods for acquiring, processing, analyzing and understanding digital images, and in general, deal with the extraction of high-dimensional data from the real world in order to produce numerical or symbolic information, e.g., in the forms of decisions. Understanding in this context means the transformation of visual images (the input of the retina) into descriptions of the world that can interface with other thought processes and elicit appropriate action. This image understanding can be seen as the disentangling of symbolic information from image data using models constructed with the aid of geometry, physics, statistics, and learning theory. As a scientific discipline, computer vision is concerned with the theory behind artificial systems that extract information from images. The image data can take many forms, such as video sequences, views from multiple cameras, or multi-dimensional data from a medical scanner. As a technological discipline, computer vision seeks to apply its theories and models for the construction of computer vision systems. Sub-domains of computer vision include scene reconstruction, event detection, video tracking, object recognition, object pose estimation, learning, indexing, motion estimation, and image restoration."
"http://dbpedia.org/resource/Automated_Imaging_Association"	"Automated Imaging Association"	"Computer vision"	"Automated Imaging Association (AIA) is the world's largest machine vision trade group. AIA has more than 330 members from 32 countries, including system integrators, camera, lighting and other vision components manufacturers, vision software providers, OEMs and distributors. The association's headquarters is located in Ann Arbor, Michigan."
"http://dbpedia.org/resource/Structured-light_3D_scanner"	"Structured-light 3D scanner"	"Computer vision"	"A structured-light 3D scanner is a 3D scanning device for measuring the three-dimensional shape of an object using projected light patterns and a camera system."
"http://dbpedia.org/resource/Condensation_algorithm"	"Condensation algorithm"	"Computer vision"	"The condensation algorithm (Conditional Density Propagation) is a computer vision algorithm. The principal application is to detect and track the contour of objects moving in a cluttered environment. Object tracking is one of the more basic and difficult aspects of computer vision and is generally a prerequisite to object recognition. Being able to identify which pixels in an image make up the contour of an object is a non-trivial problem. Condensation is a probabilistic algorithm that attempts to solve this problem. The algorithm itself is described in detail by Isard and Blake in a publication in the International Journal of Computer Vision in 1998. One of the most interesting facets of the algorithm is that it does not compute on every pixel of the image. Rather, pixels to process are chosen at random, and only a subset of the pixels end up being processed. Multiple hypotheses about what is moving are supported naturally by the probabilistic nature of the approach. The evaluation functions come largely from previous work in the area and include many standard statistical approaches. The original part of this work is the application of particle filter estimation techniques. The algorithm’s creation was inspired by the inability of Kalman filtering to perform object tracking well in the presence of significant background clutter. The presence of clutter tends to produce probability distributions for the object state which are multi-modal and therefore poorly modeled by the Kalman filter. The Condensation Algorithm in its most general form requires no assumptions about the probability distributions of the object or measurements."
"http://dbpedia.org/resource/Convolutional_neural_network"	"Convolutional neural network"	"Computer vision"	"In machine learning, a convolutional neural network (CNN, or ConvNet) is a type of feed-forward artificial neural network in which the connectivity pattern between its neurons is inspired by the organization of the animal visual cortex. Individual cortical neurons respond to stimuli in a restricted region of space known as the receptive field. The receptive fields of different neurons partially overlap such that they tile the visual field. The response of an individual neuron to stimuli within its receptive field can be approximated mathematically by a convolution operation. Convolutional networks were inspired by biological processes and are variations of multilayer perceptrons designed to use minimal amounts of preprocessing. They have wide applications in image and video recognition, recommender systems and natural language processing. The convolutional neural network is also known as shift invariant or space invariant artificial neural network (SIANN), which is named based on its shared weights architecture and translation invariance characteristics."
"http://dbpedia.org/resource/Phase_correlation"	"Phase correlation"	"Computer vision"	"Phase correlation is an approach to estimate the relative translative offset between two similar images (digital image correlation) or other data sets. It is commonly used in image registration and relies on a frequency-domain representation of the data, usually calculated by fast Fourier transforms. The term is applied particularly to a subset of cross-correlation techniques that isolate the phase information from the Fourier-space representation of the cross-correlogram."
"http://dbpedia.org/resource/Pose_(computer_vision)"	"Pose (computer vision)"	"Computer vision"	"In computer vision and in robotics, a typical task is to identify specific objects in an image and to determine each object's position and orientation relative to some coordinate system. This information can then be used, for example, to allow a robot to manipulate an object or to avoid moving into the object. The combination of position and orientation is referred to as the pose of an object, even though this concept is sometimes used only to describe the orientation. Exterior orientation and Translation are also used as synonyms to pose. The image data from which the pose of an object is determined can be either a single image, a stereo image pair, or an image sequence where, typically, the camera is moving with a known speed. The objects which are considered can be rather general, including a living being or body parts, e.g., a head or hands. The methods which are used for determining the pose of an object, however, are usually specific for a class of objects and cannot generally be expected to work well for other types of objects. The pose can be described by means of a rotation and translation transformation which brings the object from a reference pose to the observed pose. This rotation transformation can be represented in different ways, e.g., as a rotation matrix or a quaternion."
"http://dbpedia.org/resource/Active_contour_model"	"Active contour model"	"Computer vision"	"Active contour model, also called snakes, is a framework in computer vision for delineating an object outline from a possibly noisy 2D image. The snakes model is popular in computer vision, and snakes are greatly used in applications like object tracking, shape recognition, segmentation, edge detection and stereo matching. A snake is an energy minimizing, deformable spline influenced by constraint and image forces that pull it towards object contours and internal forces that resist deformation. Snakes may be understood as a special case of the general technique of matching a deformable model to an image by means of energy minimization. In two dimensions, the active shape model represents a discrete version of this approach, taking advantage of the point distribution model to restrict the shape range to an explicit domain learned from a training set. Snakes do not solve the entire problem of finding contours in images, since the method requires knowledge of the desired contour shape beforehand. Rather, they depend on other mechanisms such as interaction with a user, interaction with some higher level image understanding process, or information from image data adjacent in time or space."
"http://dbpedia.org/resource/Shape_context"	"Shape context"	"Computer vision"	"Shape context is a feature descriptor used in object recognition. Serge Belongie and Jitendra Malik proposed the term in their paper ""Matching with Shape Contexts"" in 2000."
"http://dbpedia.org/resource/Dynamic_Graphics_Project"	"Dynamic Graphics Project"	"Computer vision"	"The Dynamic Graphics Project (commonly referred to as dgp) is an interdisciplinary research laboratory at the University of Toronto devoted to projects involving Computer Graphics, Computer Vision, and Human Computer Interaction. The lab began as the computer graphics research group of Computer Science Professor Leslie Mezei in 1967. Mezei invited Bill Buxton, a pioneer of human–computer interaction to join. In 1972, Ronald Baecker, another HCI pioneer joined dgp, establishing dgp as the first Canadian university group focused on computer graphics and human-computer interaction. Since then, dgp has hosted many well known faculty and students in computer graphics, computer vision and HCI (e.g., Bill Reeves, Jos Stam, Demetri Terzopoulos). dgp also occasionally hosts artists in residence (e.g., Oscar-winner Chris Landreth). Many past and current researchers at Autodesk (and before that Alias Wavefront) graduated after working at dgp. dgp is located in the St. George Campus of University of Toronto in the Bahen Centre for Information Technology. dgp researchers regularly publish at ACM SIGGRAPH, ACM SIGCHI and ICCV. dgp hosts the Toronto User Experience (TUX) Speaker Series and the Sanders Series Lectures"
"http://dbpedia.org/resource/Landmark_point"	"Landmark point"	"Computer vision"	"In morphometrics, landmark point or shortly landmark is a point in a shape object in which correspondences between and within the populations of the object are preserved. In other disciplines, landmarks may be known as vertices, anchor points, control points, sites, profile points, 'sampling' points, nodes, markers, fiducial markers, etc. Landmarks can be defined either manually by experts or automatically by a computer program. There are three basic types of landmarks: anatomical landmarks, mathematical landmarks or pseudo-landmarks. An anatomical landmark is a biologically-meaningful point in an organism. Usually experts define anatomical points to ensure their correspondences within the same species. Examples of anatomical landmark in shape of a skull are the eye corner, tip of the nose, jaw, etc. Anatomical landmarks determine homologous parts of an organism, which share a common ancestry. Mathematical landmarks are points in a shape that are located according to some mathematical or geometrical property, for instance, a high curvature point or an extreme point. A computer program usually determines mathematical landmarks used for an automatic pattern recognition. Pseudo-landmarks are constructed points located between anatomical or mathematical landmarks. A typical example is an equally spaced set of points between two anatomical landmarks to get more sample points from a shape. Pseudo-landmarks are useful during shape matching, when the matching process requires a large number of points."
"http://dbpedia.org/resource/Pyramid_(image_processing)"	"Pyramid (image processing)"	"Computer vision"	"Pyramid, or pyramid representation, is a type of multi-scale signal representation developed by the computer vision, image processing and signal processing communities, in which a signal or an image is subject to repeated smoothing and subsampling. Pyramid representation is a predecessor to scale-space representation and multiresolution analysis."
"http://dbpedia.org/resource/Glossary_of_machine_vision"	"Glossary of machine vision"	"Computer vision"	"The following are common definitions related to the machine vision field. General related fields 
*  Machine vision 
*  Computer vision 
*  Image processing 
*  Signal processing"
"http://dbpedia.org/resource/3D_data_acquisition_and_object_reconstruction"	"3D data acquisition and object reconstruction"	"Computer vision"	"3D data acquisition and reconstruction is the generation of three-dimensional or spatiotemporal models from sensor data. The techniques and theories, generally speaking, work with most or all sensor types including optical, acoustic, laser scanning, radar, thermal, seismic."
"http://dbpedia.org/resource/Relaxation_labelling"	"Relaxation labelling"	"Computer vision"	"Relaxation labelling is an image treatment methodology. Its goal is to associate a label to the pixels of a given image or nodes of a given graph."
"http://dbpedia.org/resource/N-jet"	"N-jet"	"Computer vision"	"An N-jet is the set of (partial) derivatives of a function up to order . Specifically, in the area of computer vision, the N-jet is usually computed from a scale space representation of the input image , and the partial derivatives of are used as a basis for expressing various types of visual modules. For example, algorithms for tasks such as feature detection, feature classification, stereo matching, tracking and object recognition can be expressed in terms of N-jets computed at one or several scales in scale space."
"http://dbpedia.org/resource/Orientation_(computer_vision)"	"Orientation (computer vision)"	"Computer vision"	"In computer vision and image processing a common assumption is that sufficiently small image regions can be characterized as locally one-dimensional, e.g., in terms of lines or edges. For natural images this assumption is usually correct except at specific points, e.g., corners or line junctions or crossings, or in regions of high frequency textures. However, what size the regions have to be in order to appear as one-dimensional varies both between images and within an image. Also, in practice a local region is never exactly one-dimensional but can be so to a sufficient degree of approximation. Image regions which are one-dimensional are also referred to as simple or intrinsic one-dimensional (i1D). Given an image of dimension d (d = 2 for ordinary images), a mathematical representation of a local i1D image region is where is the image intensity function which varies over a local image coordinate (a d-dimensional vector), is a one-variable function, and is a unit vector. The intensity function is constant in all directions which are perpendicular to . Intuitively, the orientation of an i1D-region is therefore represented by the vector . However, for a given , is not uniquely determined. If then can be written as which implies that also is a valid representation of the local orientation. In order to avoid this ambiguity in the representation of local orientation two representations have been proposed 
*  The double angle representation 
*  The tensor representation The double angle representation is only valid for 2D images (d=2), but the tensor representation can be defined for arbitrary dimensions d of the image data."
"http://dbpedia.org/resource/Point_distribution_model"	"Point distribution model"	"Computer vision"	"The point distribution model is a model for representing the mean geometry of a shape and some statistical modes of geometric variation inferred from a training set of shapes."
"http://dbpedia.org/resource/Image_analysis"	"Image analysis"	"Computer vision"	"Image analysis is the extraction of meaningful information from images; mainly from digital images by means of digital image processing techniques. Image analysis tasks can be as simple as reading bar coded tags or as sophisticated as identifying a person from their face. Computers are indispensable for the analysis of large amounts of data, for tasks that require complex computation, or for the extraction of quantitative information. On the other hand, the human visual cortex is an excellent image analysis apparatus, especially for extracting higher-level information, and for many applications — including medicine, security, and remote sensing — human analysts still cannot be replaced by computers. For this reason, many important image analysis tools such as edge detectors and neural networks are inspired by human visual perception models."
"http://dbpedia.org/resource/Image_fusion"	"Image fusion"	"Computer vision"	"In computer vision, Multisensor Image fusion is the process of combining relevant information from two or more images into a single image. The resulting image will be more informative than any of the input images. In remote sensing applications,the increasing availability of space borne sensors gives a motivation for different image fusion algorithms.Several situations in image processing require high spatial and high spectral resolution in a single image. Most of the available equipment is not capable of providing such data convincingly. Image fusion techniques allow the integration of different information sources. The fused image can have complementary spatial and spectral resolution characteristics. However, the standard image fusion techniques can distort the spectral information of the multispectral data while merging. In satellite imaging, two types of images are available. The panchromatic image acquired by satellites is transmitted with the maximum resolution available and the multispectral data are transmitted with coarser resolution. This will usually be two or four times lower. At the receiver station, the panchromatic image is merged with the multispectral data to convey more information. Many methods exist to perform image fusion. The very basic one is the high pass filtering technique. Later techniques are based on Discrete Wavelet Transform, uniform rational filter bank, and Laplacian pyramid."
"http://dbpedia.org/resource/Neighborhood_operation"	"Neighborhood operation"	"Computer vision"	"In computer vision and image processing a neighborhood operation is a commonly used class of computations on image data which implies that it is processed according to the following pseudo code: Visit each point p in the image data and do {  N = a neighborhood or region of the image data around the point p  result(p) = f(N)} This general procedure can be applied to image data of arbitrary dimensionality. Also, the image data on which the operation is applied does not have to be defined in terms of intensity or color, it can be any type of information which is organized as a function of spatial (and possibly temporal) variables in p. The result of applying a neighborhood operation on an image is again something which can be interpreted as an image, it has the same dimension as the original data. The value at each image point, however, does not have to be directly related to intensity or color. Instead it is an element in the range of the function f, which can be of arbitrary type. Normally the neighborhood N is of fixed size and is a square (or a cube, depending on the dimensionality of the image data) centered on the point p. Also the function f is fixed, but may in some cases have parameters which can vary with p, see below. In the simplest case, the neighborhood N may be only a single point. This type of operation is often referred to as a point-wise operation."
"http://dbpedia.org/resource/Boustrophedon_cell_decomposition"	"Boustrophedon cell decomposition"	"Computer vision"	"The boustrophedon cell decomposition (BCD) is a method used in artificial intelligence and robotics for configuration space solutions. Like other cellular decomposition methods, this method transforms the configuration space into cell regions that can be used for path planning. A strength of the boustrophedon cell decomposition is that it allows for more diverse, non-polygonal obstacles within a configuration space. The representation still depicts polygonal obstacles, but the representations are complex enough that they are very effective when describing things like rounded surfaces, jagged edges, etc. It is a goal of the method to optimize a path that can be chosen by an intelligent system. While a BCD can represent the existence of objects in a physical space, it does very little to nothing in terms of recognizing the objects. This would be done using another method, one which most likely requires additional sensory data in order to be used."
"http://dbpedia.org/resource/Image_registration"	"Image registration"	"Computer vision"	"Image registration is the process of transforming different sets of data into one coordinate system. Data may be multiple photographs, data from different sensors, times, depths, or viewpoints. It is used in computer vision, medical imaging, military automatic target recognition, and compiling and analyzing images and data from satellites. Registration is necessary in order to be able to compare or integrate the data obtained from these different measurements."
"http://dbpedia.org/resource/Alignments_of_random_points"	"Alignments of random points"	"Computer vision"	"Alignments of random points in the plane can be demonstrated by statistics to be remarkably and counter-intuitively easy to find when a large number of random points are marked on a bounded flat surface. This has been put forward as a demonstration that ley lines and other similar mysterious alignments believed by some to be phenomena of deep significance might exist solely due to chance alone, as opposed to the supernatural or anthropological explanations put forward by their proponents. The topic has also been studied in the fields of computer vision and astronomy. A number of studies have examined the mathematics of alignment of random points on the plane. In all of these, the width of the line - the allowed displacement of the positions of the points from a perfect straight line - is important. It allows the fact that real-world features are not mathematical points, and that their positions need not line up exactly for them to be considered in alignment. Alfred Watkins, in his classic work on ley lines The Old Straight Track, used the width of a pencil line on a map as the threshold for the tolerance of what might be regarded as an alignment. For example, using a 1 mm pencil line to draw alignments on an 1:50,000 Ordnance Survey map, the corresponding width on the ground would be 50 m."
"http://dbpedia.org/resource/Image_moment"	"Image moment"	"Computer vision"	"In image processing, computer vision and related fields, an image moment is a certain particular weighted average (moment) of the image pixels' intensities, or a function of such moments, usually chosen to have some attractive property or interpretation. Image moments are useful to describe objects after segmentation.  which are found via image moments include area (or total intensity), its centroid, and ."
"http://dbpedia.org/resource/Spatial_verification"	"Spatial verification"	"Computer vision"	"The spatial verification consists in verify a spatial correlation between certain points of a pair of images. The main problem is that outliers (that does not fit or does not match the selected model) affect adjustment called least squares (numerical analysis technique framed in mathematical optimization, which, given an set of ordered pairs: independent variable, dependent variable, and a family of functions, try to find the continuous function)."
"http://dbpedia.org/resource/Background_subtraction"	"Background subtraction"	"Computer vision"	"Background subtraction, also known as Foreground Detection, is a technique in the fields of image processing and computer vision wherein an image's foreground is extracted for further processing (object recognition etc.). Generally an image's regions of interest are objects (humans, cars, text etc.) in its foreground. After the stage of image preprocessing (which may include image denoising, post processing like morphology etc.) object localisation is required which may make use of this technique. Background subtraction is a widely used approach for detecting moving objects in videos from static cameras. The rationale in the approach is that of detecting the moving objects from the difference between the current frame and a reference frame, often called “background image”, or “background model”. Background subtraction is mostly done if the image in question is a part of a video stream. Background subtraction provides important cues for numerous applications in computer vision, for example surveillance tracking or human poses estimation. However, background subtraction is generally based on a static background hypothesis which is often not applicable in real environments. With indoor scenes, reflections or animated images on screens lead to background changes. In a same way, due to wind, rain or illumination changes brought by weather, static backgrounds methods have difficulties with outdoor scenes."
"http://dbpedia.org/resource/Scale_space"	"Scale space"	"Computer vision"	"Scale-space theory is a framework for multi-scale signal representation developed by the computer vision, image processing and signal processing communities with complementary motivations from physics and biological vision. It is a formal theory for handling image structures at different scales, by representing an image as a one-parameter family of smoothed images, the scale-space representation, parametrized by the size of the smoothing kernel used for suppressing fine-scale structures. The parameter in this family is referred to as the scale parameter, with the interpretation that image structures of spatial size smaller than about have largely been smoothed away in the scale-space level at scale . The main type of scale space is the linear (Gaussian) scale space, which has wide applicability as well as the attractive property of being possible to derive from a small set of scale-space axioms. The corresponding scale-space framework encompasses a theory for Gaussian derivative operators, which can be used as a basis for expressing a large class of visual operations for computerized systems that process visual information. This framework also allows visual operations to be made scale invariant, which is necessary for dealing with the size variations that may occur in image data, because real-world objects may be of different sizes and in addition the distance between the object and the camera may be unknown and may vary depending on the circumstances."
"http://dbpedia.org/resource/Phase_congruency"	"Phase congruency"	"Computer vision"	"Phase congruency is a measure of feature significance in computer images, a method of edge detection that is particularly robust against changes in illumination and contrast."
"http://dbpedia.org/resource/CAPTCHA"	"CAPTCHA"	"Computer vision"	"A CAPTCHA (a backronym for ""Completely Automated Public Turing test to tell Computers and Humans Apart"") is a type of challenge-response test used in computing to determine whether or not the user is human. The term was coined in 2003 by Luis von Ahn, Manuel Blum, Nicholas J. Hopper, and John Langford. The most common type of CAPTCHA was first invented in 1997 by two groups working in parallel: (1) Mark D. Lillibridge, Martin Abadi, Krishna Bharat, and Andrei Z. Broder; and (2) Reshef, Raanan and Solan. This form of CAPTCHA requires that the user type the letters of a distorted image, sometimes with the addition of an obscured sequence of letters or digits that appears on the screen. Because the test is administered by a computer, in contrast to the standard Turing test that is administered by a human, a CAPTCHA is sometimes described as a reverse Turing test. This user identification procedure has received many criticisms, especially from disabled people, but also from other people who feel that their everyday work is slowed down by distorted words that are difficult to read. It takes the average person approximately 10 seconds to solve a typical CAPTCHA."
"http://dbpedia.org/resource/Cuboid_(computer_vision)"	"Cuboid (computer vision)"	"Computer vision"	"In computer vision, the term cuboid is used to describe a small spatiotemporal volume extracted for purposes of behavior recognition."
"http://dbpedia.org/resource/Tango_(platform)"	"Tango (platform)"	"Computer vision"	"Tango (formerly named Project Tango in-testing) is a technology platform developed and authored by Google that uses computer vision to enable mobile devices, such as smartphones and tablets, to detect their position relative to the world around them without using GPS or other external signals. This allows application developers to create user experiences that include indoor navigation, 3D mapping, physical space measurement, environmental recognition, augmented reality, and windows into a virtual world. The first product to emerge from ATAP's Skunkworks group, Tango was developed by a team led by computer scientist Johnny Lee, a core contributor to Microsoft's Kinect. In an interview in June 2015, Lee said, ""We're developing the hardware and software technologies to help everything and everyone understand precisely where they are, anywhere."" Google has produced two devices to demonstrate the Tango technology: the discontinued Peanut phone and the Yellowstone 7-inch tablet. More than 3,000 of these devices had been sold as of June 2015, chiefly to researchers and software developers interested in building applications for the platform. In the summer of 2015, Qualcomm and Intel both announced that they are developing Tango reference devices as models for device manufacturers who use their mobile chipsets. At CES, in January 2016, Google announced a partnership with Lenovo to release a consumer smartphone during the summer of 2016 to feature Tango technology marketed at consumers, noting a less than $500 price-point and a small form factor below 6.5 inches. At the same time, both companies also announced an application incubator to get applications developed to be on the device on launch. At Lenovo Tech World 2016, Lenovo launched the world's first consumer phone based on Tango, as well as releasing it as ""Tango""."
"http://dbpedia.org/resource/OrCam_device"	"OrCam device"	"Computer vision"	"The OrCam MyEye is a portable, artificial vision device that allows the visually impaired to understand text and identify objects. The device was developed by OrCam Technologies Limited, and was released as a prototype in September 2013. OrCam's next product, OrCam MyMe, will be coming soon. OrCam MyMe is the first wearable AI that will monitor the activities that make up your day-to-day routine and interact with your smartphone or smart watch."
"http://dbpedia.org/resource/Statistical_shape_analysis"	"Statistical shape analysis"	"Computer vision"	"Statistical shape analysis is an analysis of the geometrical properties of some given set of shapes by statistical methods. For instance, it could be used to quantify differences between male and female Gorilla skull shapes, normal and pathological bone shapes, leaf outlines with and without herbivory by insects, etc. Important aspects of shape analysis are to obtain a measure of distance between shapes, to estimate mean shapes from (possibly random) samples, to estimate shape variability within samples, to perform clustering and to test for differences between shapes. One of the main methods used is principal component analysis(PCA). Statistical shape analysis has applications in various fields, including medical imaging, computer vision, computational anatomy, sensor measurement, and geographical profiling."
"http://dbpedia.org/resource/Color_histogram"	"Color histogram"	"Computer vision"	"In image processing and photography, a color histogram is a representation of the distribution of colors in an image. For digital images, a color histogram represents the number of pixels that have colors in each of a fixed list of color ranges, that span the image's color space, the set of all possible colors. The color histogram can be built for any kind of color space, although the term is more often used for three-dimensional spaces like RGB or HSV. For monochromatic images, the term intensity histogram may be used instead. For multi-spectral images, where each pixel is represented by an arbitrary number of measurements (for example, beyond the three measurements in RGB), the color histogram is N-dimensional, with N being the number of measurements taken. Each measurement has its own wavelength range of the light spectrum, some of which may be outside the visible spectrum. If the set of possible color values is sufficiently small, each of those colors may be placed on a range by itself; then the histogram is merely the count of pixels that have each possible color. Most often, the space is divided into an appropriate number of ranges, often arranged as a regular grid, each containing many similar color values. The color histogram may also be represented and displayed as a smooth function defined over the color space that approximates the pixel counts. Like other kinds of histograms, the color histogram is a statistic that can be viewed as an approximation of an underlying continuous distribution of colors values."
"http://dbpedia.org/resource/Geometric_hashing"	"Geometric hashing"	"Computer vision"	"In computer science, geometric hashing is originally a method for efficiently finding two-dimensional objects represented by discrete points that have undergone an affine transformation (example below is based on similarity transformation), though extensions exist to some other object representations and transformations. In an off-line step, the objects are encoded by treating each pairs of points as a geometric basis. The remaining points can be represented in an invariant fashion with respect to this basis using two parameters. For each point, its quantized transformed coordinates are stored in the hash table as a key, and indices of the basis points as a value. Then a new pair of basis points is selected, and the process is repeated. In the on-line (recognition) step, randomly selected pairs of data points are considered as candidate bases. For each candidate basis, the remaining data points are encoded according to the basis and possible correspondences from the object are found in the previously constructed table. The candidate basis is accepted if a sufficiently large number of the data points index a consistent object basis. Geometric hashing was originally suggested in computer vision for object recognition in 2D and 3D, but later was applied to different problems such as structural alignment of proteins."
"http://dbpedia.org/resource/Outline_of_computer_vision"	"Outline of computer vision"	"Computer vision"	"The following outline is provided as an overview of and topical guide to computer vision: Computer vision – interdisciplinary field that deals with how computers can be made to gain high-level understanding from digital images or videos. From the perspective of engineering, it seeks to automate tasks that the human visual system can do. Computer vision tasks include methods for acquiring digital images (through image sensors), image processing, and image analysis, to reach an understanding of digital images. In general, it deals with the extraction of high-dimensional data from the real world in order to produce numerical or symbolic information that the computer can interpret. The image data can take many forms, such as video sequences, views from multiple cameras, or multi-dimensional data from a medical scanner. As a technological discipline, computer vision seeks to apply its theories and models for the construction of computer vision systems. As a scientific discipline, computer vision is concerned with the theory behind artificial systems that extract information from images."
"http://dbpedia.org/resource/Photometric_stereo"	"Photometric stereo"	"Computer vision"	"Photometric stereo is a technique in computer vision for estimating the surface normals of objects by observing that object under different lighting conditions. It is based on the fact that the amount of light reflected by a surface is dependent on the orientation of the surface in relation to the light source and the observer. By measuring the amount of light reflected into a camera, the space of possible surface orientations is limited. Given enough light sources from different angles, the surface orientation may be constrained to a single orientation or even overconstrained. The technique was originally introduced by Woodham in 1980. The special case where the data is a single image is known as shape from shading, and was analyzed by B. K. P. Horn in 1989. Photometric stereo has since been generalized to many other situations, including extended light sources and non-Lambertian surface finishes. Current research aims to make the method work in the presence of projected shadows, highlights, and non-uniform lighting."
"http://dbpedia.org/resource/Contextual_image_classification"	"Contextual image classification"	"Computer vision"	"Contextual image classification, a topic of pattern recognition in computer vision, is an approach of classification based on contextual information in images. ""Contextual"" means this approach is focusing on the relationship of the nearby pixels, which is also called neighbourhood. The goal of this approach is to classify the images by using the contextual information."
"http://dbpedia.org/resource/Active_vision"	"Active vision"	"Computer vision"	"An area of computer vision is active vision, sometimes also called active computer vision. An active vision system is one that can manipulate the viewpoint of the camera(s) in order to investigate the environment and get better information from it."
"http://dbpedia.org/resource/EigenMoments"	"EigenMoments"	"Computer vision"	"EigenMoments is a set of orthogonal, noise robust, invariant to rotation, scaling and translation and distribution sensitive moments. Their application can be found in signal processing and computer vision as descriptors of the signal or image. The descriptors can later be used for classification purposes. It is obtained by performing orthogonalization, via eigen analysis on geometric moments."
"http://dbpedia.org/resource/PatchMatch"	"PatchMatch"	"Computer vision"	"The core PatchMatch algorithm quickly finds correspondences between small square regions (or patches) of an image. The algorithm can be used in various applications such as object removal from images, reshuffling or moving contents of images, or retargeting or changing aspect ratios of images."
"http://dbpedia.org/resource/Point_set_registration"	"Point set registration"	"Computer vision"	"In computer vision and pattern recognition, point set registration, also known as point matching, is the process of finding a spatial transformation that aligns two point sets. The purpose of finding such a transformation includes merging multiple data sets into a globally consistent model, and mapping a new measurement to a known data set to identify features or to estimate its pose. A point set may be raw data from 3D scanning or an array of rangefinders. For use in image processing and feature-based image registration, a point set may be a set of features obtained by feature extraction from an image, for example corner detection. Point set registration is used in optical character recognitionand aligning data from magnetic resonance imaging with computer aided tomography scans."
"http://dbpedia.org/resource/Joint_compatibility_branch_and_bound"	"Joint compatibility branch and bound"	"Computer vision"	"Joint compatibility branch and bound (JCBB) is an algorithm in computer vision and robotics commonly used for data association in simultaneous localization and mapping. JCBB measures the joint compatibility of a set of pairings that successfully rejects spurious matchings and is hence known to robust in complex environments."
"http://dbpedia.org/resource/Photo-consistency"	"Photo-consistency"	"Computer vision"	"In computer vision Photo-consistency determines whether a given voxel is occupied. A voxel is considered to be photo consistent when its color appears to be similar to all the cameras that can see it. Most voxel coloring or space carving techniques require using photo consistency as a check condition in Image-based modeling and rendering applications."
"http://dbpedia.org/resource/Local_ternary_patterns"	"Local ternary patterns"	"Computer vision"	"Local ternary patterns (LTP) are an extension of Local binary patterns (LBP). Unlike LBP, it does not threshold the pixels into 0 and 1, rather it uses a threshold constant to threshold pixels into three values. Considering k as the threshold constant, c as the value of the center pixel, a neighboring pixel p, the result of threshold is: In this way, each thresholded pixel has one of the three values. Neighboring pixels are combined after thresholding into a ternary pattern. Computing a histogram of these ternary values will result in a large range, so the ternary pattern is split into two binary patterns. Histograms are concatenated to generate a descriptor double the size of LBP."
"http://dbpedia.org/resource/Texton"	"Texton"	"Computer vision"	"The term texton was introduced by Bela Julesz in 1981 to describe ""the putative units of pre-attentive human texture perception.""The term reemerged in the late 1990s and early 2000s to describe vector quantized responses of a linear filter bank."
"http://dbpedia.org/resource/Mean_shift"	"Mean shift"	"Computer vision"	"Mean shift is a non-parametric feature-space analysis technique for locating the maxima of a density function, a so-called mode-seeking algorithm. Application domains include cluster analysis in computer vision and image processing."
"http://dbpedia.org/resource/Randomized_Hough_transform"	"Randomized Hough transform"	"Computer vision"	"Hough transforms are techniques for object detection, a critical step in many implementations of computer vision, or data mining from images. Specifically, the Randomized Hough transform is a probabilistic variant to the classical Hough transform, and is commonly used to detect curves (straight line, circle, ellipse, etc.) The basic idea of Hough transform (HT) is to implement a voting procedure for all potential curves in the image, and at the termination of the algorithm, curves that do exist in the image will have relatively high voting scores. Randomized Hough transform (RHT) is different from HT in that it tries to avoid conducting the computationally expensive voting process for every nonzero pixel in the image by taking advantage of the geometric properties of analytical curves, and thus improve the time efficiency and reduce the storage requirement of the original algorithm."
"http://dbpedia.org/resource/Image_formation"	"Image formation"	"Computer vision"	"The study of image formation encompasses the radiometric and geometric processes by which 2D images of 3D objects are formed. In the case of digital images, the image formation process also includes analog to digital conversion and sampling. Image Formation in Eye The principal difference between the lens of the eye and an ordinary optical lens is that the former is flexible. The radius of the curvature of the anterior surface of the lens is greater than the radius of its posterior surface. The shape of the lens is controlled by tension in the fibers of the ciliary body. To focus on distant objects, the controlling muscles cause the lens to be relatively flattened. Similarly, these muscles allow the lens to become thicker in order to focus on objects near the eye. The distance between the center of the lens and the retina (focal length) varies from approximately 17 mm to about 14 mm, as the refractive power of the lens increases from its minimum to its maximum. When the eye focuses on an object farther away than about 3 m, the lens exhibits its lowest refractivee power. When the eye focuses on a nearly object, the lens is most strongly refractive."
"http://dbpedia.org/resource/3D_pose_estimation"	"3D pose estimation"	"Computer vision"	"3D pose estimation is the problem of determining the transformation of an object in a 2D image which gives the 3D object. The need for 3D pose estimation arises from the limitations of feature based pose estimation. There exist environments where it is difficult to extract corners or edges from an image. To circumvent these issues, the object is dealt with as a whole through the use of free-form contours."
"http://dbpedia.org/resource/Color_normalization"	"Color normalization"	"Computer vision"	"Color normalization is a topic in computer vision concerned with artificial color vision and object recognition. In general, the distribution of color values in an image depends on the illumination, which may vary depending on lighting conditions, cameras, and other factors. Colour normalisation allows for object recognition techniques based on colour to compensate for these variations."
"http://dbpedia.org/resource/Ortho3D"	"Ortho3D"	"Computer vision"	"Ortho3D True 3D city reconstruction created automatically from multi-angle stereoscopic vertical and oblique aerial and/or street photographs, thanks to very robust photogrammetric dense matching software based on strong GPU computing. Workflow : 
*  image matching, keypoints extraction 
*  bundle adjustment 
*  dense matching, point cloud extraction 
*  point cloud cleaning 
*  3D Meshing"
"http://dbpedia.org/resource/Underwater_computer_vision"	"Underwater computer vision"	"Computer vision"	"Underwater computer vision is a subfield of computer vision. In recent years, with the development of underwater vehicles ( ROV, AUV, gliders), the need to be able to record and process huge amount of information has become increasingly important. Applications range from inspection of underwater structures for the offshore industry to the identification and counting of fishes for biological research. However, no matter how big the impact of this technology can be to industry and research, it still is in a very early stage of development compared to traditional computer vision. One reason for this is that, the moment the camera goes into the water, a whole new set of challenges appear. On one hand, cameras have to be made waterproof, marine corrosion deteriorates materials quickly and access and modifications to experimental setups are costly, both in time and resources. On the other hand, the physical properties of the water make light behave differently, changing the appearance of a same object with variations of depth, organic material, currents, temperature etc."
"http://dbpedia.org/resource/M-Theory_(learning_framework)"	"M-Theory (learning framework)"	"Computer vision"	"In Machine Learning and Computer Vision, M-Theory is a learning framework inspired by feed-forward processing in the ventral stream of visual cortex and originally developed for recognition and classification of objects in visual scenes. M-Theory was later applied to other areas, such as speech recognition. On certain image recognition tasks, algorithms based on a specific instantiation of M-Theory, HMAX, achieved human-level performance. The core principle of M-Theory is extracting representations invariant to various transformations of images (translation, scale, 2D and 3D rotation and others). In contrast with other approaches using invariant representations, in M-Theory they are not hardcoded into the algorithms, but learned. M-Theory also shares some principles with Compressed Sensing. The theory proposes multilayered hierarchical learning architecture, similar to that of visual cortex."
"http://dbpedia.org/resource/Poisson_image_editing"	"Poisson image editing"	"Computer vision"	"Poisson image editing refers to set of image processing routines that permit one to clone, adjust illumination, fill holes and tile texture patches without visible seams."
"http://dbpedia.org/resource/Cyclodisparity"	"Cyclodisparity"	"Computer vision"	"Cyclodisparity refers to the difference in the rotation angle of an object or scene viewed by the left and right eyes. Cyclodisparity can result from the eyes' torsional rotation (cyclorotation) or can be created artificially by presenting to the eyes two images that need to be rotated relative to each other for binocular fusion to take place."
"http://dbpedia.org/resource/Extensible_Device_Metadata"	"Extensible Device Metadata"	"Computer vision"	"The Extensible Device Metadata (XDM) specification is an open file format for embedding device-related metadata in JPEG and other common image files without breaking compatibility with ordinary image viewers. The metadata types include: depth map, camera pose, point cloud, lens model, image reliability data, and identifying info about the hardware components. This metadata can be used, for instance, to create depth effects such as a bokeh filter, recreate the exact location and position of the camera when the picture was taken, or create 3D data models of environments or objects. The format uses XML and is based on the XMP standard. It can support multiple ""cameras"" (image sources and types) in a single image file, and each can include data about its position and orientation relative to the primary camera. A camera data structure may include an image, depth map, etc. The XDM 1.0 documentation uses JPEG as the basic model, but states that the concepts generally apply to other image-file types supported by XMP, including PNG, TIFF, and GIF. The XDM specification is developed and maintained by a working group that includes engineers from Intel and Google. The version 1.01 specification is posted at the website xdm.org; an earlier 1.0 version was posted at the Intel website in late 2015. XDM builds upon the Depthmap Metadata specification, introduced in 2014 and used in commercial applications including Google Lens Blur and Intel RealSense Depth Enabled Photography (DEP). That original specification was designed only for depth-photography use cases. Due to changes and expansions of the data structure, and the use of different namespaces, the two standards are not compatible. Existing applications that used that older standard will not work with XDM without modifications."
"http://dbpedia.org/resource/Video_Sequences_Saliency_Map"	"Video Sequences Saliency Map"	"Computer vision"	"In computer vision, a saliency map is an image that shows each pixel's unique quality. And saliency is an algorithm to compute the unique quality of each pixel and then showing each pixel's unique quality in a new image, this image is saliency map. The goal of a saliency map is simplified and/or change the representation of an image into something that is more meaningful and easier to analyze. Like a pixel has a high grey level or other unique color quality sees in [color image], each pixel's quality will show in the saliency map and in an obvious way. Saliency is a kind of Image segmentation. The result of saliency map is set of contours extracted from the image (see edge detection). Each of the pixels in a region is similar with respect to some characteristic or computed property, such as color, intensity, or texture. There are three major map calculation ways that obey linear computational complexity with respect to a number of image pixels. The first saliency function is using each pixel value minus the rest of this image's single pixel's value and then sum those value together. Saliency sum is the distance of a pixel to the rest of pixels in an input image, and the value of the sum is every pixel's saliency value of a saliency map. And this algorithm is using the grey value of each pixel. The second saliency function is very close to the previous one, and the only difference is the saliency distance calculation way. In the previous function, we use a pixel value minus same frame's pixel value. However, in this function, we get two input images from a video, then use a pixel value in present image minus the rest pixel value in the previous image, then sum all of this value. This value is the saliency distance of the second function. This function is exactly using previous frame's same coordinate pixels as compared pixels. The third saliency function uses the calculation result of the first saliency function. After we get the result of the first function, we use current frame's saliency result minus the previous saliency result to get a new saliency map."
"http://dbpedia.org/resource/DATR"	"DATR"	"Natural language processing"	"DATR is a language for lexical knowledge representation. The lexical knowledge is encoded in a network of nodes. Each node has a set of attributes encoded with it. A node can represent a word or a word form. DATR was developed in the late 1980s by Roger Evans, Gerald Gazdar and Bill Keller, and used extensively in the 1990s; the standard specification is contained in the Evans and Gazdar RFC, available on the Sussex website (below). DATR has been implemented in a variety of programming languages, and several implementations are available on the internet, including an RFC compliant implementation at the Bielefeld website (below). DATR is still used for encoding inheritance networks in various linguistic and non-linguistic domains and is under discussion as a standard notation for the representation of lexical information."
"http://dbpedia.org/resource/Brill_tagger"	"Brill tagger"	"Natural language processing"	"The Brill tagger is an inductive method for part-of-speech tagging. It was described and invented by Eric Brill in his 1995 PhD thesis. It can be summarized as an ""error-driven transformation-based tagger"". It is 
*  a form of supervised learning, which aims to minimize error 
*  transformation-based in the sense that a tag is assigned to each word and changed using a set of predefined rules. Note: If the word is known, it first assigns the most frequent tag, or if the word is unknown, it naively assigns the tag ""noun"" to it. Applying over and over these rules, changing the incorrect tags, a quite high accuracy is achieved. This approach ensures that valuable information such as morphosyntact construct of words are employed in an automatic tagging process."
"http://dbpedia.org/resource/CLAWS_(linguistics)"	"CLAWS (linguistics)"	"Natural language processing"	"In Natural Language Processing, CLAWS is a program that performs Part-of-speech tagging."
"http://dbpedia.org/resource/Minimal_recursion_semantics"	"Minimal recursion semantics"	"Natural language processing"	"Minimal recursion semantics (MRS) is a framework for computational semantics. It can be implemented in typed feature structure formalisms such as head-driven phrase structure grammar and lexical functional grammar. It is suitable for computational language parsing and natural language generation. MRS enables a simple formulation of the grammatical constraints on lexical and phrasal semantics, including the principles of semantic composition. This technique is used in machine translation. Early pioneers of MRS include Ann Copestake, Dan Flickinger, Carl Pollard, and Ivan Sag."
"http://dbpedia.org/resource/Spell_checker"	"Spell checker"	"Natural language processing"	"In computing, a spell checker (or spell check) is an application program that flags words in a document that may not be spelled correctly. Spell checkers may be stand-alone, capable of operating on a block of text, or as part of a larger application, such as a word processor, email client, electronic dictionary, or search engine."
"http://dbpedia.org/resource/Bigram"	"Bigram"	"Natural language processing"	"A bigram or digram is a sequence of two adjacent elements from a string of tokens, which are typically letters, syllables, or words. A bigram is an n-gram for n=2. The frequency distribution of every bigram in a string is commonly used for simple statistical analysis of text in many applications, including in computational linguistics, cryptography, speech recognition, and so on. Gappy bigrams or skipping bigrams are word pairs which allow gaps (perhaps avoiding connecting words, or allowing some simulation of dependencies, as in a dependency grammar). Head word bigrams are gappy bigrams with an explicit dependency relationship. Bigrams help provide the conditional probability of a token given the preceding token, when the relation of the conditional probability is applied: That is, the probability of a token given the preceding token is equal to the probability of their bigram, or the co-occurrence of the two tokens , divided by the probability of the preceding token."
"http://dbpedia.org/resource/Grammar_checker"	"Grammar checker"	"Natural language processing"	"A grammar checker, in computing terms, is a program, or part of a program, that attempts to verify written text for grammatical correctness. Grammar checkers are most often implemented as a feature of a larger program, such as a word processor, but are also available as a stand-alone application that can be activated from within programs that work with editable text. The implementation of a grammar checker makes use of natural language processing"
"http://dbpedia.org/resource/Syntax_guessing"	"Syntax guessing"	"Natural language processing"	"Syntax guessing, also known as guess-the-verb, guess-the-noun and the syntax quest, is a problem sometimes encountered in text-based video games, such as interactive fiction games and MUDs. For various reasons - including a limited vocabulary, or a simple VERB-NOUN parser - the command syntax necessary to carry out an action may be obscure, such as with a button where the player must type POKE BUTTON, while PUSH BUTTON does not work (a ""guess-the-verb"" problem), an item described as a pillow that can only be obtained by typing GET CUSHION rather than GET PILLOW (a ""guess-the-noun"" problem), or a situation where the command TURN THE DIAL works but TURN DIAL does not (a generalized syntax guessing problem). If syntax guessing is necessary at a critical step, the game may appear unwinnable, with the player stuck until the right phrasing is guessed or is supplied by a walkthrough. A quest that requires syntax guessing to complete is a ""syntax quest"", especially if it consists of little to no content other than syntax guessing. People whose native language is not English are particularly affected by syntax guessing. TADS games supply a fair list of verbs commonly used in their documentation; if the game designer uses new actions not covered in this list, and which cannot easily be inferred by the purpose and context of the object, the player will probably encounter this problem. A similar problem can occur when attempting to accomplish a game goal using a certain combination of actions, locations and objects, which may appear rational and legitimate to the player, and are accepted by the game, only to be incorrectly handled and resulting in a gameplay error or game crash."
"http://dbpedia.org/resource/Powerset_(company)"	"Powerset (company)"	"Natural language processing"	"Powerset was an American company based in San Francisco, California, that, in 2006, was developing a natural language search engine for the Internet. On July 1, 2008, Powerset was acquired by Microsoft for an estimated $100 million. Powerset was working on building a natural language search engine that could find targeted answers to user questions (as opposed to keyword based search). For example, when confronted with a question like ""Which U.S. state has the highest income tax?"", conventional search engines ignore the question phrasing and instead do a search on the keywords ""state"", ""highest"", ""income"", and ""tax"". Powerset on the other hand, attempts to use natural language processing to understand the nature of the question and return pages containing the answer. The company was in the process of ""building a natural language search engine that reads and understands every sentence on the Web"". The company has licensed natural language technology from PARC, the former Xerox Palo Alto Research Center. On May 11, 2008, the company unveiled a tool for searching a fixed subset of Wikipedia using conversational phrases rather than keywords."
"http://dbpedia.org/resource/Morphological_pattern"	"Morphological pattern"	"Natural language processing"	"A morphological pattern is a set of associations and/or operations that build the various forms of a lexeme, possibly by inflection, agglutination, compounding or derivation."
"http://dbpedia.org/resource/Filtered-popping_recursive_transition_network"	"Filtered-popping recursive transition network"	"Natural language processing"	"A filtered-popping recursive transition network (FPRTN), or simply filtered-popping network (FPN), is a recursive transition network (RTN) extended with a map of states to keys where returning from a subroutine jump requires the acceptor and return states to be mapped to the same key. RTNs are finite-state machines that can be seen as finite-state automata extended with a stack of return states; as well as consuming transitions and -transitions, RTNs may define call transitions. These transitions perform a subroutine jump by pushing the transition's target state onto the stack and bringing the machine to the called state. Each time an acceptor state is reached, the return state at the top of the stack is popped out, provided that the stack is not empty, and the machine is brought to this state. Throughout this article we refer to filtered-popping recursive transition networks as FPNs, though this acronym is ambiguous (e.g.: fuzzy Petri nets). Filtered-popping networks and FPRTNs are unambiguous alternatives."
"http://dbpedia.org/resource/Naive_semantics"	"Naive semantics"	"Natural language processing"	"Naive semantics is an approach used in computer science for representing basic knowledge about a specific domain, and has been used in applications such as the representation of the meaning of natural language sentences in artificial intelligence applications. In a general setting the term has been used to refer to the use of a limited store of generally understood knowledge about a specific domain in the world, and has been applied to fields such as the knowledge based design of data schemas. In natural language understanding, naive semantics involves the use of a lexical theory which maps each word sense to a simple theory (or set of assertions) about the objects or events of reference. In this sense, naive semantic theory is based upon a particular language, its syntax and its word senses. For instance the word ""water"" and the assertion water(X) may be associated with the three predicates clear(X), liquid(X) and tasteless(X)."
"http://dbpedia.org/resource/Tehran_Monolingual_Corpus"	"Tehran Monolingual Corpus"	"Natural language processing"	"The Tehran Monolingual Corpus (TMC) is a large-scale Persian monolingual corpus. TMC is suited for Language Modeling and relevant research areas in Natural Language Processing. The corpus is extracted from Hamshahri Corpus and ISNA news agency website. The quality of Hamshahri corpus is improved for language modeling purpose by a series of tokenization and spell-checking steps. TMC comprises more than 250 million words. The total number of unique words (with frequency of two or more) of the corpus is about 300 thousand, which is relatively good for a highly-inflectional language like Persian. TMC is created by Natural Language Processing Lab of University of Tehran. The corpus is free for research use, after obtaining permission from the corpus aggregator."
"http://dbpedia.org/resource/William_Aaron_Woods"	"William Aaron Woods"	"Natural language processing"	"Woods received a Bachelor's degree from Ohio Wesleyan University (1964) and a Master's (1965) and Ph.D. (1968) in Applied Mathematics from Harvard University, where he then served as an Assistant Professor and later as a Gordon McKay Professor of the Practice of Computer Science."
"http://dbpedia.org/resource/Terminology_extraction"	"Terminology extraction"	"Natural language processing"	"Terminology mining, term extraction, term recognition, or glossary extraction, is a subtask of information extraction. The goal of terminology extraction is to automatically extract relevant terms from a given corpus. In the semantic web era, a growing number of communities and networked enterprises started to access and interoperate through the internet. Modeling these communities and their information needs is important for several web applications, like topic-driven web crawlers, web services, recommender systems, etc. The development of terminology extraction is essential to the language industry. One of the first steps to model the knowledge domain of a virtual community is to collect a vocabulary of domain-relevant terms, constituting the linguistic surface manifestation of domain concepts. Several methods to automatically extract technical terms from domain-specific document warehouses have been described in the literature. Typically, approaches to automatic term extraction make use of linguistic processors (part of speech tagging, phrase chunking) to extract terminological candidates, i.e. syntactically plausible terminological noun phrases, NPs (e.g. compounds ""credit card"", adjective-NPs ""local tourist information office"", and prepositional-NPs ""board of directors"" - in English, the first two constructs are the most frequent). Terminological entries are then filtered from the candidate list using statistical and machine learning methods. Once filtered, because of their low ambiguity and high specificity, these terms are particularly useful for conceptualizing a knowledge domain or for supporting the creation of a domain ontology or a terminology base. Furthermore, terminology extraction is a very useful starting point for semantic similarity, knowledge management, human translation and machine translation, etc."
"http://dbpedia.org/resource/History_of_machine_translation"	"History of machine translation"	"Natural language processing"	"Machine translation is a sub-field of computational linguistics that investigates the use of software to translate text or speech from one natural language to another. In the 1950s Machine translation became a reality in research, although references to subject can be found as early as the 17th century. The Georgetown experiment, which involved successful fully automatic translation of more than sixty Russian sentences into English in 1954, was one of the earliest recorded projects. Researchers of the Georgetown experiment asserted their belief that machine translation would be a solved problem within three to five years. In the Soviet Union, similar experiments were performed shortly after. Consequently, the success of the experiment ushered in an era of significant funding for machine translation research in the United States. The achieved progress was much slower than expected; in 1966, the ALPAC report found that ten years of research had not fulfilled the expectations of the Georgetown experiment and resulted in dramatically reduced funding. Interest grew in statistical models for machine translation, which became more common and also less expensive in the 1980s as available computational power increased. Although there exists no autonomous system of ""fully automatic high quality translation of unrestricted text,"" there are many programs now available that are capable of providing useful output within strict constraints. Several of these programs are available online, such as Google Translate and the SYSTRAN system that powers AltaVista's BabelFish (now Yahoo's Babelfish as of 9 May 2008)."
"http://dbpedia.org/resource/W-shingling"	"W-shingling"	"Natural language processing"	"In natural language processing a w-shingling is a set of unique ""shingles"" (n-grams, contiguous subsequences of tokens in a document) that can be used to gauge the similarity of two documents. The w denotes the number of tokens in each shingle in the set. The document, ""a rose is a rose is a rose"" can be tokenized as follows: (a,rose,is,a,rose,is,a,rose) The set of all contiguous sequences of 4 tokens (4-grams) is { (a,rose,is,a), (rose,is,a,rose), (is,a,rose,is), (a,rose,is,a), (rose,is,a,rose) } = { (a,rose,is,a), (rose,is,a,rose), (is,a,rose,is) }"
"http://dbpedia.org/resource/Studies_in_Natural_Language_Processing"	"Studies in Natural Language Processing"	"Natural language processing"	"Studies in Natural Language Processing is the book series of theAssociation for Computational Linguistics, published byCambridge University Press.Steven Bird is the series editor.The editorial board has the following members:Chu-Ren Huang, Chair Professor of Applied Chinese Language Studies in the Department of Chinese and Bilingual Studies and the Dean of the Faculty of Humanities (The Hong Kong Polytechnic University),Chris Manning, Associate Professor of Linguistics and Computer Science in the Department of Linguistics and Computer Science (Stanford University), Yuji Matsumoto, Professor of Computational Linguistics in Graduate School of Information Science (Nara Institute of Science and Technology), Maarten de Rijke, Professor of Information Processing and Internet in the Informatics Institute (the University of Amsterdam) andHarold Somers, Professor of Language Engineering(Emeritus)in School of Computer Science (University of Manchester)."
"http://dbpedia.org/resource/Sukhotin's_algorithm"	"Sukhotin's algorithm"	"Natural language processing"	"Sukhotin's algorithm (introduced by Boris V. Sukhotin) is a statistical classification algorithm for classifying characters in a text as vowels or consonants. It may also be of use in some of substitution ciphers and has been considered in deciphering the Voynich manuscript, though one problem is to agree on the set of symbols the manuscript is written in."
"http://dbpedia.org/resource/Language_identification"	"Language identification"	"Natural language processing"	"In natural language processing, language identification or language guessing is the problem of determining which natural language given content is in. Computational approaches to this problem view it as a special case of text categorization, solved with various statistical methods."
"http://dbpedia.org/resource/Latent_semantic_analysis"	"Latent semantic analysis"	"Natural language processing"	"Latent semantic analysis (LSA) is a technique in natural language processing, in particular distributional semantics, of analyzing relationships between a set of documents and the terms they contain by producing a set of concepts related to the documents and terms. LSA assumes that words that are close in meaning will occur in similar pieces of text. A matrix containing word counts per paragraph (rows represent unique words and columns represent each paragraph) is constructed from a large piece of text and a mathematical technique called singular value decomposition (SVD) is used to reduce the number of rows while preserving the similarity structure among columns. Words are then compared by taking the cosine of the angle between the two vectors (or the dot product between the normalizations of the two vectors) formed by any two rows. Values close to 1 represent very similar words while values close to 0 represent very dissimilar words. An information retrieval technique using latent semantic structure was patented in 1988 (US Patent 4,839,853, now expired) by Scott Deerwester, Susan Dumais, George Furnas, Richard Harshman, Thomas Landauer, Karen Lochbaum and Lynn Streeter. In the context of its application to information retrieval, it is sometimes called Latent Semantic Indexing (LSI)."
"http://dbpedia.org/resource/Recursive_transition_network"	"Recursive transition network"	"Natural language processing"	"A recursive transition network (""RTN"") is a graph theoretical schematic used to represent the rules of a context-free grammar. RTNs have application to programming languages, natural language and lexical analysis. Any sentence that is constructed according to the rules of an RTN is said to be ""well-formed"". The structural elements of a well-formed sentence may also be well-formed sentences by themselves, or they may be simpler structures. This is why RTNs are described as recursive."
"http://dbpedia.org/resource/Speech_segmentation"	"Speech segmentation"	"Natural language processing"	"Speech segmentation is the process of identifying the boundaries between words, syllables, or phonemes in spoken natural languages. The term applies both to the mental processes used by humans, and to artificial processes of natural language processing. Speech segmentation is a subfield of general speech perception and an important subproblem of the technologically focused field of speech recognition, and cannot be adequately solved in isolation. As in most natural language processing problems, one must take into account context, grammar, and semantics, and even so the result is often a probabilistic division (statistically based on likelihood) rather than a categorical one. Though it seems that coarticulation - a phenomenon which may happen between adjacent words just as easily as within a single word - presents the main challenge in speech segmentation across languages, some other problems and strategies employed in solving those problems can be seen in the following sections. This problem overlaps to some extent with the problem of text segmentation that occurs in some languages which are traditionally written without inter-word spaces, like Chinese and Japanese, compared to writing systems which indicate speech segmentation between words by a word divider, such as the space. However, even for those languages, text segmentation is often much easier than speech segmentation, because the written language usually has little interference between adjacent words, and often contains additional clues not present in speech (such as the use of Chinese characters for word stems in Japanese). Word Boundary Identification can be overcome by NLU approaches such as Patom theory integrated with Role and Reference Grammar (RRG) for languages without spaces between words such as Japanese and Chinese."
"http://dbpedia.org/resource/Text_normalization"	"Text normalization"	"Natural language processing"	"Text normalization is the process of transforming text into a single canonical form that it might not have had before. Normalizing text before storing or processing it allows for separation of concerns, since input is guaranteed to be consistent before operations are performed on it. Text normalization requires being aware of what type of text is to be normalized and how it is to be processed afterwards; there is no all-purpose normalization procedure."
"http://dbpedia.org/resource/Natural_language_understanding"	"Natural language understanding"	"Natural language processing"	"Natural language understanding (NLU) is a subtopic of natural language processing in artificial intelligence that deals with machine reading comprehension. NLU is considered an AI-hard problem. The process of disassembling and parsing input is more complex than the reverse process of assembling output in natural language generation because of the occurrence of unknown and unexpected features in the input and the need to determine the appropriate syntactic and semantic schemes to apply to it, factors which are pre-determined when outputting language. There is considerable commercial interest in the field because of its application to news-gathering, text categorization, voice-activation, archiving, and large-scale content-analysis."
"http://dbpedia.org/resource/SHRDLU"	"SHRDLU"	"Natural language processing"	"SHRDLU was an early natural language understanding computer program, developed by Terry Winograd at MIT in 1968–1970. In it, the user carries on a conversation with the computer, moving objects, naming collections and querying the state of a simplified ""blocks world"", essentially a virtual box filled with different blocks. SHRDLU was written in the Micro Planner and Lisp programming language on the DEC PDP-6 computer and a DEC graphics terminal. Later additions were made at the computer graphics labs at the University of Utah, adding a full 3D rendering of SHRDLU's ""world"". The name SHRDLU was derived from ETAOIN SHRDLU, the arrangement of the alpha keys on a Linotype machine, arranged in descending order of usage frequency in English."
"http://dbpedia.org/resource/Controlled_natural_language"	"Controlled natural language"	"Natural language processing"	"Controlled natural languages (CNLs) are subsets of natural languages that are obtained by restricting the grammar and vocabulary in order to reduce or eliminate ambiguity and complexity. Traditionally, controlled languages fall into two major types: those that improve readability for human readers (e.g. non-native speakers),and those that enable reliable automatic semantic analysis of the language. The first type of languages (often called ""simplified"" or ""technical"" languages), for example ASD Simplified Technical English, Caterpillar Technical English, IBM's Easy English, are used in the industry to increase the quality of technical documentation, and possibly simplify the (semi-)automatic translation of the documentation.These languages restrict the writer by general rules such as ""Keep sentences short"", ""Avoid the use of pronouns"", ""Only use dictionary-approved words"", and ""Use only the active voice"". The second type of languages have a formal logical basis, i.e. they have a formal syntax and semantics, and can be mapped to an existing formal language, such as first-order logic. Thus, those languages can be used as knowledge-representation languages, and writing of those languages is supported by fully automatic consistency and redundancy checks, query answering, etc."
"http://dbpedia.org/resource/Cross-language_information_retrieval"	"Cross-language information retrieval"	"Natural language processing"	"Cross-language information retrieval (CLIR) is a subfield of information retrieval dealing with retrieving information written in a language different from the language of the user's query. For example, a user may pose their query in English but retrieve relevant documents written in French. To do so, most of CLIR systems use translation techniques. CLIR techniques can be classified into different categories based on different translation resources: 
*  Dictionary-based CLIR techniques 
*  Parallel corpora based CLIR techniques 
*  Comparable corpora based CLIR techniques 
*  Machine translator based CLIR techniques The first workshop on CLIR was held in Zürich during the SIGIR-96 conference. Workshops have been held yearly since 2000 at the meetings of the Cross Language Evaluation Forum (CLEF). The term ""cross-language information retrieval"" has many synonyms, of which the following are perhaps the most frequent: cross-lingual information retrieval, translingual information retrieval, multilingual information retrieval. The term ""multilingual information retrieval"" refers to CLIR in general, but it also has a specific meaning of cross-language information retrieval where a document collection is multilingual. Google Search had a cross-language search feature that was removed in 2013."
"http://dbpedia.org/resource/Realization_(linguistics)"	"Realization (linguistics)"	"Natural language processing"	"In linguistics, realization is the process by which some kind of surface representation is derived from its underlying representation; that is, the way in which some abstract object of linguistic analysis comes to be produced in actual language. Phonemes are often said to be realized by speech sounds. The different sounds that can realize a particular phoneme are called its allophones. Realization is also a subtask of natural language generation, which involvescreating an actual text in a human language (English, French, etc.) from a syntacticrepresentation. There are a number of software packages available for realization,most of which have been developed by academic research groups in NLG. The remainder of this article concerns realization of this kind."
"http://dbpedia.org/resource/Automatic_summarization"	"Automatic summarization"	"Natural language processing"	"Automatic summarization is the process of reducing a text document with a computer program in order to create a summary that retains the most important points of the original document. Technologies that can make a coherent summary take into account variables such as length, writing style and syntax. Automatic data summarization is part of machine learning and data mining. The main idea of summarization is to find a representative subset of the data, which contains the information of the entire set. Summarization technologies are used in a large number of sectors in industry today. An example of the use of summarization technology is search engines such as Google. Other examples include document summarization, image collection summarization and video summarization. Document summarization, tries to automatically create a representative summary or abstract of the entire document, by finding the most informative sentences. Similarly, in image summarization the system finds the most representative and important (or salient) images. Similarly, in consumer videos one would want to remove the boring or repetitive scenes, and extract out a much shorter and concise version of the video. This is also important, say for surveillance videos, where one might want to extract only important events in the recorded video, since most part of the video may be uninteresting with nothing going on. As the problem of information overload grows, and as the amount of data increases, the interest in automatic summarization is also increasing. Generally, there are two approaches to automatic summarization: extraction and abstraction. Extractive methods work by selecting a subset of existing words, phrases, or sentences in the original text to form the summary. In contrast, abstractive methods build an internal semantic representation and then use natural language generation techniques to create a summary that is closer to what a human might generate. Such a summary might contain words not explicitly present in the original. Research into abstractive methods is an increasingly important and active research area, however due to complexity constraints, research to date has focused primarily on extractive methods. In some application domains, extractive summarization makes more sense. Examples of these include image collection summarization and video summarization."
"http://dbpedia.org/resource/PropBank"	"PropBank"	"Natural language processing"	"PropBank is a corpus that is annotated with verbal propositions and their arguments—a ""proposition bank"". Although ""PropBank"" refers to a specific corpus produced by Martha Palmer et al., the term propbank is also coming to be used as a common noun referring to any corpus that has been annotated with propositions and their arguments. The PropBank project has been extremely influential in recent research in natural language processing. It led to wide popularity for the semantic role labelling task."
"http://dbpedia.org/resource/Information_retrieval"	"Information retrieval"	"Natural language processing"	"Information retrieval (IR) is the activity of obtaining information resources relevant to an information need from a collection of information resources. Searches can be based on full-text or other content-based indexing. Automated information retrieval systems are used to reduce what has been called ""information overload"". Many universities and public libraries use IR systems to provide access to books, journals and other documents. Web search engines are the most visible IR applications."
"http://dbpedia.org/resource/Kleene_star"	"Kleene star"	"Natural language processing"	"In mathematical logic and computer science, the Kleene star (or Kleene operator or Kleene closure) is a unary operation, either on sets of strings or on sets of symbols or characters. In mathematicsit is more commonly known as the free monoid construction. The application of the Kleene star to a set V is written as V*. It is widely used for regular expressions, which is the context in which it was introduced by Stephen Kleene to characterise certain automata, where it means ""zero or more"". 1.  
*  If V is a set of strings, then V* is defined as the smallest superset of V that contains the empty string ε and is closed under the string concatenation operation. 2.  
*  If V is a set of symbols or characters, then V* is the set of all strings over symbols in V, including the empty string ε. The set V* can also be described as the set of finite-length strings that can be generated by concatenating arbitrary elements of V, allowing the use of the same element multiple times. If V is either the empty set ∅ or the singleton set {ε}, then V*={ε}; if V is any other finite set, then V* is a countably infinite set. The operators are used in rewrite rules for generative grammars."
"http://dbpedia.org/resource/Natural_language"	"Natural language"	"Natural language processing"	"In neuropsychology, linguistics and the philosophy of language, a natural language or ordinary language is any language that has evolved naturally in humans through use and repetition without conscious planning or premeditation. Natural languages can take different forms, such as speech, signing, or writing. They are distinguished from constructed and formal languages such as those used to program computers or to study logic."
"http://dbpedia.org/resource/Natural_language_processing"	"Natural language processing"	"Natural language processing"	"Natural language processing is a field of computer science, artificial intelligence, and computational linguistics concerned with the interactions between computers and human (natural) languages. As such, NLP is related to the area of human–computer interaction. Many challenges in NLP involve: natural language understanding, enabling computers to derive meaning from human or natural language input; and others involve natural language generation."
"http://dbpedia.org/resource/Attempto_Controlled_English"	"Attempto Controlled English"	"Natural language processing"	"Attempto Controlled English (ACE) is a controlled natural language, i.e. a subset of standard English with a restricted syntax and restricted semantics described by a small set of construction and interpretation rules. It has been under development at the University of Zurich since 1995. In 2011, ACE version 6.6 was announced. ACE can serve as knowledge representation, specification, and query language, and is intended for professionals who want to use formal notations and formal methods, but may not be familiar with them. Though ACE appears perfectly natural – it can be read and understood by any speaker of English – it is in fact a formal language. ACE and its related tools have been used in the fields of software specifications, theorem proving, text summaries, ontologies, rules, querying, medical documentation and planning. Here are some simple examples: 1.  
*  Every woman is a human. 2.  
*  A woman is a human. 3.  
*  A man tries-on a new tie. If the tie pleases his wife then the man buys it. ACE construction rules require that each noun be introduced by a determiner (a, every, no, some, at least 5, ...). ACE interpretation rules decide that (1) is interpreted as universally quantified, while (2) is interpreted as existentially quantified. Sentences like ""Women are human"" do not follow ACE syntax and are consequently not valid. Interpretation rules resolve the anaphoric references in (3): the tie and it of the second sentence refer to a new tie of the first sentence, while his and the man of the second sentence refer to a man of the first sentence. Thus an ACE text is a coherent entity of anaphorically linked sentences. The Attempto Parsing Engine (APE) translates ACE texts unambiguously into discourse representation structures (DRS) that use a variant of the language of first-order logic. A DRS can be further translated into other formal languages, for instance AceRules with various semantics, OWL, and SWRL. Translating an ACE text into (a fragment of) first-order logic allows users to reason about the text, for instance to verify, to validate, and to query it."
"http://dbpedia.org/resource/Information_extraction"	"Information extraction"	"Natural language processing"	"Information extraction (IE) is the task of automatically extracting structured information from unstructured and/or semi-structured machine-readable documents. In most of the cases this activity concerns processing human language texts by means of natural language processing (NLP). Recent activities in multimedia document processing like automatic annotation and content extraction out of images/audio/video could be seen as information extraction. Due to the difficulty of the problem, current approaches to IE focus on narrowly restricted domains. An example is the extraction from news wire reports of corporate mergers, such as denoted by the formal relation: , from an online news sentence such as: ""Yesterday, New York based Foo Inc. announced their acquisition of Bar Corp."" A broad goal of IE is to allow computation to be done on the previously unstructured data. A more specific goal is to allow logical reasoning to draw inferences based on the logical content of the input data. Structured data is semantically well-defined data from a chosen target domain, interpreted with respect to category and context. Information Extraction is the part of a greater puzzle which deals with the problem of devising automatic methods for text management, beyond its transmission, storage and display. The discipline of information retrieval (IR) has developed automatic methods, typically of a statistical flavor, for indexing large document collections and classifying documents. Another complementary approach is that of natural language processing (NLP) which has solved the problem of modelling human language processing with considerable success when taking into account the magnitude of the task. In terms of both difficulty and emphasis, IE deals with tasks in between both IR and NLP. In terms of input, IE assumes the existence of a set of documents in which each document follows a template, i.e. describes one or more entities or events in a manner that is similar to those in other documents but differing in the details. An example, consider a group of newswire articles on Latin American terrorism with each article is presumed to be based upon one or more terroristic acts. We also define for any given IE task a template, which is a(or a set of) case frame(s) to hold the information contained in a single document. For the terrorism example, a template would have slots corresponding to the perpetrator, victim, and weapon of the terroristic act, and the date on which the event happened. An IE system for this problem is required to “understand” an attack article only enough to find data corresponding to the slots in this template."
"http://dbpedia.org/resource/Latent_semantic_mapping"	"Latent semantic mapping"	"Natural language processing"	"Latent semantic mapping (LSM) is a data-driven framework to model globally meaningful relationships implicit in large volumes of (often textual) data. It is a generalization of latent semantic analysis. In information retrieval, LSA enables retrieval on the basis of conceptual content, instead of merely matching words between queries and documents. LSM was derived from earlier work on latent semantic analysis. There are 3 main characteristics of latent semantic analysis: Discrete entities, usually in the form of words and documents, are mapped onto continuous vectors, the mapping involves a form of global correlation pattern, and dimensionality reduction is an important aspect of the analysis process. These constitute generic properties, and have been identified as potentially useful in a variety of different contexts. This usefulness has encouraged great interest in LSM. The intended product of latent semantic mapping, is a data-driven framework for modeling relationships in large volumes of data. Mac OS X v10.5 and later includes a framework implementing latent semantic mapping."
"http://dbpedia.org/resource/Native-language_identification"	"Native-language identification"	"Natural language processing"	"Native-language identification (NLI) is the task of determining an author's native language (L1) based only on their writings in a second language (L2).NLI works through identifying language-usage patterns that are common to specific L1 groups and then applying this knowledge to predict the native language of previously unseen texts.This is motivated in part by applications in second-language acquisition, language teaching and forensic linguistics, amongst others."
"http://dbpedia.org/resource/ROUGE_(metric)"	"ROUGE (metric)"	"Natural language processing"	"ROUGE, or Recall-Oriented Understudy for Gisting Evaluation, is a set of metrics and a software package used for evaluating automatic summarization and machine translation software in natural language processing. The metrics compare an automatically produced summary or translation against a reference or a set of references (human-produced) summary or translation."
"http://dbpedia.org/resource/Lesk_algorithm"	"Lesk algorithm"	"Natural language processing"	"The Lesk algorithm is a classical algorithm for word sense disambiguation introduced by Michael E. Lesk in 1986."
"http://dbpedia.org/resource/ChaSen"	"ChaSen"	"Natural language processing"	"ChaSen  is a morphological parser for the Japanese language. This tool for analyzing morphemes was developed at the Matsumoto laboratory, Nara Institute of Science and Technology."
"http://dbpedia.org/resource/Document_classification"	"Document classification"	"Natural language processing"	"Document classification or document categorization is a problem in library science, information science and computer science. The task is to assign a document to one or more classes or categories. This may be done ""manually"" (or ""intellectually"") or algorithmically. The intellectual classification of documents has mostly been the province of library science, while the algorithmic classification of documents is mainly in information science and computer science. The problems are overlapping, however, and there is therefore interdisciplinary research on document classification. The documents to be classified may be texts, images, music, etc. Each kind of document possesses its special classification problems. When not otherwise specified, text classification is implied. Documents may be classified according to their subjects or according to other attributes (such as document type, author, printing year etc.). In the rest of this article only subject classification is considered. There are two main philosophies of subject classification of documents: the content-based approach and the request-based approach."
"http://dbpedia.org/resource/Robby_Garner"	"Robby Garner"	"Natural language processing"	"Robby Garner (born 1963) is an American natural language programmer and software developer. He won the 1998 and 1999 Loebner Prize contests with the program called Albert One. He is listed in the 2001 Guinness Book of World Records as having written the ""most human"" computer program."
"http://dbpedia.org/resource/Text_Retrieval_Conference"	"Text Retrieval Conference"	"Natural language processing"	"The Text REtrieval Conference (TREC) is an ongoing series of workshops focusing on a list of different information retrieval (IR) research areas, or tracks. It is co-sponsored by the National Institute of Standards and Technology (NIST) and the Intelligence Advanced Research Projects Activity (part of the office of the Director of National Intelligence), and began in 1992 as part of the TIPSTER Text program. Its purpose is to support and encourage research within the information retrieval community by providing the infrastructure necessary for large-scale evaluation of text retrieval methodologies and to increase the speed of lab-to-product transfer of technology. Each track has a challenge wherein NIST provides participating groups with data sets and test problems. Depending on track, test problems might be questions, topics, or target extractable features. Uniform scoring is performed so the systems can be fairly evaluated. After evaluation of the results, a workshop provides a place for participants to collect together thoughts and ideas and present current and future research work."
"http://dbpedia.org/resource/LKB"	"LKB"	"Natural language processing"	"Linguistic Knowledge Builder (LKB) is a free and open source grammar engineering environment for creating grammars and lexicons of natural languages. Any unification-based grammar can be implemented, but LKB is typically used for grammars with typed feature structures such as HPSG. It is implemented in Common Lisp, and constitutes one core component of the DELPH-IN collaboration."
"http://dbpedia.org/resource/Predictive_text"	"Predictive text"	"Natural language processing"	"Predictive text is an input technology used where one key or button represents many letters, such as on the numeric keypads of mobile phones and in accessibility technologies. Each key press results in a prediction rather than repeatedly sequencing through the same group of ""letters"" it represents, in the same, invariable order. Predictive text could allow for an entire word to be input by single keypress. Predictive text makes efficient use of fewer device keys to input writing into a text message, an e-mail, an address book, a calendar, and the like. The most widely used, general, predictive text systems are T9, iTap, eZiText, and LetterWise/WordWise. There are many ways to build a device that predicts text, but all predictive text systems have initial linguistic settings that offer predictions that are re-prioritized to adapt to each user. This learning adapts, by way of the device memory, to a user's disambiguating feedback that results in corrective key presses, such as pressing a ""next"" key to get to the intention. Most predictive text systems have a user database to facilitate this process. Theoretically the number of keystrokes required per desired character in the finished writing is, on average, comparable to using a keyboard. This is approximately true providing that all words used are in its database, punctuation is ignored, and no input mistakes are made typing or spelling.In practice, these factors are found to cause tremendous variance in the efficiency gain. The theoretical keystrokes per character, KSPC, of a keyboard is KSPC=1.00, and of multi-tap is KSPC=2.03. Eatoni' LetterWise is a predictive multi-tap hybrid, which when operating on a standard telephone keypad achieves KSPC=1.15 for English. The choice of which predictive text system is the best to use involves matching the user's preferred interface style, the user's level of learned ability to operate predictive text software, and the user's efficiency goal. There are various levels of risk in predictive text systems, versus multi-tap systems, because the predicted text that is automatically written that provide the speed and mechanical efficiency benefit, could, if the user is not careful to review, result in transmitting misinformation. Predictive text systems take time to learn to use well, and so generally, a device's system has user options to set up the choice of multi-tap or of any one of several schools of predictive text methods."
"http://dbpedia.org/resource/Affix_grammar_over_a_finite_lattice"	"Affix grammar over a finite lattice"	"Natural language processing"	"In linguistics, the affix grammars over a finite lattice (AGFL) formalism is a notation for context-free grammars with finite set-valued features, acceptable to linguists of many different schools. The AGFL-project aims at the development of a technology for Natural language processing available under the GNU GPL."
"http://dbpedia.org/resource/Transderivational_search"	"Transderivational search"	"Natural language processing"	"Transderivational search (often abbreviated to TDS) is a psychological and cybernetics term, meaning when a search is being conducted for a fuzzy match across a broad field. In computing the equivalent function can be performed using content-addressable memory. Unlike usual searches, which look for literal (i.e. exact, logical, or regular expression) matches, a transderivational search is a search for a possible meaning or possible match as part of communication, and without which an incoming communication cannot be made any sense of whatsoever. It is thus an integral part of processing language, and of attaching meaning to communication. A psychological example of TDS is in Ericksonian hypnotherapy, where vague suggestions are used that the patient must process intensely in order to find their own meanings, thus ensuring that the practitioner does not intrude his own beliefs into the subject's inner world."
"http://dbpedia.org/resource/Tatoeba"	"Tatoeba"	"Natural language processing"	"Tatoeba.org is a free collaborative online database of example sentences geared towards foreign language learners. Its name comes from the Japanese term ""tatoeba"" (例えば　tatoeba), meaning ""for example"". Unlike other online dictionaries, which focus on words, Tatoeba focuses on translation of complete sentences. In addition, the structure of the database and interface emphasize one-to-many relationships. Not only can a sentence have multiple translations within a single language, but its translations into all languages are readily visible, as are indirect translations that involve a chain of stepwise links from one language to another."
"http://dbpedia.org/resource/Grammar_induction"	"Grammar induction"	"Natural language processing"	"Grammar induction, also known as grammatical inference or syntactic pattern recognition, refers to the process in machine learning of learning a formal grammar (usually as a collection of re-write rules or productions or alternatively as a finite state machine or automaton of some kind) from a set of observations, thus constructing a model which accounts for the characteristics of the observed objects. More generally, grammatical inference is that branch of machine learning where the instance space consists of discrete combinatorial objects such as strings, trees and graphs."
"http://dbpedia.org/resource/Just_This_Once"	"Just This Once"	"Natural language processing"	"Just This Once is a 1993 romance novel written in the style of Jacqueline Susann by a Macintosh IIcx computer named ""Hal"" in collaboration with its programmer, Scott French. French reportedly spent $40,000 and 8 years developing an artificial intelligence program to analyze Susann's works and attempt to create a novel that Susann might have written. A legal dispute between the estate of Jacqueline Susann and the publisher resulted in a settlement to split the profits, and the book was referenced in several legal journal articles about copyright laws. The book had two small print runs totaling 35,000 copies, receiving mixed reviews."
"http://dbpedia.org/resource/Naomi_Sager"	"Naomi Sager"	"Natural language processing"	"Naomi Sager (born 1927) is an American computational linguistics research scientist. She is a former research professor at New York University, now retired. She is a pioneer in the development of natural language processing for computers."
"http://dbpedia.org/resource/Multilingual_notation"	"Multilingual notation"	"Natural language processing"	"A multilingual notation is a representation in a lexical resource that allows the translation between two or more words."
"http://dbpedia.org/resource/CMU_Pronouncing_Dictionary"	"CMU Pronouncing Dictionary"	"Natural language processing"	"The CMU Pronouncing Dictionary (also known as CMUdict) is an open source pronouncing dictionary originally created by the Speech Group at Carnegie Mellon University (CMU) for use in speech recognition research. CMUdict provides a mapping orthopraphic/phonetic for English words in their North American pronunciations. It is commonly used to generate representations for speech recognition (ASR), e.g. the CMU Sphinx system, and speech synthesis (TTS), e.g. the Festival system. CMUdict can be used as a training corpus for building statistical grapheme-to-phoneme (g2p) models that will generate pronunciations for words not yet included in the dictionary. The most recent release is 0.7b; it contains over 134,000 entries. An interactive lookup version is available."
"http://dbpedia.org/resource/Attensity"	"Attensity"	"Natural language processing"	"Attensity provides social analytics and engagement applications for social customer relationship management (social CRM). Attensity's text analytics software applications extract facts, relationships and sentiment from unstructured data, which comprise approximately 85% of the information companies store electronically. The software uses natural language processing technology to address collective intelligence in blogs, online forums and social media such as Twitter and Facebook; the voice of the customer in surveys and emails; customer experience management (CEM); e-services; research and e-discovery; risk and compliance; and intelligence analysis. Corporations that use or have used Attensity software include Airbus, Charles Schwab, Citigroup, HP, JetBlue, Lloyds Banking Group, Safeway, Siemens, StubHub, TiVo, Travelocity, Unilever, Walgreens, Wells Fargo and Whirlpool. Government organizations that use or have used Attensity solutions include the Federal Bureau of Investigation, the National Security Agency and the Defense Intelligence Agency. As part of the 2012 election campaign, Attensity partnered with Yahoo! to provide social analytics around the televised GOP debates. Attensity also provided research on customer sentiment on campaign issues to media outlets such as USA Today."
"http://dbpedia.org/resource/IGlue"	"IGlue"	"Natural language processing"	"iGlue is an experimental database with detailed search options, containing entities and information editing tool. It organizes interrelated images, videos, individuals, institutions, objects, websites, geographical locations into cohesive data structures. The most important components of iGlue system are: the flexible database which contains semantic elements, and entities, and their relational connections. In4 Kft. was established in August 2007 as a company specialised in the development of online applications based on university researches, with the participation of young university researchers and of the Power of the Dream Ventures, as financial investor."
"http://dbpedia.org/resource/Lexalytics"	"Lexalytics"	"Natural language processing"	"Lexalytics, Inc. provides sentiment and intent analysis to an array of companies using SaaS and cloud based technology. Salience 6, the engine behind Lexalytics, was built as an on-premises, multi-lingual text analysis engine. It is leased to other companies who use it to power filtering and reputation management programs. In July, 2015 Lexalytics acquired Semantria to be used as a cloud option for its technology."
"http://dbpedia.org/resource/Abzooba"	"Abzooba"	"Natural language processing"	"Abzooba is a San Francisco Bay Area based analytics company with centres in India located at Calcutta, Pune and Bangalore. Founded in 2010, Abzooba is involved in research work in areas like software product engineering, data mining, high end data analytics, natural language processing, semantictechnologies, machine learning, and analysis of structured and unstructured data. As of 2014, Abzooba has acquired customers, primarily in the US, and some Indian customers."
"http://dbpedia.org/resource/Artificial_Solutions"	"Artificial Solutions"	"Natural language processing"	"Artificial Solutions is a multinational software company that develops and sells natural language interaction products for enterprise and consumer use. The company's natural language solutions have been deployed in a wide range of industries including finance, telecoms, the public sector, retail and travel."
"http://dbpedia.org/resource/NetOwl"	"NetOwl"	"Natural language processing"	"NetOwl is a suite of multilingual text and entity analytics products that analyze Big Data in the form of text data – reports, web, social media, etc. – as well as structured entity data about people, organizations, places, and things. NetOwl utilizes computational linguistics, natural language processing, and machine learning approaches to extract entities, links, and events, to perform sentiment analysis, to assign latitude/longitude to geographical references in text, to translate names written in foreign languages, and to perform name matching and identity resolution.NetOwl's customers use the products for, among others, semantic search and discovery, geospatial analysis, intelligence analysis, content enrichment, compliance monitoring, cyber threat monitoring, risk management, and bioinformatics."
"http://dbpedia.org/resource/T9_(predictive_text)"	"T9 (predictive text)"	"Natural language processing"	"T9, which stands for Text on 9 keys, is a USA-patented predictive text technology for mobile phones (specifically those that contain a 3x4 numeric keypad), originally developed by Tegic Communications, now part of Nuance Communications. T9 was used on phones from Verizon Wireless, NEC, Nokia, Samsung Electronics, Siemens, Sony Ericsson, Sanyo, Sagem and others. It was also used by Texas Instruments PDA Avigo during the late 1990s. Its main competitors are iTap created by Motorola, SureType created by RIM, Eatoni's LetterWise and WordWise, and Intelab's Tauto."
"http://dbpedia.org/resource/AFNLP"	"AFNLP"	"Natural language processing"	"AFNLP (Asian Federation of Natural Language Processing Associations) is the organization for coordinating the natural language processing related activities and events in the Asia-Pacific region."
"http://dbpedia.org/resource/GeneRIF"	"GeneRIF"	"Natural language processing"	"A GeneRIF or Gene Reference Into Function is a short (255 characters or fewer) statement about the function of a gene. GeneRIFs provide a simple mechanism for allowing scientists to add to the functional annotation of genes described in the Entrez Gene database. In practice, function is construed quite broadly. For example, there are GeneRIFs that discuss the role of a gene in a disease, GeneRIFs that point the viewer towards a review article about the gene, and GeneRIFs that discuss the structure of a gene. However, the stated intent is for GeneRIFs to be about gene function. Currently over half a million geneRIFs have been created for genes from almost 1000 different species. GeneRIFs are always associated with specific entries in the Entrez Gene database. Each GeneRIF has a pointer to the PubMed ID (a type of document identifier) of a scientific publication that provides evidence for the statement made by the GeneRIF. GeneRIFs are often extracted directly from the document that is identified by the PubMed ID, very frequently from its title or from its final sentence. GeneRIFs are usually produced by NCBI indexers, but anyone may submit a GeneRIF.To be processed, a valid Gene ID must exist for the specific gene, or the Gene staff must have assigned an overall Gene ID to the species. The latter case is implemented via records in Gene with the symbol NEWENTRY. Once the Gene ID is identified, only three types of information are required to complete a submission: 1.  
*  a concise phrase describing a function or functions (less than 255 characters in length, preferably more than a restatement of the title of the paper); 2.  
*  a published paper describing that function, implemented by supplying the PubMed ID of a citation in PubMed; 3.  
*  a valid e-mail address (which will remain confidential)."
"http://dbpedia.org/resource/Triphone"	"Triphone"	"Natural language processing"	"In linguistics, a triphone is a sequence of three phonemes. Triphones are useful in models of natural language processing where they are used to establish the various contexts in which a phoneme can occur in a particular natural language."
"http://dbpedia.org/resource/Computational_semantics"	"Computational semantics"	"Natural language processing"	"Computational semantics is the study of how to automate the process of constructing and reasoning with meaning representations of natural language expressions. It consequently plays an important role in natural language processing and computational linguistics. Some traditional topics of interest are: construction of meaning representations, semantic underspecification, anaphora resolution, presupposition projection, and quantifier scope resolution. Methods employed usually draw from formal semantics or statistical semantics. Computational semantics has points of contact with the areas of lexical semantics (word sense disambiguation and semantic role labeling), discourse semantics, knowledge representation and automated reasoning (in particular, automated theorem proving). Since 1999 there has been an ACL special interest group on computational semantics, SIGSEM."
"http://dbpedia.org/resource/Multi-document_summarization"	"Multi-document summarization"	"Natural language processing"	"Multi-document summarization is an automatic procedure aimed at extraction of information from multiple texts written about the same topic. The resulting summary report allows individual users, such as professional information consumers, to quickly familiarize themselves with information contained in a large cluster of documents. In such a way, multi-document summarization systems are complementing the news aggregators performing the next step down the road of coping with information overload."
"http://dbpedia.org/resource/Language_Computer_Corporation"	"Language Computer Corporation"	"Natural language processing"	"Language Computer Corporation (LCC) is a natural language processing research company based in Richardson, Texas. The company develops a variety of natural language processing products, including software for question answering, information extraction, and automatic summarization. Since its founding in 1995, the low-profile company has landed significant United States Government contracts, with $8,353,476 in contracts in 2006-2008. While the company has focused primarily on the government software market, LCC has also used its technology to spin off three start-up companies. The first spin off, known as Lymba Corporation, markets the PowerAnswer question answering product originally developed at LCC. In 2010, LCC's CEO, Andrew Hickl, co-founded two start-ups which made use of the company's technology. These included Swingly, an automatic question answering start-up, and Extractiv, an information extraction service that was founded in partnership with Houston, Texas-based 80legs."
"http://dbpedia.org/resource/Lexical_Markup_Framework"	"Lexical Markup Framework"	"Natural language processing"	"Language resource management - Lexical markup framework (LMF; ISO 24613:2008), is the ISO International Organization for Standardization ISO/TC37 standard for natural language processing (NLP) and machine-readable dictionary (MRD) lexicons.The scope is standardization of principles and methods relating to language resources in the contexts of multilingual communication and cultural diversity."
"http://dbpedia.org/resource/Phrase_structure_grammar"	"Phrase structure grammar"	"Natural language processing"	"The term phrase structure grammar was originally introduced by Noam Chomsky as the term for grammars as defined by phrase structure rules, i.e. rewrite rules of the type studied previously by Emil Post and Axel Thue (Post canonical systems). Some authors, however, reserve the term for more restricted grammars in the Chomsky hierarchy: context-sensitive grammars, or context-free grammars. In a broader sense, phrase structure grammars are also known as constituency grammars. The defining trait of phrase structure grammars is thus their adherence to the constituency relation, as opposed to the dependency relation of dependency grammars."
"http://dbpedia.org/resource/Production_(computer_science)"	"Production (computer science)"	"Natural language processing"	"A production or production rule in computer science is a rewrite rule specifying a symbol substitution that can be recursively performed to generate new symbol sequences. A finite set of productions is the main component in the specification of a formal grammar (specifically a generative grammar). The other components are a finite set of nonterminal symbols, a finite set (known as an alphabet) of terminal symbols that is disjoint from and a distinguished symbol that is the start symbol. In an unrestricted grammar, a production is of the form where and are arbitrary strings of terminals and nonterminals however may not be the empty string. If is the empty string, this is denoted by the symbol , or (rather than leave the right-hand side blank). So productions are of the form: where is the Kleene star operator, and denotes set union. The other types of formal grammar in the Chomsky hierarchy impose additional restrictions on what constitutes a production. Notably in a context-free grammar, the left-hand side of a production must be a single nonterminal symbol. So productions are of the form:"
"http://dbpedia.org/resource/SPL_notation"	"SPL notation"	"Natural language processing"	"SPL (Sentence Plan Language) is an abstract notation representing the semantics of a sentence in natural language. A generator can be used to transform input in SPL notation into a sentence in a natural language. There is no distinct definition of SPL available on the Internet, although some papers are available on the subject."
"http://dbpedia.org/resource/Word-sense_disambiguation"	"Word-sense disambiguation"	"Natural language processing"	"In computational linguistics, word-sense disambiguation (WSD) is an open problem of natural language processing and ontology. WSD is identifying which sense of a word (i.e. meaning) is used in a sentence, when the word has multiple meanings. The solution to this problem impacts other computer-related writing, such as discourse, improving relevance of search engines, anaphora resolution, coherence, inference et cetera. The human brain is quite proficient at word-sense disambiguation. The fact that natural language is formed in a way that requires so much of it is a reflection of that neurologic reality. In other words, human language developed in a way that reflects (and also has helped to shape) the innate ability provided by the brain's neural networks. In computer science and the information technology that it enables, it has been a long-term challenge to develop the ability in computers to do natural language processing and machine learning. To date, a rich variety of techniques have been researched, from dictionary-based methods that use the knowledge encoded in lexical resources, to supervised machine learning methods in which a classifier is trained for each distinct word on a corpus of manually sense-annotated examples, to completely unsupervised methods that cluster occurrences of words, thereby inducing word senses. Among these, supervised learning approaches have been the most successful algorithms to date. Accuracy of current algorithms is difficult to state without a host of caveats. In English, accuracy at the coarse-grained (homograph) level is routinely above 90%, with some methods on particular homographs achieving over 96%. On finer-grained sense distinctions, top accuracies from 59.1% to 69.0% have been reported in recent evaluation exercises (SemEval-2007, Senseval-2), where the baseline accuracy of the simplest possible algorithm of always choosing the most frequent sense was 51.4% and 57%, respectively."
"http://dbpedia.org/resource/Calais_(Reuters_product)"	"Calais (Reuters product)"	"Natural language processing"	"Calais is a service by Thomson Reuters that automatically extracts semantic information from web pages in a format that can be used on the semantic web. Calais was launched in January 2008, and is free to use. The Calais Web service reads unstructured text and returns Resource Description Framework formatted results identifying entities, facts and events within the text. The service appears to be based on technology acquired when Reuters purchased ClearForest in 2007. The technology has also been used to automatically tag blog articles and organize museum collections. Calais uses natural language processing technologies delivered via a web service interface."
"http://dbpedia.org/resource/International_Conference_on_Language_Resources_and_Evaluation"	"International Conference on Language Resources and Evaluation"	"Natural language processing"	"LREC, the International Conference on Language Resources and Evaluation is a biennial conference organised by the European Language Resources Association with the support of institutions and organisations involved in Natural language processing.The series of LREC conferences was launched in Granada in 1998. Since then, LREC has become the major event on Language Resources (LRs) and Evaluation for Human Language Technologies (HLT). The aim of LREC is to provide an overview of the state-of-the-art, explore new R&D directions and emerging trends, exchange information regarding LRs and their applications, evaluation methodologies and tools, ongoing and planned activities, industrial uses and needs, requirements coming from the e-society, both with respect to policy issues and to technological and organisational ones."
"http://dbpedia.org/resource/Vocabulary_mismatch"	"Vocabulary mismatch"	"Natural language processing"	"Vocabulary mismatch is a common phenomenon in the usage of natural languages, occurring when different people name the same thing or concept differently. Furnas et al. (1987) were perhaps the first to quantitatively study the vocabulary mismatch problem. Their results show that on average 80% of the times different people (experts in the same field) will name the same thing differently. There are usually tens of possible names that can be attributed to the same thing. This research motivated the work on latent semantic indexing. The vocabulary mismatch between user created queries and relevant documents in a corpus causes the term mismatch problem in information retrieval. Zhao and Callan (2010) were perhaps the first to quantitatively study the vocabulary mismatch problem in a retrieval setting. Their results show that an average query term fails to appear in 30-40% of the documents that are relevant to the user query. They also showed that this probability of mismatch is a central probability in one of the fundamental probabilistic retrieval models, the Binary Independence Model. They developed novel term weight prediction methods that can lead to potentially 50-80% accuracy gains in retrieval over strong keyword retrieval models. Further research along the line shows that expert users can use Boolean Conjunctive Normal Form expansion to improve retrieval performance by 50-300% over unexpanded keyword queries."
"http://dbpedia.org/resource/Lexxe"	"Lexxe"	"Natural language processing"	"Lexxe is an internet search engine that applies Natural Language Processingin its semantic search technology. Founded in 2005 by Dr. Hong Liang Qiao,Lexxe is based in Sydney, Australia. Lexxe launched its Alpha version in 2005, featuring Natural Language questionanswering (i.e. users could ask questions in English to the search engine apart from keyword searches — this feature has been suspended for redevelopment since 2010). In 2011, Lexxe launched Beta version  with a new search technology called Semantic Key. Semantic Keys enable users to query with a conceptual keyword(or a keyword with a special meaning, hence the term Semantic Key) in orderto find instances under the concept, e.g. price → $5.95 or €200, color →red, yellow, white. For example, “price: a pound of apples”, “color:ferrari”. With initial 500 Semantic Keys at the Beta launch, Lexxebecame the first search engine in the world to offer this unique and usefulsearch technology to the users. In human communication, Semantic Keys are common in questions andinformation exchange. Likewise, they could be a useful tool for searchengines between users and internet information, particularly when dealingwith unstructured data. Lexxe Beta version also supports normal keyword search. On June 17, 2015, Lexxe closed down its beta version service. However, there are plans to launch a new Lexxe search engine."
"http://dbpedia.org/resource/NetBase_Solutions,_Inc."	"NetBase Solutions, Inc."	"Natural language processing"	"NetBase Solutions, Inc. is a Mountain View, CA based developer of natural language processing technology used to analyze social media and other web content. It was founded by two engineers from Ariba in 2004 as Accelovation, before changing names to NetBase in 2008. It has raised a total of $21 million in funding. It's sold primarily on a subscription basis to large companies to conduct market research and social media marketing analytics. NetBase has been used to evaluate the top reasons men wear stubble, the products Kraft should develop and the favorite tech company based on digital conversations."
"http://dbpedia.org/resource/Paco_Nathan"	"Paco Nathan"	"Natural language processing"	"Paco Nathan (born 1962) is an American computer scientist, author, and performance art show producer from San Luis Obispo, California, who established much of his career in Austin, Texas."
"http://dbpedia.org/resource/Machine_translation_software_usability"	"Machine translation software usability"	"Natural language processing"	"The sections below give objective criteria for evaluating the usability of machine translation software output."
"http://dbpedia.org/resource/Ontology_learning"	"Ontology learning"	"Natural language processing"	"Ontology learning (ontology extraction, ontology generation, or ontology acquisition) is the automatic or semi-automatic creation of ontologies, including extracting the corresponding domain's terms and the relationships between those concepts from a corpus of natural language text, and encoding them with an ontology language for easy retrieval. As building ontologies manually is extremely labor-intensive and time-consuming, there is great motivation to automate the process. Typically, the process starts by extracting terms and concepts or noun phrases from plain text using linguistic processors such as part-of-speech tagging and phrase chunking. Then statistical or symbolictechniques are used to extract relation signatures, often based on pattern-based or definition-based hypernym extraction techniques."
"http://dbpedia.org/resource/Natural_language_programming"	"Natural language programming"	"Natural language processing"	"Natural Language Programming (NLP) is an ontology-assisted way of programming in terms of natural language sentences, e.g. English. A structured document with Content, sections and subsections for explanations of sentences forms a NLP document, which is actually a computer program. Natural languages and natural language user interfaces include Inform7, a natural programming language for making interactive fiction; Shakespeare, an esoteric natural programming language in the style of the plays of William Shakespeare, and Wolfram Alpha, a computational knowledge engine, using natural language input."
"http://dbpedia.org/resource/Question_answering"	"Question answering"	"Natural language processing"	"Question Answer (Q AND A) is a computer science discipline within the fields of information retrieval and natural language processing (NLP), which is concerned with building systems that automatically answer questions posed by humans in a natural language. A QA implementation, usually a computer program, may construct its answers by querying a structured database of knowledge or information, usually a knowledge base. More commonly, QA systems can pull answers from an unstructured collection of natural language documents. Some examples of natural language document collections used for QA systems include: 
*  a local collection of reference texts 
*  internal organization documents and web pages 
*  compiled newswire reports 
*  a set of Wikipedia pages 
*  a subset of World Wide Web pages QA research attempts to deal with a wide range of question types including: fact, list, definition, How, Why, hypothetical, semantically constrained, and cross-lingual questions. 
*  Closed-domain question answering deals with questions under a specific domain (for example, medicine or automotive maintenance), and can be seen as an easier task because NLP systems can exploit domain-specific knowledge frequently formalized in ontologies. Alternatively, closed-domain might refer to a situation where only a limited type of questions are accepted, such as questions asking for descriptive rather than procedural information. QA systems in the context of machine reading applications have also been constructed in the medical domain, for instance related to Alzheimers disease  
*  Open-domain question answering deals with questions about nearly anything, and can only rely on general ontologies and world knowledge. On the other hand, these systems usually have much more data available from which to extract the answer."
"http://dbpedia.org/resource/Logic_form"	"Logic form"	"Natural language processing"	"Logic forms are simple, first-order logic knowledge representations of natural language sentences formed by the conjunction of concept predicates related through shared arguments. Each noun, verb, adjective, adverb, pronoun, preposition and conjunction generates a predicate. Logic forms can be decorated with word senses to disambiguate the semantics of the word. There are two types of predicates: events are marked with e, and entities are marked with x. The shared arguments connect the subjects and objects of verbs and prepositions together. Example input/output might look like this: Input:  The Earth provides the food we eat every day.Output: Earth:n_#1(x1) provide:v_#2(e1, x1, x2) food:n_#1(x2) we(x3) eat:v_#1(e2, x3, x2; x4) day:n_#1(x4) Logic forms are used in some natural language processing techniques, such as question answering, as well as in inference both for database systems and QA systems."
"http://dbpedia.org/resource/Modular_Audio_Recognition_Framework"	"Modular Audio Recognition Framework"	"Natural language processing"	"Modular Audio Recognition Framework (MARF) is an open-source research platform and a collection of voice, sound, speech, text and natural language processing (NLP) algorithms written in Java and arranged into a modular and extensible framework that attempts to facilitate addition of new algorithms. MARF may act as a library in applications or be used as a source for learning and extension. A few example applications are provided to show how to use the framework. There is also a detailed manual and the API reference in the javadoc format as the project tends to be well documented. MARF, its applications, and the corresponding source code and documentation are released under the BSD-style license."
"http://dbpedia.org/resource/Natural_Language_Toolkit"	"Natural Language Toolkit"	"Natural language processing"	"The Natural Language Toolkit, or more commonly NLTK, is a suite of libraries and programs for symbolic and statistical natural language processing (NLP) for English written in the Python programming language. It was developed by Steven Bird and Edward Loper in the Department of Computer and Information Science at the University of Pennsylvania. NLTK includes graphical demonstrations and sample data. It is accompanied by a book that explains the underlying concepts behind the language processing tasks supported by the toolkit, plus a cookbook. NLTK is intended to support research and teaching in NLP or closely related areas, including empirical linguistics, cognitive science, artificial intelligence, information retrieval, and machine learning.NLTK has been used successfully as a teaching tool, as an individual study tool, and as a platform for prototyping and building research systems.There are 32 universities in the US and 25 countries using NLTK in their courses. NLTK supports classification, tokenization, stemming, tagging, parsing, and semantic reasoning functionalities."
"http://dbpedia.org/resource/Language_technology"	"Language technology"	"Natural language processing"	"Language technology, often called human language technology (HLT), consists of natural language processing (NLP) and computational linguistics (CL) on the one hand, and speech technology on the other. It also includes many application oriented aspects of these. Language technology is closely connected to computer science and general linguistics. In 2013, the Globalization and Localization Association (GALA), released the independent platform LT Advisor for users to search for language technology and review translation tools. The platform was developed in collaboration with the German Research Center for Artificial Intelligence (DFKI). Anywhere the language comes in contact with information technology languages needs and it is organized so that it can be handled and processed by computational means. This often requires broad knowledge not only about linguistics, but also about computer science and related fields."
"http://dbpedia.org/resource/Text_mining"	"Text mining"	"Natural language processing"	"Text mining, also referred to as text data mining, roughly equivalent to , refers to the process of deriving high-quality information from text. High-quality information is typically derived through the devising of patterns and trends through means such as statistical pattern learning. Text mining usually involves the process of structuring the input text (usually parsing, along with the addition of some derived linguistic features and the removal of others, and subsequent insertion into a database), deriving patterns within the structured data, and finally evaluation and interpretation of the output. 'High quality' in text mining usually refers to some combination of relevance, novelty, and interestingness. Typical text mining tasks include text categorization, text clustering, concept/entity extraction, production of granular taxonomies, sentiment analysis, document summarization, and entity relation modeling (i.e., learning relations between named entities). Text analysis involves information retrieval, lexical analysis to study word frequency distributions, pattern recognition, tagging/annotation, information extraction, data mining techniques including link and association analysis, visualization, and predictive analytics. The overarching goal is, essentially, to turn text into data for analysis, via application of natural language processing (NLP) and analytical methods. A typical application is to scan a set of documents written in a natural language and either model the document set for predictive classification purposes or populate a database or search index with the information extracted."
"http://dbpedia.org/resource/Trigram"	"Trigram"	"Natural language processing"	"Trigrams are a special case of the n-gram, where n is 3. They are often used in natural language processing for doing statistical analysis of texts."
"http://dbpedia.org/resource/People_Pattern"	"People Pattern"	"Natural language processing"	"People Pattern is a digital marketing company based in Austin, Texas. The company's text analytics software applications extract facts, relationships and sentiment from unstructured data, which comprise approximately 85% of the information companies store electronically. Corporations that use or have used People Pattern software include McDonalds, Charles Schwab, Campbell's Soup, Discover, Wal-Mart, Nintendo, The University of Texas, and Cisco Systems."
"http://dbpedia.org/resource/Sentence_extraction"	"Sentence extraction"	"Natural language processing"	"Sentence extraction is a technique used for automatic summarization of a text.In this shallow approach, statistical heuristics are used to identify the most salient sentences of a text. Sentence extraction is a low-cost approach compared to more knowledge-intensive deeper approaches which require additional knowledge bases such as ontologies or linguistic knowledge. In short ""sentence extraction"" works as a filter which allows only important sentences to pass. The major downside of applying sentence-extraction techniques to the task of summarization is the loss of coherence in the resulting summary. Nevertheless, sentence extraction summaries can give valuable clues to the main points of a document and are frequently sufficiently intelligible to human readers."
"http://dbpedia.org/resource/Natural_language_user_interface"	"Natural language user interface"	"Natural language processing"	"Natural language user interfaces (LUI or NLUI) are a type of computer human interface where linguistic phenomena such as verbs, phrases and clauses act as UI controls for creating, selecting and modifying data in software applications. In interface design natural language interfaces are sought after for their speed and ease of use, but most suffer the challenges to understanding wide varieties of ambiguous input.Natural language interfaces are an active area of study in the field of natural language processing and computational linguistics. An intuitive general natural language interface is one of the active goals of the Semantic Web. Text interfaces are ""natural"" to varying degrees. Many formal (un-natural) programming languages incorporate idioms of natural human language. Likewise, a traditional keyword search engine could be described as a ""shallow"" natural language user interface."
"http://dbpedia.org/resource/Automated_essay_scoring"	"Automated essay scoring"	"Natural language processing"	"Automated essay scoring (AES) is the use of specialized computer programs to assign grades to essays written in an educational setting. It is a method of educational assessment and an application of natural language processing. Its objective is to classify a large set of textual entities into a small number of discrete categories, corresponding to the possible grades—for example, the numbers 1 to 6. Therefore, it can be considered a problem of statistical classification. Several factors have contributed to a growing interest in AES. Among them are cost, accountability, standards, and technology. Rising education costs have led to pressure to hold the educational system accountable for results by imposing standards. The advance of information technology promises to measure educational achievement at reduced cost. The use of AES for high-stakes testing in education has generated significant backlash, with opponents pointing to research that computers cannot yet grade writing accurately and arguing that their use for such purposes promotes teaching writing in reductive ways (i.e. teaching to the test)."
"http://dbpedia.org/resource/Dragomir_R._Radev"	"Dragomir R. Radev"	"Natural language processing"	"Dragomir R. Radev is a University of Michigan computer science professor and Columbia University computer science adjunct professor working on natural language processing and information retrieval. He is currently working on the fields of open domain question answering, multi-document summarization, and the application of NLP in Bioinformatics and Political Science. Radev received his PhD in Computer Science from Columbia University in 1999. He is the secretary of ACL (2006–present) and associate editor of JAIR."
"http://dbpedia.org/resource/METEOR"	"METEOR"	"Natural language processing"	"METEOR (Metric for Evaluation of Translation with Explicit ORdering) is a metric for the evaluation of machine translation output. The metric is based on the harmonic mean of unigram precision and recall, with recall weighted higher than precision. It also has several features that are not found in other metrics, such as stemming and synonymy matching, along with the standard exact word matching. The metric was designed to fix some of the problems found in the more popular BLEU metric, and also produce good correlation with human judgement at the sentence or segment level. This differs from the BLEU metric in that BLEU seeks correlation at the corpus level. Results have been presented which give correlation of up to 0.964 with human judgement at the corpus level, compared to BLEU's achievement of 0.817 on the same data set. At the sentence level, the maximum correlation with human judgement achieved was 0.403."
"http://dbpedia.org/resource/Sentiment_analysis"	"Sentiment analysis"	"Natural language processing"	"Sentiment analysis (also known as opinion mining) refers to the use of natural language processing, text analysis and computational linguistics to identify and extract subjective information in source materials. Sentiment analysis is widely applied to reviews and social media for a variety of applications, ranging from marketing to customer service. Generally speaking, sentiment analysis aims to determine the attitude of a speaker or a writer with respect to some topic or the overall contextual polarity of a document. The attitude may be his or her judgment or evaluation (see appraisal theory), affective state (that is to say, the emotional state of the author when writing), or the intended emotional communication (that is to say, the emotional effect the author wishes to have on the reader)."
"http://dbpedia.org/resource/Teragram_Corporation"	"Teragram Corporation"	"Natural language processing"	"Teragram Corporation is a fully owned subsidiary of SAS Institute, a major producer of statistical analysis software, headquartered in Cary, North Carolina, USA. Teragram is based in Cambridge, Massachusetts and specializes in the application of computational linguistics to multilingual natural language processing. Teragram's technology is licensed to public search engines such as Ask.com and Yahoo!, to media companies such as the New York Times and the Tribune Company, and to original equipment manufacturer (OEM) customers such as Fast Search & Transfer and Verity. Teragram was founded by Emmanuel Roche and Yves Schabes in 1997 and acquired by SAS Institute in 2008 Knowledge management and digital content trade magazines have named it one of the top 100 companies in each of those fields. Its major competitor is Inxight."
"http://dbpedia.org/resource/Document-term_matrix"	"Document-term matrix"	"Natural language processing"	"A document-term matrix or term-document matrix is a mathematical matrix that describes the frequency of terms that occur in a collection of documents. In a document-term matrix, rows correspond to documents in the collection and columns correspond to terms. There are various schemes for determining the value that each entry in the matrix should take. One such scheme is tf-idf. They are useful in the field of natural language processing."
"http://dbpedia.org/resource/Bag-of-words_model"	"Bag-of-words model"	"Natural language processing"	"The bag-of-words model is a simplifying representation used in natural language processing and information retrieval (IR). In this model, a text (such as a sentence or a document) is represented as the bag (multiset) of its words, disregarding grammar and even word order but keeping multiplicity. The bag-of-words model has also been used for computer vision. The bag-of-words model is commonly used in methods of document classification where the (frequency of) occurrence of each word is used as a feature for training a classifier. An early reference to ""bag of words"" in a linguistic context can be found in Zellig Harris's 1954 article on Distributional Structure."
"http://dbpedia.org/resource/Concept_mining"	"Concept mining"	"Natural language processing"	"Concept mining is an activity that results in the extraction of concepts from artifacts. Solutions to the task typically involve aspects of artificial intelligence and statistics, such as data mining and text mining. Because artifacts are typically a loosely structured sequence of words and other symbols (rather than concepts), the problem is nontrivial, but it can provide powerful insights into the meaning, provenance and similarity of documents."
"http://dbpedia.org/resource/Semantic_analytics"	"Semantic analytics"	"Natural language processing"	"Semantic analytics is the use of ontologies to analyze content in web resources. This field of research combines text analytics and Semantic Web technologies like RDF. Some academic research groups that have active project in this area include Kno.e.sis center at Wright State University among others."
"http://dbpedia.org/resource/Linguistic_Issues_in_Language_Technology"	"Linguistic Issues in Language Technology"	"Natural language processing"	"Linguistic Issues in Language Technology (LiLT) is an open-access journal that, according to its web page, ""focusses on relationships between linguistic insights, which can prove valuable to language technology, and language technology, which can enrich linguistic research"" [1]. It is published by CSLI Publications."
"http://dbpedia.org/resource/OpenNLP"	"OpenNLP"	"Natural language processing"	"The Apache OpenNLP library is a machine learning based toolkit for the processing of natural language text. It supports the most common NLP tasks, such as tokenization, sentence segmentation, part-of-speech tagging, named entity extraction, chunking, parsing, and coreference resolution. These tasks are usually required to build more advanced text processing services."
"http://dbpedia.org/resource/Stemming"	"Stemming"	"Natural language processing"	"In linguistic morphology and information retrieval, stemming is the process of reducing inflected (or sometimes derived) words to their word stem, base or root form—generally a written word form. The stem need not be identical to the morphological root of the word; it is usually sufficient that related words map to the same stem, even if this stem is not in itself a valid root. Algorithms for stemming have been studied in computer science since the 1960s. Many search engines treat words with the same stem as synonyms as a kind of query expansion, a process called conflation. Stemming programs are commonly referred to as stemming algorithms or stemmers."
"http://dbpedia.org/resource/Frederick_J._Damerau"	"Frederick J. Damerau"	"Natural language processing"	"Frederick J. Damerau (December 25, 1931 – January 27, 2009) was a pioneer of research on natural language processing and data mining. After earning his B.A. from Cornell University in 1953, he spent most of his career at IBM, in the Thomas J. Watson Research Center. One of his most influential and ground-breaking papers was ""A technique for computer detection and correction of spelling errors"" published in 1964. He also developed and patented for IBM the first algorithm for placing hyphens automatically in words.In 1971 he published the book ""Markov Models and Linguistic Theory : An Experimental Study of a Model for English."" After being active in research for over four decades, Fred Damerau died on January 27, 2009."
"http://dbpedia.org/resource/Example-based_machine_translation"	"Example-based machine translation"	"Natural language processing"	"Example-based machine translation (EBMT) is a method of machine translation often characterized by its use of a bilingual corpus with parallel texts as its main knowledge base at run-time. It is essentially a translation by analogy and can be viewed as an implementation of a case-based reasoning approach to machine learning."
"http://dbpedia.org/resource/LRE_Map"	"LRE Map"	"Natural language processing"	"The LRE Map (Language Resources and Evaluation) is a freely accessible large database on resources dedicated to Natural language processing (NLP). The original feature of LRE Map is that the records are collected during the submission of different major NLP conferences. The records are then cleaned and gathered into a global database called ""LRE Map"". The LRE Map is intended to be an instrument for collecting information about language resourcesand to become, at the same time, a community for users, a place to share and discover resources,discuss opinions, provide feedback, discover new trends, etc. It is an instrument for discovering, searching and documenting language resources, here intended in a broad sense, as both data and tools. The large amount of information contained in the Map can be analyzed in many different ways. Afew, general analyses are available on the Resource Map website at (click on the “Show(Hide) Quick Pies” link). For instance, the LRE Map can provide information about the most frequent type of resource, themost represented language, the applications for which resources are used or are being developed,the proportion of new resources vs. already existing ones, or the way in which resources aredistributed to the community."
"http://dbpedia.org/resource/Classic_monolingual_word-sense_disambiguation"	"Classic monolingual word-sense disambiguation"	"Natural language processing"	"Classic monolingual Word Sense Disambiguation evaluation tasks uses WordNet as its sense inventory and is largely based on supervised / semi-supervised classification with the manually sense annotated corpora: 
* Classic English WSD uses the Princeton WordNet as it sense inventory and the primary classification input is normally based on the SemCor corpus. 
* Classical WSD for other languages uses their respective WordNet as sense inventories and sense annotated corpora tagged in their respective languages. Often researchers will also tapped on the SemCor corpus and aligned bitexts with English as its source language"
"http://dbpedia.org/resource/Lexical_substitution"	"Lexical substitution"	"Natural language processing"	"Lexical substitution is the task of identifying a substitute for a word in the context of a clause. For instance, given the following text: ""After the match, replace any remaining ﬂuid deﬁcit to prevent chronic dehydration throughout the tournament"", a substitute of game might be given. Lexical substitution is strictly related to word sense disambiguation (WSD), in that both aim to determine the meaning of a word. However, while WSD consists of automatically assigning the appropriate sense from a fixed sense inventory, lexical substitution does not impose any constraint on which substitute to choose as the best representative for the word in context. By not prescribing the inventory, lexical substitution overcomes the issue of the granularity of sense distinctions and provides a level playing field for automatic systems that automatically acquire word senses (a task referred to as Word Sense Induction)."
"http://dbpedia.org/resource/Keyword_extraction"	"Keyword extraction"	"Natural language processing"	"Keyword extraction is tasked with the automatic identification of terms that best describe the subject of a document. Key phrases, key terms, key segments or just keywords are the terminology which is used for defining the terms that represent the most relevant information contained in the document. Although the terminology is different, function is the same: characterization of the topic discussed in a document. Keyword extraction task is important problem in Text Mining, Information Retrieval and Natural Language Processing."
"http://dbpedia.org/resource/ClearForest"	"ClearForest"	"Natural language processing"	"ClearForest  was a software company that developed and marketed text analytics and text mining solutions. Founded in 1998, ClearForest had its headquarters just outside Boston and had development in Israel near Tel Aviv. It was acquired by Reuters in April, 2007. It now commercializes its services under the names Calais, OpenCalais, and OneCalais. ClearForest was previously venture-backed; its last funding round was led by Greylock Ventures and closed in 2005. Other investors included DB Capital Partners, Pitango, Walden Israel, Booz Allen, JP Morgan Partners and HarbourVest Partners."
"http://dbpedia.org/resource/N-gram"	"N-gram"	"Natural language processing"	"In the fields of computational linguistics and probability, an n-gram is a contiguous sequence of n items from a given sequence of text or speech. The items can be phonemes, syllables, letters, words or base pairs according to the application. The n-grams typically are collected from a text or speech corpus. When the items are words, n-grams may also be called shingles. An n-gram of size 1 is referred to as a ""unigram""; size 2 is a ""bigram"" (or, less commonly, a ""digram""); size 3 is a ""trigram"". Larger sizes are sometimes referred to by the value of n, e.g., ""four-gram"", ""five-gram"", and so on."
"http://dbpedia.org/resource/Text_simplification"	"Text simplification"	"Natural language processing"	"Text simplification is an operation used in natural language processing to modify, enhance, classify or otherwise process an existing corpus of human-readable text in such a way that the grammar and structure of the prose is greatly simplified, while the underlying meaning and information remains the same. Text simplification is an important area of research, because natural human languages ordinarily contain complex compound constructions that are not easily processed through automation. In terms of reducing language diversity, semantic compression can be employed to limit and simplify a set of words used in given texts."
"http://dbpedia.org/resource/TipTop_Technologies"	"TipTop Technologies"	"Natural language processing"	"TipTop Technologies offers a real-time web, social search engine with a unique platform for semantic analysis of natural language. TipTop Search provides results capturing individual and group sentiment, opinions, and experiences from content of various sorts including real-time messages from Twitter or consumer product reviews on Amazon.com. TipTop Technologies and ITC Infotech have worked together to develop a semantic engine and search interface for both enterprise and consumer applications. TipTop's products are part of the ""emerging Web 3.0 applications which use semantic technologies to augment the underlying Web system's functionalities."" Through semantic analysis of large data sets, TipTop gleaned behavioral insights from Tweets around events like Halloween, Thanksgiving, Holiday Gifting, the Super Bowl and the Academy Awards: 2010 Oscar Nominees coverage. Sentiment analysis, concept trend tracking and real-time market research are other applications included in the TipTop Search product. TipTop's insight engine solves the problem of real-time data noise, and its ability ""to sort the “good tweets” from the “bad tweets” when it comes to a product, service or a region..."" In addition, products like TipTop Shopping with customizable search widgets, bring together consumer reviews, social search, and sentiment analysis enabling product comparisons across attributes like overall value and aiding purchasing decisions through user-driven product tips and pits. TipTop Finance adds another complexity to real-time search results by incorporating corporate sentiment, company stock tickers and social media into TipTop's existing social search platform. Additional success applying semantic technologies has been with polling, ""if you compare these Gallup  results with TipTop, a sentiment engine based on Twitter, the results are not way off. It does surprise you but it tells me that sentiment analysis in case of public opinion about a burning social issue or a famous personality is relatively easier."". With the increasing amount of unstructured, opinion-oriented, and user-generated content available on the Web, TipTop's technology aims to make sense of all this data, and deliver it in a useful way for consumer and enterprise users alike. For more insight into TipTop founder Shyam Kapur's inspiration for building semantic-based products and the vision for using semantic technologies in new and creative ways read the Semantic Web article, What's The Inspiration for Semantic Web Innovation?. TipTop Technologies is a privately held company headquartered in Silicon Valley, California with team members located globally."
"http://dbpedia.org/resource/Noisy_text_analytics"	"Noisy text analytics"	"Natural language processing"	"Noisy text analytics is a process of information extraction whose goal is to automatically extract structured or semistructured information from noisy unstructured text data. While Text analytics is a growing and mature field that has great value because of the huge amounts of data being produced, processing of noisy text is gaining in importance because a lot of common applications produce noisy text data. Noisy unstructured text data is found in informal settings such as online chat, text messages, e-mails, message boards, newsgroups, blogs, wikis and web pages. Also, text produced by processing spontaneous speech using automatic speech recognition and printed or handwritten text using optical character recognition contains processing noise. Text produced under such circumstances is typically highly noisy containing spelling errors, abbreviations, non-standard words, false starts, repetitions, missing punctuations, missing letter case information, pause filling words such as “um” and “uh” and other texting and speech disfluencies. Such text can be seen in large amounts in contact centers, chat rooms, optical character recognition (OCR) of text documents, short message service (SMS) text, etc. Documents with historical language can also be considered noisy with respect to today’s knowledge about the language. Such text contains important historical, religious, ancient medical knowledge that is useful. The nature of the noisy text produced in all these contexts warrants moving beyond traditional text analysis techniques."
"http://dbpedia.org/resource/NooJ"	"NooJ"	"Natural language processing"	"NooJ is a linguistic development environment software as well as a corpus processor constructed by Max Silberztein. NooJ allows linguists to construct the four classes of the Chomsky-Schützenberger hierarchy of generative grammars:  Finite-State Grammars,  Context-Free Grammars,  Context-Sensitive Grammars as well as  Unrestricted Grammars, using either a text editor (e.g. to write down regular expressions), or a Graph editor. NooJ allows linguists to develop orthographical and morphological grammars, dictionaries of simple words, of compound words as well as discontinuous expressions, local syntactic grammars (such as  Named Entities Recognizers), structural syntactic grammars (that produce syntactic trees) as well as Zellig Harris‘  transformational grammars. All NooJ parsers process Atomic Linguistic Units (ALUs), as opposed to word forms (i.e. sequences of letters between two space characters). This allows NooJ’s syntactic parser to parse sequences of word forms such as “can not” exactly as contracted word forms such as “cannot” or “can’t”. This allows linguists to write relatively simple syntactic grammars, even for agglutinative languages. ALUs are represented by annotations that are stored in the Text Annotation Structure (or TAS): all NooJ parsers add, or remove annotations in the TAS. A typical NooJ analysis involves applying to a text a series of elementary grammars in cascade, in a bottom-up approach (from spelling to semantics)."
"http://dbpedia.org/resource/Averbis"	"Averbis"	"Natural language processing"	"Averbis has a focus on healthcare, pharma, automotive and intellectual property analytics. Averbis is involved in various research projects of the German Federal Ministry of Economics and Energy and the European Union such as DebugIT, EUCases, Mantra  and SEMCARE. In addition to these projects, Averbis was also involved in the following projects: Greenpilot is a virtual library, which provides technical information in the fields of nutrition, environment and agriculture.Medpilot is a virtual library, which provides informations about medicine and related sciences. In 2013, Averbis has been nominated for the German Founder Prize 2013. Averbis GmbH provides text analytics and text mining software to transform unstructured text into actionable information. It was founded in 2007 by IT experts after years of relevant scientific experience in the field of text mining and multilingual information retrieval. Averbis works in the field of terminology management, natural language processing, machine learning and semantic search. Its text mining software is embedded into the text mining framework UIMA."
"http://dbpedia.org/resource/Deeplearning4j"	"Deeplearning4j"	"Natural language processing"	"Deeplearning4j is a deep learning programming library written for Java and the Java virtual machine (JVM) and a computing framework with wide support for deep learning algorithms. Deeplearning4j includes implementations of the restricted Boltzmann machine, deep belief net, deep autoencoder, stacked denoising autoencoder and recursive neural tensor network, word2vec, doc2vec, and GloVe. These algorithms all include distributed parallel versions that integrate with Apache Hadoop and Spark. Deeplearning4j is open-source software released under Apache License 2.0, developed mainly by a machine learning group in San Francisco led by Adam Gibson. It is supported commercially by the startup Skymind. It is the only open-source project listed on Google's Word2vec page for its Java implementation."
"http://dbpedia.org/resource/LIVAC_Synchronous_Corpus"	"LIVAC Synchronous Corpus"	"Natural language processing"	"LIVAC is an uncommon language corpus dynamically maintained since 1995. Different from other existing corpora, LIVAC has adopted a rigorous and regular as well as ""Windows"" approach in processing and filtering massive media texts from representative communities in the Pan-Chinese region including Hong Kong, Macau, Taipei, Singapore, Shanghai, Beijing, Guangzhou, and Shenzhen. The contents are thus deliberately repetitive in most cases, represented by textual samples drawn from editorials, local and international news, cross-Formosan Straits news, as well as news on finance, sports and entertainment. By 2014, more than 550 million characters of news media texts have been processed and analyzed and have yielded an expanding Pan-Chinese dictionary of 1.7 million words from the Pan-Chinese printed media. Through rigorous analysis based on computational methodology, LIVAC has at the same time accumulated a large amount of accurate and meaningful statistical data on the Chinese language and their speech communities in the Pan-Chinese region, and the result shows considerable and important variation. The ""Windows"" approach is the most representative feature of LIVAC and has enabled Chinese media texts from the Pan-Chinese context to be quantitatively analyzed according to various attributes such as locations, time and subject domains. Thus, various types of comparative studies and applications in information technology as well as development of related innovative applications have been possible. Moreover, LIVAC has allowed longitudinal development to be taken into account, facilitating Key Word in Context (KWIC) and comprehensive study of target words and their underlying concepts as well as linguistic structures over 19 years, based on variables such as specifications of region, duration and content domain. Results from the extensive and accumulative data analysis contained in LIVAC have enabled the cultivation of textual databases of proper names, places, organizations, new words, and bi-weekly and annual rosters of media figures. Related applications have included the establishment of verb and adjective lexicons, the formulation of sentiment indices, and related opinion mining, to measure and compare the popularity of global media figures in the Chinese media (LIVAC Annual Pan-Chinese Celebrity Rosters, later renamed as Pan-Chinese Media Personalities Rosters) and construction of monthly new word lexicons (LIVAC Annual Pan-Chinese New Word Rosters). On this basis, the analysis of the emergence, diffusion and transformation of new words, and the publication of dictionaries of neologisms have been made possible."
"http://dbpedia.org/resource/Explicit_semantic_analysis"	"Explicit semantic analysis"	"Natural language processing"	"In natural language processing and information retrieval, explicit semantic analysis (ESA) is a vectorial representation of text (individual words or entire documents) that uses a document corpus as a knowledge base. Specifically, in ESA, a word is represented as a column vector in the tf–idf matrix of the text corpus and a document (string of words) is represented as the centroid of the vectors representing its words. Typically, the text corpus is Wikipedia, though other corpora including the Open Directory Project have been used. ESA was designed by Evgeniy Gabrilovich and Shaul Markovitch as a means of improving text categorizationand has been used by this pair of researchers to compute what they refer to as ""semantic relatedness"" by means of cosine similarity between the aforementioned vectors, collectively interpreted as a space of ""concepts explicitly defined and described by humans"", where Wikipedia articles (or ODP entries, or otherwise titles of documents in the knowledge base corpus) are equated with concepts.The name ""explicit semantic analysis"" contrasts with latent semantic analysis (LSA), because the use of a knowledge base makes it possible to assign human-readable labels to the concepts that make up the vector space."
"http://dbpedia.org/resource/MAREC"	"MAREC"	"Natural language processing"	"The MAtrixware REsearch Collection (MAREC) is a standardised patent data corpus available for research purposes. MAREC seeks to represent patent documents of several languages in order to answer specific research questions. It consists of 19 million patent documents in different languages, normalised to a highly specific XML schema. MAREC is intended as raw material for research in areas such as information retrieval, natural language processing or machine translation, which require large amounts of complex documents. The collection contains documents in 19 languages, the majority being English, German and French, and about half of the documents include full text. In MAREC, the documents from different countries and sources are normalised to a common XML format with a uniform patent numbering scheme and citation format. The standardised fields include dates, countries, languages, references, person names, and companies as well as subject classifications such as IPC codes. MAREC is a comparable corpus, where many documents are available in similar versions in other languages. A comparable corpus can be defined as consisting of texts that share similar topics – news text from the same time period in different countries, while a parallel corpus is defined as a collection of documents with aligned translations from the source to the target language. Since the patent document refers to the same “invention” or “concept of idea” the text is a translation of the invention, but it does not have to be a direct translation of the text itself – text parts could have been removed or added for clarification reasons. The 19,386,697 XML files measure a total of 621 GB and are hosted by the Information Retrieval Facility. Access and support are free of charge for research purposes."
"http://dbpedia.org/resource/Grammatik"	"Grammatik"	"Natural language processing"	"Grammatik was the first grammar checking program developed for home computer systems. Aspen Software of Albuquerque, NM, released the earliest version of this diction and style checker for personal computers, in 1981. Grammatik was first available for a Radio Shack - TRS-80, and soon had versions for CP/M and the IBM PC. Reference Software of San Francisco, CA, acquired Grammatik in 1985. Development of Grammatik continued, and it became an actual grammar checker that could detect writing errors beyond simple style checking. Subsequent versions were released for the DOS, Windows, Macintosh and Unix platforms. Grammatik was ultimately acquired by Corel and is integrated in the WordPerfect word processor."
"http://dbpedia.org/resource/Semantic_neural_network"	"Semantic neural network"	"Natural language processing"	"Semantic neural network (SNN) is based on John von Neumann's neural network [von Neumann, 1966] and Nikolai Amosov M-Network. There are limitations to a link topology for the von Neumann’s network but SNN accept a case without these limitations. Only logical values can be processed, but SNN accept that fuzzy values can be processed too. All neurons into the von Neumann network are synchronized by tacts. For further use of self-synchronizing circuit technique SNN accepts neurons can be self-running or synchronized. In contrast to the von Neumann network there are no limitations for topology of neurons for semantic networks. It leads to the impossibility of relative addressing of neurons as it was done by von Neumann. In this case an absolute readdressing should be used. Every neuron should have a unique identifier that would provide a direct access to another neuron. Of course, neurons interacting by axons-dendrites should have each other's identifiers. An absolute readdressing can be modulated by using neuron specificity as it was realized for biological neural networks. There’s no description for self-reflectiveness and self-modification abilities into the initial description of semantic networks [Dudar Z.V., Shuklin D.E., 2000]. But in [Shuklin D.E. 2004] a conclusion had been drawn about the necessity of introspection and self-modification abilities in the system. For maintenance of these abilities a concept of pointer to neuron is provided. Pointers represent virtual connections between neurons. In this model, bodies and signals transferring through the neurons connections represent a physical body, and virtual connections between neurons are representing an astral body. It is proposed to create models of artificial neuron networks on the basis of virtual machine supporting the opportunity for paranormal effects. SNN is generally used for natural language processing."
"http://dbpedia.org/resource/History_of_natural_language_processing"	"History of natural language processing"	"Natural language processing"	"The history of natural language processing describes the advances of natural language processing (Outline of natural language processing). There is some overlap with the history of machine translation and the history of artificial intelligence."
"http://dbpedia.org/resource/Temporal_annotation"	"Temporal annotation"	"Natural language processing"	"Temporal annotation is the study of how to automatically add semantic information regarding time to natural language documents. It plays a role in natural language processing and computational linguistics."
"http://dbpedia.org/resource/News_analytics"	"News analytics"	"Natural language processing"	"News analysis refers to the measurement of the various qualitative and quantitative attributes of textual (unstructured data) news stories. Some of these attributes are: sentiment, relevance, and novelty. Expressing news stories as numbers and metadata permits the manipulation of everyday information in a mathematical and statistical way. News analytics are used in financial modeling, particularly in quantitative and algorithmic trading. Further, news analytics can be used to plot and characterize firm behaviors over time and thus yield important strategic insights about rival firms. News analytics are usually derived through automated text analysis and applied to digital texts using elements from natural language processing and machine learning such as latent semantic analysis, support vector machines, ""bag of words"" among other techniques."
"http://dbpedia.org/resource/Cache_language_model"	"Cache language model"	"Natural language processing"	"A cache language model is a type of statistical language model. These occur in the natural language processing subfield of computer science and assign probabilities to given sequences of words by means of a probability distribution. Statistical language models are key components of speech recognition systems and of many machine translation systems: they tell such systems which possible output word sequences are probable and which are improbable. The particular characteristic of a cache language model is that it contains a cache component and assigns relatively high probabilities to words or word sequences that occur elsewhere in a given text. The primary, but by no means sole, use of cache language models is in speech recognition systems. To understand why it is a good idea for a statistical language model to contain a cache component one might consider someone who is dictating a letter about elephants to a speech recognition system. Standard (non-cache) N-gram language models will assign a very low probability to the word ""elephant"" because it is a very rare word in English. If the speech recognition system does not contain a cache component the person dictating the letter may be annoyed: each time the word ""elephant"" is spoken another sequence of words with a higher probability according to the N-gram language model may be recognized (e.g., ""tell a plan""). These erroneous sequences will have to be deleted manually and replaced in the text by ""elephant"" each time ""elephant"" is spoken. If the system has a cache language model, ""elephant"" will still probably be misrecognized the first time it is spoken and will have to be entered into the text manually; however, from this point on the system is aware that ""elephant"" is likely to occur again – the estimated probability of occurrence of ""elephant"" has been increased, making it more likely that if it is spoken it will be recognized correctly. Once ""elephant"" has occurred several times the system is likely to recognize it correctly every time it is spoken until the letter has been completely dictated. This increase in the probability assigned to the occurrence of ""elephant"" is an example of a consequence of machine learning and more specifically of pattern recognition. There exist variants of the cache language model in which not only single words but also multi-word sequences that have occurred previously are assigned higher probabilities (e.g., if ""San Francisco"" occurred near the beginning of the text subsequent instances of it would be assigned a higher probability). The cache language model was first proposed in a paper published in 1990, after which the IBM speech-recognition group experimented with the concept. The group found that implementation of a form of cache language model yielded a 24% drop in word-error rates once the first few hundred words of a document had been dictated. A detailed survey of language modeling techniques concluded that the cache language model was one of the few new language modeling techniques that yielded improvements over the standard N-gram approach: ""Our caching results show that caching is by far the most useful technique for perplexity reduction at small and medium training data sizes"". The development of the cache language model has generated considerable interest among those concerned with computational linguistics in general and statistical natural language processing in particular: recently there has been interest in applying the cache language model in the field of statistical machine translation. The success of the cache language model in improving word prediction rests on the human tendency to use words in a ""bursty"" fashion: when one is discussing a certain topic in a certain context the frequency with which one uses certain words will be quite different from their frequencies when one is discussing other topics in other contexts. The traditional N-gram language models, which rely entirely on information from a very small number (four, three, or two) of words preceding the word to which a probability is to be assigned, do not adequately model this ""burstiness""."
"http://dbpedia.org/resource/Discourse_relation"	"Discourse relation"	"Natural language processing"	"A discourse relation (or rhetorical relation) is a description of how two segments of discourse are logically connected to one another. One method of modeling discourse involves a set of concepts that constitute ""segmented discourse representation theory"" (SDRT)."
"http://dbpedia.org/resource/DELPH-IN"	"DELPH-IN"	"Natural language processing"	"DEep Linguistic Processing with HPSG - INitiative (DELPH-IN) is a collaboration where computational linguists worldwide develop natural language processing tools for deep linguistic processing of human language. The goal of DELPH-IN is to combine linguistic and statistical processing methods in order to computationally understand the meaning of texts and utterances. The tools developed by DELPH-IN adopts two linguistics formalisms for deep linguistic analysis, viz. head-driven phrase structure grammar (HPSG) and minimal recursion semantics (MRS). All tools under the DELPH-IN collaboration are developed for general use of open-source licensing. Since 2005, DELPH-IN has held an annual Summit. This is a loosely structured unconference where people update each other about the work they are doing, seek feedback on current work, and occasionally hammer out agreement on standards and best practice."
"http://dbpedia.org/resource/Lexical_choice"	"Lexical choice"	"Natural language processing"	"Lexical choice is the subtask of Natural language generation that involves choosing the content words (nouns, verbs, adjectives, and adverbs) in a generated text. Function words (determiners, for example) are usually chosen during realisation."
"http://dbpedia.org/resource/Referring_expression_generation"	"Referring expression generation"	"Natural language processing"	"Referring expression generation(REG) is the subtask of Natural language generation (NLG) that received most scholarly attention. While NLG is concerned with the conversion of non-linguistic information into natural language, REG focuses only on the creation of referring expressions (noun phrases) that identify specific entities called targets. This task can be split into two sections. The content selection part determines which set of properties distinguish the intended target and the linguistic realization part defines how these properties are translated into natural language.A variety of algorithms have been developed in the NLG community to generate different types of referring expressions."
"http://dbpedia.org/resource/ETBLAST"	"ETBLAST"	"Natural language processing"	"eTBLAST is a free text similarity service search engine currently offering access to the MEDLINE database, the National Institutes of Health (NIH) CRISP database, the Institute of Physics (IOP) database, Wikipedia, arXiv, the NASA technical reports database, Virginia Tech class descriptions and a variety of databases of clinical interest. It is continuously expanding with additional text-based databases. eTBLAST searches citation databases and databases containing full text, such as PUBMED. The eTBLAST server compares a user's natural text query to target databases using a hybrid search algorithm consisting of a low-sensitivity weighted keyword-based first pass followed by a novel sentence-alignment based second pass. eTBLAST is a free web-based service of The Innovation Laboratory at the Virginia Bioinformatics Institute. eTBLAST, as a text similarity engine, made possible a large study of duplicate publications and potential plagiarisms in the biomedical literature. Thousands of random samples of Medline abstracts were submitted to eTBLAST, and those with the highest similarity were studied and entered into an on-line database. This study is on-going, with the database maturing as the entries are manually inspected and classified. This work revealed several trends, including an increasing rate of duplication in the biomedical literature, as reported in the journals Bioinformatics, Anaesthesia and Intensive Care, Clinical Chemistry, Urologic Oncology, Nature, and Science."
"http://dbpedia.org/resource/Aggregation_(linguistics)"	"Aggregation (linguistics)"	"Natural language processing"	"Aggregation is a subtask of natural language generation, which involves merging syntactic constituents (such as sentences and phrases) together. Sometimes aggregation can be done at a conceptual level."
"http://dbpedia.org/resource/Entity_linking"	"Entity linking"	"Natural language processing"	"In natural language processing, entity linking, named entity linking (NEL), named entity disambiguation (NED), named entity recognition and disambiguation (NERD) or named entity normalization (NEN) is the task of determining the identity of entities mentioned in text. For example, given the sentence ""Paris is the capital of France"", the idea is to determine that ""Paris"" refers to the city of Paris and not to Paris Hilton or any other entity that could be referred as ""Paris"". NED is different from named entity recognition (NER) in that NER identifies the occurrence or mention of a named entity in text but it does not identify which specific entity it is. Entity linking requires a knowledge base containing the entities to which entity mentions can be linked. A popular choice for entity linking on open domain text are knowledge-bases based on Wikipedia, in which each page is regarded as a named entity. NED using Wikipedia entities has been also called wikification (see Wikify! an early entity linking system ). A knowledge base may also be induced automatically from training text or manually built. Named entity mentions can be highly ambiguous, any entity linking method must address this inherent ambiguity. Various approaches to tackle this problem have been tried to date. In the seminal approach of Milne and Witten, supervised learning is employed using the anchor texts of Wikipedia entities as training data. Kulkarni et al. exploited the common property that topically coherent documents refer to entities belonging to strongly related types. Other approaches also collected training data based on unambiguous synonyms. More recent systems for NED include AIDA, AGDISTIS, Babelfy and TagMe. Entity linking has been used to improve the performance of information retrieval systems and to improve search performance on digital libraries. NED is also a key input for Semantic Search. Entity Linking evaluation campaigns are organized by the U.S. National Institute of Standards and Technology (NIST) in the context of the Knowledge Base Population task of the Text Analysis Conference."
"http://dbpedia.org/resource/Gorn_address"	"Gorn address"	"Natural language processing"	"A Gorn address (Gorn, 1967) is a method of identifying and addressing any interior node within a tree data structure from a phrase structure rule description or parse tree. The Gorn address is a string made up of a series of one or more integers separated by dots, e.g., 0 or 0.0.1. The -th child of the -th node has an address . It is named after American computer scientist Saul Gorn. "
"http://dbpedia.org/resource/Word-sense_induction"	"Word-sense induction"	"Natural language processing"	"In computational linguistics, word-sense induction (WSI) or discrimination is an open problem of natural language processing, which concerns the automatic identification of the senses of a word (i.e. meanings). Given that the output of word-sense induction is a set of senses for the target word (sense inventory), this task is strictly related to that of word-sense disambiguation (WSD), which relies on a predefined sense inventory and aims to solve the ambiguity of words in context."
"http://dbpedia.org/resource/Automatic_acquisition_of_lexicon"	"Automatic acquisition of lexicon"	"Natural language processing"	"Automatic acquisition of lexicon is a computerized process used for the development of a complex morphological lexicon of a language. The lexicon is essential for the NLP (Natural language processing), as well as a prerequisite to any wide-coverage parser.The two main requirements represent raw corpus and the morphological description of the language. The aim is to provide lemmas that will serve to the explanation of all the words that occur within the corpus. For the achievement of a quality lexicon it is necessary to manually validate the generated lemmas and iterate the whole process several times.The process is focused on the open word classes (e.g. nouns, adjectives, verbs). Closed classes (e.g. prepositions, pronouns, numerals) are excluded.This method is applicable to the languages with a rich morphology, such as Slovak, Russian or Croatian. Applied to Slovak, being an inflectional language, the automatic acquisition focuses on the inflectional morphology as well as on the derivational morphology. This fact enables the users to find out the information about derivational relations (e.g. adjectivizations, prefixes) in the lexicon. For example, Slovak word korpusový is an adjectivization of korpus (eng. corpus)."
"http://dbpedia.org/resource/BulSemCor"	"BulSemCor"	"Natural language processing"	"The Bulgarian Sense-annotated Corpus (BulSemCor) (in Bulgarian: Български семантично анотиран корпус (БулСемКор)) is a structured corpus of Bulgarian texts in which each lexical item is assigned a sense tag. BulSemCor was created by the Department of Computational Linguistics at the Institute for Bulgarian Language of the Bulgarian Academy of Sciences."
"http://dbpedia.org/resource/AUTINDEX"	"AUTINDEX"	"Natural language processing"	"AUTINDEX is a commercial text mining software package based on sophisticated linguistics. AUTINDEX resulting from research in information extraction  is a product of the Institute of Applied Information Sciences (IAI) which is a non-profit institute that has been researching and developing language technology since its foundation in 1985. IAI is an institute affiliated to Saarland University in Saarbrücken, Germany. AUTINDEX is the result of a number of research projects funded by the EU (Project BINDEX ), by Deutsche Forschungsgemeinschaft and the German Ministry for Economy. Amongst the latter there are the projects LinSearch  and WISSMER, see also the reference to IAI-Webite. The basic functionality of AUTINDEX is the extraction of key words from a document to represent the semantics of the document. Ideally the system is integrated with a thesaurus that defines the standardised terms to be used for key word assignment. AUTINDEX is used in library applications (e.g. integrated in dandelon.com) as well as in high quality (expert) information systems  and in document management and content management environments. Together with AUTINDEX a number of additional software comes along such as an integration with Apache Solr / Lucene to provide a complete information retrieval environment, a classification and categorisation system on the basis of a machine learning  software that assigns domains to the document, and a system for searching with semantically similar terms that are collected in so called tag clouds."
"http://dbpedia.org/resource/LanguageWare"	"LanguageWare"	"Natural language processing"	"LanguageWare is a natural language processing (NLP) technology developed by IBM, which allows applications to process natural language text. It comprises a set of Java libraries which provide a range of NLP functions: language identification, text segmentation/tokenization, normalization, entity and relationship extraction, and semantic analysis and disambiguation. The analysis engine uses Finite State Machine approach at multiple levels, which aids its performance characteristics, while maintaining a reasonably small footprint. The behaviour of the system is driven by a set of configurable lexico-semantic resources which describe the characteristics and domain of the processed language. A default set of resources comes as part of LanguageWare and these describe the native language characteristics, such as morphology, and the basic vocabulary for the language. Supplemental resources have been created which capture additional vocabularies, terminologies, rules and grammars, which may be generic to the language or specific to one or more domains. A set of Eclipse-based customization tooling, LanguageWare Resource Workbench, is available on IBM's alphaWorks site, and allows domain knowledge to be compiled into these resources and thereby incorporated into the analysis process. LanguageWare can be deployed as a set of UIMA-compliant annotators, Eclipse plug-ins or Web Services."
"http://dbpedia.org/resource/Deep_linguistic_processing"	"Deep linguistic processing"	"Natural language processing"	"Deep linguistic processing is a natural language processing framework which draws on theoretical and descriptive linguistics. It models language predominantly by way of theoretical syntactic/semantic theory (e.g. CCG, HPSG, LFG, TAG, the Prague School). Deep linguistic processing approaches differ from ""shallower"" methods in that they yield more expressive and structural representations which directly capture long-distance dependencies and underlying predicate-argument structures. The knowledge-intensive approach of deep linguistic processing requires considerable computational power, and has in the past sometimes been judged as being intractable. However, research in the early 2000s had made considerable advancement in efﬁciency of deep processing. Today, efﬁciency is no longer a major problem for applications using deep linguistic processing."
"http://dbpedia.org/resource/Document_structuring"	"Document structuring"	"Natural language processing"	"Document Structuring is a subtask of Natural language generation, which involves deciding the order and grouping (for example into paragraphs) of sentences in a generated text. It is closely related to the Content determination NLG task."
"http://dbpedia.org/resource/Legal_information_retrieval"	"Legal information retrieval"	"Natural language processing"	"Legal information retrieval is the science of information retrieval applied to legal text, including legislation, case law, and scholarly works. Accurate legal information retrieval is important to provide access to the law to laymen and legal professionals. Its importance has increased because of the vast and quickly increasing amount of legal documents available through electronic means. Legal information retrieval is a part of the growing field of legal informatics."
"http://dbpedia.org/resource/Lessac_Technologies"	"Lessac Technologies"	"Natural language processing"	"Lessac Technologies, Inc. (LTI) is an American firm which develops voice synthesis software, licenses technology and sells synthesized novels as MP3 files. The firm currently has seven patents granted and three more pending for its automated methods of converting digital text into human-sounding speech, more accurately recognizing human speech and outputting the text representing the words and phrases of said speech, along with recognizing the speaker’s emotional state. The LTI technology is partly based on the work of the late Arthur Lessac, a Professor of Theater at the State University of New York and the creator of Lessac Kinesensic Training, and LTI has licensed exclusive rights to exploit Arthur Lessac's copyrighted works in the fields of speech synthesis and speech recognition. Based on the view that music is speech and speech is music, Lessac's work and books focused on body and speech energies and how they go together. Arthur Lessac's textual annotation system, which was originally developed to assist actors, singers, and orators in marking up scripts to prepare for performance, is adapted in LTI's speech synthesis system as the basic representation of the speech to be synthesized (Lessemes), in contrast to many other systems which use a phonetic representation. LTI’s software has two major components: (1) a linguistic front-end that converts plain text to a sequence of prosodic and phonosensory graphic symbols (Lessemes) based on Arthur Lessac's annotation system, which specify the speech units to be synthesized; (2) a signal-processing back-end that takes the Lessemes as acoustic data and produces human-sounding synthesized speech as output, using unit selection and concatenation. LTI’s text-to-speech system came in second in the world-wide Blizzard Challenge 2011 and 2012. The first-place team in 2011 also employed LTI's ""front-end"" technology, but with its own back-end. The Blizzard Challenge, conducted by the Language Technologies Institute of Carnegie Mellon University, was devised as a way to evaluate speech synthesis techniques by having different research groups build voices from the same voice-actor recordings, and comparing the results through listening tests. LTI was founded in 2000 by H. Donald Wilson (chairman), a lawyer, LexisNexis entrepreneur and business associate of Arthur Lessac; and Gary A. Marple (chief inventor), after Marple suggested that Arthur Lessac's kinesensic voice training might be applicable to computational linguistics. After Wilson’s death in 2006, his nephew John Reichenbach became the firm’s CEO."
"http://dbpedia.org/resource/MeaningCloud"	"MeaningCloud"	"Natural language processing"	"MeaningCloud, formerly known as Textalytics is a Software as a Service product that enables users to embed text analytics and semantic processing in any application or system -in an effective and inexpensive way. MeaningCloud extends the concept of semantic API with a cloud-based framework that makes the integration of semantic processing into any system something close to a plug-and-play experience. MeaningCloud is available both in SaaS mode and on-premises. In this way, developers and integrators can build solutions (e.g., media monitoring, voice of the customer analysis, content publishing) that leverage text analytics technologies applying a low risk, fast time-to-market approach, and using an inexpensive pay-per-use model."
"http://dbpedia.org/resource/Hybrid_machine_translation"	"Hybrid machine translation"	"Natural language processing"	"Hybrid machine translation is a method of machine translation that is characterized by the use of multiple machine translation approaches within a single machine translation system. The motivation for developing hybrid machine translation systems stems from the failure of any single technique to achieve a satisfactory level of accuracy. Many hybrid machine translation systems have been successful in improving the accuracy of the translations, and there are several popular machine translation systems which employ hybrid methods. Among these are PROMT, SYSTRAN and Omniscien Technologies (formerly Asia Online)."
"http://dbpedia.org/resource/Rule-based_machine_translation"	"Rule-based machine translation"	"Natural language processing"	"Rule-based machine translation (RBMT; ""Classical Approach"" of MT) is machine translation systems based on linguistic information about source and target languages basically retrieved from (unilingual, bilingual or multilingual) dictionaries and grammars covering the main semantic, morphological, and syntactic regularities of each language respectively. Having input sentences (in some source language), an RBMT system generates them to output sentences (in some target language) on the basis of morphological, syntactic, and semantic analysis of both the source and the target languages involved in a concrete translation task."
"http://dbpedia.org/resource/Content_determination"	"Content determination"	"Natural language processing"	"Content determination is the subtask of Natural language generation that involves deciding on the information to be communicated in a generated text. It is closely related to the task of Document structuring."
"http://dbpedia.org/resource/Automatic_acquisition_of_sense-tagged_corpora"	"Automatic acquisition of sense-tagged corpora"	"Natural language processing"	"The knowledge acquisition bottleneck is perhaps the major impediment to solving the word sense disambiguation (WSD) problem. Unsupervised learning methods rely on knowledge about word senses, which is barely formulated in dictionaries and lexical databases. Supervised learning methods depend heavily on the existence of manually annotated examples for every word sense, a requisite that can so far be met only for a handful of words for testing purposes, as it is done in the Senseval exercises."
"http://dbpedia.org/resource/Medical_intelligence_and_language_engineering_lab"	"Medical intelligence and language engineering lab"	"Natural language processing"	"The Medical Intelligence and Language Engineering laboratory, also known as MILE lab, is a research laboratory at the Indian Institute of Science, Bangalore under the Department of Electrical Engineering. The lab is known for its work on Image processing, online handwriting recognition, Text-To-Speech and Optical character recognition systems, all of which are focused mainly on documents and speech in Indian languages. The lab is headed by A. G. Ramakrishnan."
"http://dbpedia.org/resource/String_kernel"	"String kernel"	"Natural language processing"	"In machine learning and data mining, a string kernel is a kernel function that operates on strings, i.e. finite sequences of symbols that need not be of the same length. String kernels can be intuitively understood as functions measuring the similarity of pairs of strings: the more similar two strings a and b are, the higher the value of a string kernel K(a, b) will be. Using string kernels with kernelized learning algorithms such as support vector machines allow such algorithms to work with strings, without having to translate these to fixed-length, real-valued feature vectors. String kernels are used in domains where sequence data are to be clustered or classified, e.g. in text mining and gene analysis."
"http://dbpedia.org/resource/Automatic_taxonomy_construction"	"Automatic taxonomy construction"	"Natural language processing"	"Automatic taxonomy construction (ATC) is the use of autonomous or semi-autonomous software programs to create hierarchical outlines or taxonomical classifications from a body of texts (corpus). It is a branch of natural language processing, which in turn is a branch of artificial intelligence. ATC programs are examples of software agents and intelligent agents, and may be autonomous as well (see autonomous agent). Other names for ATC include taxonomy generation, taxonomy learning, taxonomy extraction, taxonomy building, and taxonomy induction. Any of these terms may be preceded by the word ""automatic"", as in automatic taxonomy induction. ATC is also referred to as semantic taxonomy induction. A taxonomy is a tree structure and includes familial (parent-offspring, sibling, etc.) relationships built-in (like in a family tree). For example, physics is an offspring of physical science, which in turn is an offspring of science. As mentioned above, the process is also called taxonomy induction. This is because, in order for a software program to construct a taxonomy from a corpus (for example, from Wikipedia, a web page, or the World Wide Web), it must induce which terms belong to the taxonomy and what the relationships between them are. Such as by identifying hyponym-hypernym pairs, among other approaches. This is done using algorithms, including statistical algorithms. Note that deduction (deductive logic) is often also employed (e.g., if B is a sibling of A, then B has the same parent as A and gets placed under that parent in the taxonomy). The primary application of automatic taxonomy construction is in ontology learning, a central activity within ontology engineering. In computer science and artificial intelligence, an ontology is a conceptual model of a (subject) domain. A domain is a given subject area or specifically defined sphere of interest. An ontology of a domain includes the vocabulary of that domain and the relationships between those concepts or entities. The backbone of most ontologies is a taxonomy, and taxonomical structure may be used throughout an ontology. As building taxonomies manually is extremely labor-intensive and time-consuming, there is great motivation to automate the process."
"http://dbpedia.org/resource/SemEval"	"SemEval"	"Natural language processing"	"SemEval (Semantic Evaluation) is an ongoing series of evaluations of computational semantic analysis systems; it evolved from the Senseval word sense evaluation series. The evaluations are intended to explore the nature of meaning in language. While meaning is intuitive to humans, transferring those intuitions to computational analysis has proved elusive. This series of evaluations is providing a mechanism to characterize in more precise terms exactly what is necessary to compute in meaning. As such, the evaluations provide an emergent mechanism to identify the problems and solutions for computations with meaning. These exercises have evolved to articulate more of the dimensions that are involved in our use of language. They began with apparently simple attempts to identify word senses computationally. They have evolved to investigate the interrelationships among the elements in a sentence (e.g., semantic role labeling), relations between sentences (e.g., coreference), and the nature of what we are saying (semantic relations and sentiment analysis). The purpose of the SemEval and Senseval exercises is to evaluate semantic analysis systems. ""Semantic Analysis"" refers to a formal analysis of meaning, and ""computational"" refer to approaches that in principle support effective implementation. The first three evaluations, Senseval-1 through Senseval-3, were focused on word sense disambiguation, each time growing in the number of languages offered in the tasks and in the number of participating teams. Beginning with the fourth workshop, SemEval-2007 (SemEval-1), the nature of the tasks evolved to include semantic analysis tasks outside of word sense disambiguation. Triggered by the conception of the *SEM conference, the SemEval community had decided to hold the evaluation workshops yearly in association with the *SEM conference. It was also the decision that not every evaluation task will be run every year, e.g. none of the WSD tasks were included in the SemEval-2012 workshop."
"http://dbpedia.org/resource/Semantic_analysis_(computational)"	"Semantic analysis (computational)"	"Natural language processing"	"Semantic analysis (computational) is a composite of the ""semantic analysis"" and the ""computational"" components. ""Semantic analysis"" refers to a formal analysis of meaning, and ""computational"" refer to approaches that in principle support effective implementation."
"http://dbpedia.org/resource/Sayre's_paradox"	"Sayre's paradox"	"Natural language processing"	"Sayre’s Paradox is a dilemma encountered in the design of automated handwriting recognition systems. A standard statement of the paradox is that a cursively written word cannot be recognized without being segmented and cannot be segmented without being recognized. The paradox was first articulated in a 1973 publication by Kenneth M. Sayre, after whom it was named."
"http://dbpedia.org/resource/Outline_of_natural_language_processing"	"Outline of natural language processing"	"Natural language processing"	"The following outline is provided as an overview of and topical guide to natural language processing: Natural language processing – computer activity in which computers are entailed to analyze, understand, alter, or generate natural language. This includes the automation of any or all linguistic forms, activities, or methods of communication, such as conversation, correspondence, reading, written composition, dictation, publishing, translation, lip reading, and so on. Natural language processing is also the name of the branch of computer science, artificial intelligence, and linguistics concerned with enabling computers to engage in communication using natural language(s) in all forms, including but not limited to speech, print, writing, and signing."
"http://dbpedia.org/resource/Semantic_compression"	"Semantic compression"	"Natural language processing"	"In natural language processing, semantic compression is a process of compacting a lexicon used to build a textual document (or a set of documents) by reducing language heterogeneity, while maintaining text semantics. As a result, the same ideas can be represented using a smaller set of words. Semantic compression is a lossy compression, that is, some data is being discarded, and an original document cannot be reconstructed in a reverse process."
"http://dbpedia.org/resource/Empirical_Methods_in_Natural_Language_Processing"	"Empirical Methods in Natural Language Processing"	"Natural language processing"	"Empirical Methods in Natural Language Processing or EMNLP is a leading conference in the area of Natural Language Processing. EMNLP is organized by the ACM special interest group on linguistic data (SIGDAT). EMNLP was started in 1996, based on an earlier conference series called Workshop on Very Large Corpora (WLVC). As of 2014, EMNLP has a field rating of 36 within computer science and a citation count of 6937, according to Microsoft Academic Search, both within the top-300 for the field of CS."
"http://dbpedia.org/resource/List_of_text_corpora"	"List of text corpora"	"Natural language processing"	"Following is a list of text corpora in various languages. ""Text corpora"" is the plural of ""text corpus"". A text corpus is a large and structured set of texts (nowadays usually electronically stored and processed). Text corpora are used to do statistical analysis and hypothesis testing, checking occurrences or validating linguistic rules within a specific language territory."
"http://dbpedia.org/resource/Lexical_simplification"	"Lexical simplification"	"Natural language processing"	"Lexical Simplification is a sub-task of text simplification. It can be defined as any lexical substitution task that reduce text complexity."
"http://dbpedia.org/resource/Semantic_folding"	"Semantic folding"	"Natural language processing"	"Semantic folding theory describes a procedure for encoding the semantics of natural language text in a semantically grounded binary representation. This approach provides a framework for modelling how language data is processed by the neocortex."
"http://dbpedia.org/resource/Text_graph"	"Text graph"	"Natural language processing"	"In natural language processing (NLP), a text graph is a graph representation of a text item (document, passage or sentence). It is typically created as a preprocessing step to support NLP tasks such as text condensationterm disambiguation(topic-based) text summarization, relation extraction  and textual entailment"
"http://dbpedia.org/resource/Manually_Annotated_Sub-Corpus_(MASC)"	"Manually Annotated Sub-Corpus (MASC)"	"Natural language processing"	"MASC is a balanced subset of 500K words of written texts and transcribed speech drawn primarily from the Open American National Corpus (OANC). The OANC is a 15 million word (and growing) corpus of American English produced since 1990, all of which is in the public domain or otherwise free of usage and redistribution restrictions. All of MASC includes manually validated annotations for logical structure (headings, sections, paragraphs, etc.), sentence boundaries, three different tokenizations with associated part of speech tags, shallow parse (noun and verb chunks), named entities (person, location, organization, date and time), and Penn Treebank syntax. Additional manually produced or validated annotations have been produced by the MASC project for portions of the sub-corpus, including full-text annotation for FrameNet frame elements and a 100K+ sentence corpus with WordNet 3.1 sense tags, of which one-tenth are also annotated for FrameNet frame elements. Annotations of all or portions of the sub-corpus for a wide variety of other linguistic phenomena have been contributed by other projects, including PropBank, TimeBank, MPQA opinion, and several others. Co-reference annotations and clause boundaries of the entire MASC corpus are scheduled to be released by the end of 2016. WordNet sense annotations for all occurrences of 114 words are also included in the MASC distribution, as well as FrameNet annotations for 50-100 occurrences of each of the 114 words. The sentences with WordNet and FrameNet annotations are also distributed as a part of the MASC Sentence Corpus."
"http://dbpedia.org/resource/Cloem"	"Cloem"	"Natural language processing"	"Cloem is a company based in Cannes, France, which applies natural language processing (NLP) technologies to assist patent applicants in creating variants of patent claims, called ""cloems"". According to the company, these ""computer-generated claims can be published to keep potential competitors from attempting to file adjacent patent claims."""
"http://dbpedia.org/resource/Rademacher_complexity"	"Rademacher complexity"	"Machine learning"	"In computational learning theory (machine learning and theory of computation), Rademacher complexity, named after Hans Rademacher, measures richness of a class of real-valued functions with respect to a probability distribution. Given a training sample , and a class of real-valued functions defined on a domain space , the empirical Rademacher complexity of is defined as: where are independent random variables drawn from the Rademacher distribution i.e. for . Let be a probability distribution over . The Rademacher complexity of the function class with respect to for sample size is: where the above expectation is taken over an identically independently distributed (i.i.d.) sample generated according to . One can show, for example, that there exists a constant , such that any class of -indicator functions with Vapnik-Chervonenkis dimension has Rademacher complexity upper-bounded by ."
"http://dbpedia.org/resource/Relational_data_mining"	"Relational data mining"	"Machine learning"	"Relational data mining is the data mining technique for relationaldatabases. Unlike traditional data mining algorithms, which look forpatterns in a single table (propositional patterns), relational data mining algorithms look for patterns among multiple tables(relational patterns). For most types of propositionalpatterns, there are corresponding relational patterns. For example,there are relational classification rules (relational classification), relational regression tree, and relational association rules. There are several approaches to relational data mining: 1.  
*  Inductive Logic Programming (ILP) 2.  
*  Statistical Relational Learning (SRL) 3.  
*  Graph Mining 4.  
*  Propositionalization 5.  
*  Multi-view learning"
"http://dbpedia.org/resource/CIML_community_portal"	"CIML community portal"	"Machine learning"	"The computational intelligence and machine learning (CIML) community portal is an international multi-university initiative. Its primary purpose is to help facilitate a virtual scientific community infrastructure for all those involved with, or interested in, computational intelligence and machine learning. This includes CIML research-,education, and application-oriented resources residing at the portal and others that are linked from the CIML site."
"http://dbpedia.org/resource/Granular_computing"	"Granular computing"	"Machine learning"	"Granular computing (GrC) is an emerging computing paradigm of information processing. It concerns the processing of complex information entities called information granules, which arise in the process of data abstraction and derivation of knowledge from information or data. Generally speaking, information granules are collections of entities that usually originate at the numeric level and are arranged together due to their similarity, functional or physical adjacency, indistinguishability, coherency, or the like. At present, granular computing is more a theoretical perspective than a coherent set of methods or principles. As a theoretical perspective, it encourages an approach to data that recognizes and exploits the knowledge present in data at various levels of resolution or scales. In this sense, it encompasses all methods which provide flexibility and adaptability in the resolution at which knowledge or information is extracted and represented."
"http://dbpedia.org/resource/Concept_learning"	"Concept learning"	"Machine learning"	"Concept learning, also known as category learning, concept attainment, and concept formation, is largely based on the works of the cognitive psychologist Jerome Bruner. Bruner, Goodnow, & Austin (1967) defined concept attainment (or concept learning) as ""the search for and listing of attributes that can be used to distinguish exemplars from non exemplars of various categories."" More simply put, concepts are the mental categories that help us classify objects, events, or ideas, building on the understanding that each object, event, or idea has a set of common relevant features. Thus, concept learning is a strategy which requires a learner to compare and contrast groups or categories that contain concept-relevant features with groups or categories that do not contain concept-relevant features. Concept learning also refers to a learning task in which a human or machine learner is trained to classify objects by being shown a set of example objects along with their class labels. The learner simplifies what has been observed by condensing it in the form of an example. This simplified version of what has been learned is then applied to future examples. Concept learning may be simple or complex because learning takes place over many areas. When a concept is difficult, it is less likely that the learner will be able to simplify, and therefore will be less likely to learn. Colloquially, the task is known as learning from examples. Most theories of concept learning are based on the storage of exemplars and avoid summarization or overt abstraction of any kind."
"http://dbpedia.org/resource/Prior_knowledge_for_pattern_recognition"	"Prior knowledge for pattern recognition"	"Machine learning"	"Pattern recognition is a very active field of research intimately bound to machine learning. Also known as classification or statistical classification, pattern recognition aims at building a classifier that can determine the class of an input pattern. This procedure, known as training, corresponds to learning an unknown decision function based only on a set of input-output pairs that form the training data (or training set). Nonetheless, in real world applications such as character recognition, a certain amount of information on the problem is usually known beforehand. The incorporation of this prior knowledge into the training is the key element that will allow an increase of performance in many applications."
"http://dbpedia.org/resource/Decision_list"	"Decision list"	"Machine learning"	"Decision lists are a representation for Boolean functions. Single term decision lists are more expressive than disjunctions and conjunctions; however, 1-term decision lists are less expressive than the general disjunctive normal form and the conjunctive normal form. The language specified by a k-length decision list includes as a subset the language specified by a k-depth decision tree. Learning decision lists can be used for attribute efficient learning."
"http://dbpedia.org/resource/Neural_modeling_fields"	"Neural modeling fields"	"Machine learning"	"Neural modeling field (NMF) is a mathematical framework for machine learning which combines ideas from neural networks, fuzzy logic, and model based recognition. It has also been referred to as modeling fields, modeling fields theory (MFT), Maximum likelihood artificial neural networks (MLANS). This framework has been developed by Leonid Perlovsky at the AFRL. NMF is interpreted as a mathematical description of mind’s mechanisms, including concepts, emotions, instincts, imagination, thinking, and understanding. NMF is a multi-level, hetero-hierarchical system. At each level in NMF there are concept-models encapsulating the knowledge; they generate so-called top-down signals, interacting with input, bottom-up signals. These interactions are governed by dynamic equations, which drive concept-model learning, adaptation, and formation of new concept-models for better correspondence to the input, bottom-up signals."
"http://dbpedia.org/resource/Proactive_learning"	"Proactive learning"	"Machine learning"	"Proactive learning is a generalization of active learning designed to relax unrealistic assumptions and thereby reach practical applications. ""Active learning seeks to select the most informative unlabeled instances and ask an omniscient oracle for their labels, so as to retrain a learning algorithm maximizing accuracy. However, the oracle is assumed to be infallible (never wrong), indefatigable (always answers), individual (only one oracle), and insensitive to costs (always free or always charges the same)."" ""In real life, it is possible and more general to have multiple sources of information with differing reliabilities or areas of expertise. Active learning also assumes that the single oracle is perfect, always providing a correct answer when requested. In reality, though, an ""oracle"" (if we generalize the term to mean any source of expert information) may be incorrect (fallible) with a probability that should be a function of the difficulty of the question. Moreover, an oracle may be reluctant – it may refuse to answer if it is too uncertain or too busy. Finally, active learning presumes the oracle is either free or charges uniform cost in label elicitation.Such an assumption is naive since cost is likely to be regulated by difficulty (amount of work required to formulate an answer) or other factors."" Proactive learning relaxes all four of these assumptions, relying on a decision-theoretic approach to jointly select the optimal oracle and instance, by casting the problem as a utility optimization problem subject to a budget constraint."
"http://dbpedia.org/resource/Journal_of_Machine_Learning_Research"	"Journal of Machine Learning Research"	"Machine learning"	"The Journal of Machine Learning Research (usually abbreviated JMLR), is a scientific journal focusing on machine learning, a subfield of artificial intelligence. It was founded in 2000. The journal was founded as an open-access alternative to the journal Machine Learning. In 2001, forty editors of Machine Learning resigned in order to support JMLR, saying that in the era of the internet, it was detrimental for researchers to continue publishing their papers in expensive journals with pay-access archives. Instead, they wrote, they supported the model of JMLR, in which authors retained copyright over their papers and archives were freely available on the internet. Print editions of JMLR were published by MIT Press until 2004, and by Microtome Publishing thereafter. Since Summer 2007 JMLR is also publishing Machine Learning Open Source Software ."
"http://dbpedia.org/resource/Machine_Learning_(journal)"	"Machine Learning (journal)"	"Machine learning"	"Machine Learning is a peer-reviewed scientific journal, published since 1986. In 2001, forty editors and members of the editorial board of Machine Learning resigned in order to support the Journal of Machine Learning Research (JMLR), saying that in the era of the internet, it was detrimental for researchers to continue publishing their papers in expensive journals with pay-access archives. Instead, they wrote, they supported the model of JMLR, in which authors retained copyright over their papers and archives were freely available on the internet."
"http://dbpedia.org/resource/Inductive_bias"	"Inductive bias"	"Machine learning"	"The inductive bias (also known as learning bias) of a learning algorithm is the set of assumptions that the learner uses to predict outputs given inputs that it has not encountered. In machine learning, one aims to construct algorithms that are able to learn to predict a certain target output. To achieve this, the learning algorithm is presented some training examples that demonstrate the intended relation of input and output values. Then the learner is supposed to approximate the correct output, even for examples that have not been shown during training. Without any additional assumptions, this problem cannot be solved exactly since unseen situations might have an arbitrary output value. The kind of necessary assumptions about the nature of the target function are subsumed in the phrase inductive bias. A classical example of an inductive bias is Occam's razor, assuming that the simplest consistent hypothesis about the target function is actually the best. Here consistent means that the hypothesis of the learner yields correct outputs for all of the examples that have been given to the algorithm. Approaches to a more formal definition of inductive bias are based on mathematical logic. Here, the inductive bias is a logical formula that, together with the training data, logically entails the hypothesis generated by the learner. Unfortunately, this strict formalism fails in many practical cases, where the inductive bias can only be given as a rough description (e.g. in the case of neural networks), or not at all."
"http://dbpedia.org/resource/Overfitting"	"Overfitting"	"Machine learning"	"In statistics and machine learning, one of the most common tasks is to fit a ""model"" to a set of training data, so as to be able to make reliable predictions on general untrained data. In overfitting, a statistical model describes random error or noise instead of the underlying relationship. Overfitting occurs when a model is excessively complex, such as having too many parameters relative to the number of observations. A model that has been overfit has poor predictive performance, as it overreacts to minor fluctuations in the training data. The possibility of overfitting exists because the criterion used for training the model is not the same as the criterion used to judge the efficacy of a model. In particular, a model is typically trained by maximizing its performance on some set of training data. However, its efficacy is determined not by its performance on the training data but by its ability to perform well on unseen data. Overfitting occurs when a model begins to ""memorize"" training data rather than ""learning"" to generalize from trend. As an extreme example, if the number of parameters is the same as or greater than the number of observations, a simple model or learning process can perfectly predict the training data simply by memorizing the training data in its entirety, but such a model will typically fail drastically when making predictions about new or unseen data, since the simple model has not learned to generalize at all. The potential for overfitting depends not only on the number of parameters and data but also the conformability of the model structure with the data shape, and the magnitude of model error compared to the expected level of noise or error in the data. Even when the fitted model does not have an excessive number of parameters, it is to be expected that the fitted relationship will appear to perform less well on a new data set than on the data set used for fitting. In particular, the value of the coefficient of determination will shrink relative to the original training data. In order to avoid overfitting, it is necessary to use additional techniques (e.g. cross-validation, regularization, early stopping, pruning, Bayesian priors on parameters or model comparison), that can indicate when further training is not resulting in better generalization. The basis of some techniques is either (1) to explicitly penalize overly complex models, or (2) to test the model's ability to generalize by evaluating its performance on a set of data not used for training, which is assumed to approximate the typical unseen data that a model will encounter."
"http://dbpedia.org/resource/Curse_of_dimensionality"	"Curse of dimensionality"	"Machine learning"	"The curse of dimensionality refers to various phenomena that arise when analyzing and organizing data in high-dimensional spaces (often with hundreds or thousands of dimensions) that do not occur in low-dimensional settings such as the three-dimensional physical space of everyday experience.The expression was coined by Richard E. Bellman when considering problems in dynamic optimization. There are multiple phenomena referred to by this name in domains such as numerical analysis, sampling, combinatorics, machine learning, data mining, and databases. The common theme of these problems is that when the dimensionality increases, the volume of the space increases so fast that the available data become sparse. This sparsity is problematic for any method that requires statistical significance. In order to obtain a statistically sound and reliable result, the amount of data needed to support the result often grows exponentially with the dimensionality. Also, organizing and searching data often relies on detecting areas where objects form groups with similar properties; in high dimensional data, however, all objects appear to be sparse and dissimilar in many ways, which prevents common data organization strategies from being efficient."
"http://dbpedia.org/resource/Multi-task_learning"	"Multi-task learning"	"Machine learning"	"Multi-task learning (MTL) is an approach to machine learning that learns a problem together with other related problems at the same time, using a shared representation. This often leads to a better model for the main task, because it allows the learner to use the commonality among the tasks. Therefore, multi-task learning is a kind of inductive transfer. This type of machine learning is an approach to inductive transfer that improves generalization by using the domain information contained in the training signals of related tasks as an inductive bias. It does this by learning tasks in parallel while using a shared representation; what is learned for each task can help other tasks be learned better. The goal of MTL is to improve the performance of learning algorithms by learning classifiers for multiple tasks jointly. This works particularly well if these tasks have some commonality and are generally slightly under sampled. One example is a spam-filter. Everybody has a slightly different distribution over spam or not-spam emails (e.g. all emails in Russian are spam for me—but not so for my Russian colleagues), yet there is definitely a common aspect across users. Multi-task learning works, because encouraging a classifier (or a modification thereof) to also perform well on a slightly different task is a better regularization than uninformed regularizers (e.g. to enforce that all weights are small)."
"http://dbpedia.org/resource/Discriminative_model"	"Discriminative model"	"Machine learning"	"Discriminative models, also called conditional models, are a class of models used in machine learning for modeling the dependence of an unobserved variable on an observed variable . Within a probabilistic framework, this is done by modeling the conditional probability distribution , which can be used for predicting from . Discriminative models, as opposed to generative models, do not allow one to generate samples from the joint distribution of and . However, for tasks such as classification and regression that do not require the joint distribution, discriminative models can yield superior performance. On the other hand, generative models are typically more flexible than discriminative models in expressing dependencies in complex learning tasks. In addition, most discriminative models are inherently supervised and cannot easily be extended to unsupervised learning. Application specific details ultimately dictate the suitability of selecting a discriminative versus generative model."
"http://dbpedia.org/resource/Formal_concept_analysis"	"Formal concept analysis"	"Machine learning"	"In information science, formal concept analysis (FCA) is a principled way of deriving a concept hierarchy or formal ontology from a collection of objects and their properties. Each concept in the hierarchy represents the set of objects sharing the same values for a certain set of properties; and each sub-concept in the hierarchy contains a subset of the objects in the concepts above it. The term was introduced by Rudolf Wille in 1984, and builds on applied lattice and order theory that was developed by Garrett Birkhoff and others in the 1930s. Formal concept analysis finds practical application in fields including data mining, text mining, machine learning, knowledge management, semantic web, software development, chemistry and biology."
"http://dbpedia.org/resource/Instance-based_learning"	"Instance-based learning"	"Machine learning"	"In machine learning, instance-based learning (sometimes called memory-based learning) is a family of learning algorithms that, instead of performing explicit generalization, compares new problem instances with instances seen in training, which have been stored in memory. Instance-based learning is a kind of lazy learning. It is called instance-based because it constructs hypotheses directly from the training instances themselves.This means that the hypothesis complexity can grow with the data: in the worst case, a hypothesis is a list of n training items and the computational complexity of classifying a single new instance is O(n). One advantage that instance-based learning has over other methods of machine learning is its ability to adapt its model to previously unseen data: instance-based learners may simply store a new instance or throw an old instance away. Examples of instance-based learning algorithm are the k-nearest neighbor algorithm, kernel machines and RBF networks. These store (a subset of) their training set; when predicting a value/class for a new instance, they compute distances or similarities between this instance and the training instances to make a decision. To battle the memory complexity of storing all training instances, as well as the risk of overfitting to noise in the training set, instance reduction algorithms have been proposed. Gagliardi applies this family of classifiers in medical field as second-opinion diagnostic tools and as tools for the knowledge extraction phase in the process of knowledge discovery in databases. One of these classifiers (called Prototype exemplar learning classifier (PEL-C) is able to extract a mixture of abstracted prototypical cases (that are syndromes) and selected atypical clinical cases."
"http://dbpedia.org/resource/Matthews_correlation_coefficient"	"Matthews correlation coefficient"	"Machine learning"	"The Matthews correlation coefficient is used in machine learning as a measure of the quality of binary (two-class) classifications, introduced by biochemist Brian W. Matthews in 1975. It takes into account true and false positives and negatives and is generally regarded as a balanced measure which can be used even if the classes are of very different sizes. The MCC is in essence a correlation coefficient between the observed and predicted binary classifications; it returns a value between −1 and +1. A coefficient of +1 represents a perfect prediction, 0 no better than random prediction and −1 indicates total disagreement between prediction and observation. The statistic is also known as the phi coefficient. MCC is related to the chi-square statistic for a 2×2 contingency table where n is the total number of observations. While there is no perfect way of describing the confusion matrix of true and false positives and negatives by a single number, the Matthews correlation coefficient is generally regarded as being one of the best such measures. Other measures, such as the proportion of correct predictions (also termed accuracy), are not useful when the two classes are of very different sizes. For example, assigning every object to the larger set achieves a high proportion of correct predictions, but is not generally a useful classification. The MCC can be calculated directly from the confusion matrix using the formula: In this equation, TP is the number of true positives, TN the number of true negatives, FP the number of false positives and FN the number of false negatives. If any of the four sums in the denominator is zero, the denominator can be arbitrarily set to one; this results in a Matthews correlation coefficient of zero, which can be shown to be the correct limiting value. The original formula as given by Matthews was: This is equal to the formula given above. As a correlation coefficient, the Matthews correlation coefficient is the geometric mean of the regression coefficients of the problem and its dual. The component regression coefficients of the Matthews correlation coefficient are Markedness (Δp) and Youden's J statistic (Informedness or Δp'). Markedness and Informedness correspond to different directions of information flow and generalize Youden's J statistic, the deltap statistics and (as their geometric mean) the Matthews Correlation Coefficient to more than two classes."
"http://dbpedia.org/resource/Transduction_(machine_learning)"	"Transduction (machine learning)"	"Machine learning"	"In logic, statistical inference, and supervised learning,transduction or transductive inference is reasoning fromobserved, specific (training) cases to specific (test) cases. In contrast,induction is reasoning from observed training casesto general rules, which are then applied to the test cases. The distinction ismost interesting in cases where the predictions of the transductive model arenot achievable by any inductive model. Note that this is caused by transductiveinference on different test sets producing mutually inconsistent predictions. Transduction was introduced by Vladimir Vapnik in the 1990s, motivated byhis view that transduction is preferable to induction since, according to him, induction requiressolving a more general problem (inferring a function) before solving a morespecific problem (computing outputs for new cases): ""When solving a problem ofinterest, do not solve a more general problem as an intermediate step. Try toget the answer that you really need but not a more general one."" A similarobservation had been made earlier by Bertrand Russell:""we shall reach the conclusion that Socrates is mortal with a greater approach to certainty if we make our argument purely inductive than if we go by way of 'all men are mortal' and then use deduction"" (Russell 1912, chap VII). An example of learning which is not inductive would be in the case of binaryclassification, where the inputs tend to cluster in two groups. A large set oftest inputs may help in finding the clusters, thus providing useful informationabout the classification labels. The same predictions would not be obtainablefrom a model which induces a function based only on the training cases. Somepeople may call this an example of the closely related semi-supervised learning, since Vapnik's motivation is quite different. An example of an algorithm in this category is the Transductive Support Vector Machine (TSVM). A third possible motivation which leads to transduction arises through the needto approximate. If exact inference is computationally prohibitive, one may atleast try to make sure that the approximations are good at the test inputs. Inthis case, the test inputs could come from an arbitrary distribution (notnecessarily related to the distribution of the training inputs), which wouldn'tbe allowed in semi-supervised learning. An example of an algorithm falling inthis category is the Bayesian Committee Machine (BCM)."
"http://dbpedia.org/resource/Pattern_recognition"	"Pattern recognition"	"Machine learning"	"Pattern recognition is a branch of machine learning that focuses on the recognition of patterns and regularities in data, although it is in some cases considered to be nearly synonymous with machine learning. Pattern recognition systems are in many cases trained from labeled ""training"" data (supervised learning), but when no labeled data are available other algorithms can be used to discover previously unknown patterns (unsupervised learning). The terms pattern recognition, machine learning, data mining and knowledge discovery in databases (KDD) are hard to separate, as they largely overlap in their scope. Machine learning is the common term for supervised learning methods and originates from artificial intelligence, whereas KDD and data mining have a larger focus on unsupervised methods and stronger connection to business use. Pattern recognition has its origins in engineering, and the term is popular in the context of computer vision: a leading computer vision conference is named Conference on Computer Vision and Pattern Recognition. In pattern recognition, there may be a higher interest to formalize, explain and visualize the pattern, while machine learning traditionally focuses on maximizing the recognition rates. Yet, all of these domains have evolved substantially from their roots in artificial intelligence, engineering and statistics, and they've become increasingly similar by integrating developments and ideas from each other. In machine learning, pattern recognition is the assignment of a label to a given input value. In statistics, discriminant analysis was introduced for this same purpose in 1936. An example of pattern recognition is classification, which attempts to assign each input value to one of a given set of classes (for example, determine whether a given email is ""spam"" or ""non-spam""). However, pattern recognition is a more general problem that encompasses other types of output as well. Other examples are regression, which assigns a real-valued output to each input; sequence labeling, which assigns a class to each member of a sequence of values (for example, part of speech tagging, which assigns a part of speech to each word in an input sentence); and parsing, which assigns a parse tree to an input sentence, describing the syntactic structure of the sentence. Pattern recognition algorithms generally aim to provide a reasonable answer for all possible inputs and to perform ""most likely"" matching of the inputs, taking into account their statistical variation. This is opposed to pattern matching algorithms, which look for exact matches in the input with pre-existing patterns. A common example of a pattern-matching algorithm is regular expression matching, which looks for patterns of a given sort in textual data and is included in the search capabilities of many text editors and word processors. In contrast to pattern recognition, pattern matching is generally not considered a type of machine learning, although pattern-matching algorithms (especially with fairly general, carefully tailored patterns) can sometimes succeed in providing similar-quality output of the sort provided by pattern-recognition algorithms."
"http://dbpedia.org/resource/CBCL_(MIT)"	"CBCL (MIT)"	"Machine learning"	"The Center for Biological & Computational Learning is a research lab at the Massachusetts Institute of Technology. CBCL was established in 1992 with support from the National Science Foundation. It is based in the Department of Brain & Cognitive Sciences at MIT, and is associated with the McGovern Institute for Brain Research, and the MIT Computer Science and Artificial Intelligence Laboratory. It was founded with the belief that learning is at the very core of the problem of intelligence, both biological and artificial. Learning is thus the gateway to understanding how the human brain works and for making intelligent machines. CBCL studies the problem of learning within a multidisciplinary approach. Its main goal is to nurture serious research on the mathematics, the engineering and the neuroscience of learning. Research is focused on the problem of learning in theory, engineering applications, and neuroscience. In computational neuroscience, the center has developed a model of the ventral stream in the visual cortex which accounts for much of the physiological data, and psychophysical experiments in difficult object recognition tasks. The model performs at the level of the best computer vision systems."
"http://dbpedia.org/resource/Explanation-based_learning"	"Explanation-based learning"	"Machine learning"	"'Explanation-based learning (EBL)is a form of machine learning that exploits a very strong, or even perfect, domain theory to make generalizations or form concepts from training examples."
"http://dbpedia.org/resource/Ugly_duckling_theorem"	"Ugly duckling theorem"	"Machine learning"	"The Ugly Duckling theorem is an argument asserting that classification is impossible without some sort of bias. It is named for Hans Christian Andersen's story ""The Ugly Duckling."" It gets its name because it shows that, all things being equal, an ugly duckling is just as similar to a swan as two swans are to each other, although it is only a theorem in a very informal sense. It was proposed by Satosi Watanabe in 1969."
"http://dbpedia.org/resource/Learning_automata"	"Learning automata"	"Machine learning"	"Learning automata is one type of Machine Learning algorithm studied since 1970s. Compared to other learning scheme, a branch of the theory of adaptive control is devoted to learning automata surveyed by Narendra and Thathachar (1974) which were originally described explicitly as finite state automata. Learning automata select their current action based on past experiences from the environment. It will fall into the range of reinforcement learning if the environment is stochastic and Markov Decision Process (MDP is used)."
"http://dbpedia.org/resource/Meta_learning_(computer_science)"	"Meta learning (computer science)"	"Machine learning"	"Meta learning is a subfield of Machine learning where automatic learning algorithms are applied on meta-data about machine learning experiments. Although different researchers hold different views as to what the term exactly means (see below), the main goal is to use such meta-data to understand how automatic learning can become flexible in solving different kinds of learning problems, hence to improve the performance of existing learning algorithms. Flexibility is very important because each learning algorithm is based on a set of assumptions about the data, its inductive bias. This means that it will only learn well if the bias matches the data in the learning problem. A learning algorithm may perform very well on one learning problem, but very badly on the next. From a non-expert point of view, this poses strong restrictions on the use of machine learning or data mining techniques, since the relationship between the learning problem (often some kind of database) and the effectiveness of different learning algorithms is not yet understood. By using different kinds of meta-data, like properties of the learning problem, algorithm properties (like performance measures), or patterns previously derived from the data, it is possible to select, alter or combine different learning algorithms to effectively solve a given learning problem. Critiques of meta learning approaches bear a strong resemblance to the critique of metaheuristic, which can be said to be a related problem."
"http://dbpedia.org/resource/Inauthentic_text"	"Inauthentic text"	"Machine learning"	"An inauthentic text is a computer-generated expository document meant to appear as genuine, but which is actually meaningless. Frequently they are created in order to be intermixed with genuine documents and thus manipulate the results of search engines, as with Spam blogs. They are also carried along in email in order to fool spam filters by giving the spam the superficial characteristics of legitimate text. Sometimes nonsensical documents are created with computer assistance for humorous effect, as with Dissociated press or Flarf poetry. They have also been used to challenge the veracity of a publication—MIT students submitted papers generated by a computer program called SCIgen to a conference, where they were initially accepted. This led the students to claim that the bar for submissions was too low. With the amount of computer generated text outpacing the ability of people to humans to curate it, there needs some means of distinguishing between the two. Yet automated approaches to determining absolutely whether a text is authentic or not face intrinsic challenges of semantics. Noam Chomsky coined the phrase ""Colorless green ideas sleep furiously"" giving an example of grammatically-correct, but semantically incoherent sentence; some will point out that in certain contexts one could give this sentence (or any phrase) meaning. The first group to use the expression in this regard can be found below from Indiana University. Their work explains in detail an attempt to detect inauthentic texts and identify pernicious problems of inauthentic texts in cyberspace. The site has a means of submitting text that assesses, based on supervised learning, whether a corpus is inauthentic or not. Many users have submitted incorrect types of data and have correspondingly commented on the scores. This application is meant for a specific kind of data; therefore, submitting, say, an email, will not return a meaningful score."
"http://dbpedia.org/resource/Cross-entropy_method"	"Cross-entropy method"	"Machine learning"	"The cross-entropy (CE) method attributed to Reuven Rubinstein is a general Monte Carlo approach tocombinatorial and continuous multi-extremal optimization and importance sampling. The method originated from the field of rare event simulation, wherevery small probabilities need to be accurately estimated, for example in network reliability analysis, queueing models, or performance analysis of telecommunication systems.The CE method can be applied to static and noisy combinatorial optimization problems such as the traveling salesman problem, the quadratic assignment problem, DNA sequence alignment, the max-cut problem and the buffer allocation problem, as well as continuous global optimization problems with many local extrema. In a nutshell the CE method consists of two phases: 1.  
* Generate a random data sample (trajectories, vectors, etc.) according to a specified mechanism. 2.  
* Update the parameters of the random mechanism based on the data to produce a ""better"" sample in the next iteration. This step involves minimizing the cross-entropy or Kullback–Leibler divergence."
"http://dbpedia.org/resource/Active_learning_(machine_learning)"	"Active learning (machine learning)"	"Machine learning"	"Active learning is a special case of semi-supervised machine learning in which a learning algorithm is able to interactively query the user (or some other information source) to obtain the desired outputs at new data points.  In statistics literature it is sometimes also called optimal experimental design. There are situations in which unlabeled data is abundant but manually labeling is expensive. In such a scenario, learning algorithms can actively query the user/teacher for labels. This type of iterative supervised learning is called active learning. Since the learner chooses the examples, the number of examples to learn a concept can often be much lower than the number required in normal supervised learning. With this approach, there is a risk that the algorithm be overwhelmed by uninformative examples.Recent developments are dedicated to hybrid active learning and active learning in a single-pass (on-line) context, combining concepts from the field of Machine Learning (e.g., conflict and ignorance) with adaptive, incremental learning policies in the field of Online machine learning."
"http://dbpedia.org/resource/Supervised_learning"	"Supervised learning"	"Machine learning"	"Supervised learning is the machine learning task of inferring a function from labeled training data. The training data consist of a set of training examples. In supervised learning, each example is a pair consisting of an input object (typically a vector) and a desired output value (also called the supervisory signal). A supervised learning algorithm analyzes the training data and produces an inferred function, which can be used for mapping new examples. An optimal scenario will allow for the algorithm to correctly determine the class labels for unseen instances. This requires the learning algorithm to generalize from the training data to unseen situations in a ""reasonable"" way (see inductive bias). The parallel task in human and animal psychology is often referred to as concept learning."
"http://dbpedia.org/resource/Expectation_propagation"	"Expectation propagation"	"Machine learning"	"Expectation propagation (EP) is a technique in Bayesian machine learning. EP finds approximations to a probability distribution. It uses an iterative approach that leverages the factorization structure of the target distribution. It differs from other Bayesian approximation approaches such as Variational Bayesian methods."
"http://dbpedia.org/resource/Kernel_density_estimation"	"Kernel density estimation"	"Machine learning"	"In statistics, kernel density estimation (KDE) is a non-parametric way to estimate the probability density function of a random variable. Kernel density estimation is a fundamental data smoothing problem where inferences about the population are made, based on a finite data sample. In some fields such as signal processing and econometrics it is also termed the Parzen–Rosenblatt window method, after Emanuel Parzen and Murray Rosenblatt, who are usually credited with independently creating it in its current form."
"http://dbpedia.org/resource/Multi-armed_bandit"	"Multi-armed bandit"	"Machine learning"	"In probability theory, the multi-armed bandit problem (sometimes called the K- or N-armed bandit problem) is a problem in which a gambler at a row of slot machines (sometimes known as ""one-armed bandits"") has to decide which machines to play, how many times to play each machine and in which order to play them. When played, each machine provides a random reward from a probability distribution specific to that machine. The objective of the gambler is to maximize the sum of rewards earned through a sequence of lever pulls. Robbins in 1952, realizing the importance of the problem, constructed convergent population selection strategies in ""some aspects of the sequential design of experiments"". A theorem, the Gittins index, first published by John C. Gittins, gives an optimal policy for maximizing the expected discounted reward. In practice, multi-armed bandits have been used to model the problem of managing research projects in a large organization, like a science foundation or a pharmaceutical company. Given a fixed budget, the problem is to allocate resources among the competing projects, whose properties are only partially known at the time of allocation, but which may become better understood as time passes. In early versions of the multi-armed bandit problem, the gambler has no initial knowledge about the machines. The crucial tradeoff the gambler faces at each trial is between ""exploitation"" of the machine that has the highest expected payoff and ""exploration"" to get more information about the expected payoffs of the other machines. The trade-off between exploration and exploitation is also faced in reinforcement learning."
"http://dbpedia.org/resource/Native-language_identification"	"Native-language identification"	"Machine learning"	"Native-language identification (NLI) is the task of determining an author's native language (L1) based only on their writings in a second language (L2).NLI works through identifying language-usage patterns that are common to specific L1 groups and then applying this knowledge to predict the native language of previously unseen texts.This is motivated in part by applications in second-language acquisition, language teaching and forensic linguistics, amongst others."
"http://dbpedia.org/resource/Nearest_neighbor_search"	"Nearest neighbor search"	"Machine learning"	"Nearest neighbor search (NNS), also known as proximity search, similarity search or closest point search, is an optimization problem for finding closest (or most similar) points. Closeness is typically expressed in terms of a dissimilarity function: the less similar the objects, the larger the function values. Formally, the nearest-neighbor (NN) search problem is defined as follows: given a set S of points in a space M and a query point q ∈ M, find the closest point in S to q. Donald Knuth in vol. 3 of The Art of Computer Programming (1973) called it the post-office problem, referring to an application of assigning to a residence the nearest post office. A direct generalization of this problem is a k-NN search, where we need to find the k closest points. Most commonly M is a metric space and dissimilarity is expressed as a distance metric, which is symmetric and satisfies the triangle inequality. Even more common, M is taken to be the d-dimensional vector space where dissimilarity is measured using the Euclidean distance, Manhattan distance or other distance metric. However, the dissimilarity function can be arbitrary. One example are asymmetric Bregman divergences, for which the triangle inequality does not hold."
"http://dbpedia.org/resource/Pattern_language_(formal_languages)"	"Pattern language (formal languages)"	"Machine learning"	"In theoretical computer science, a pattern language is a formal language that can be defined as the set of all particular instances of a string of constants and variables. Pattern Languages were introduced by Dana Angluin in the context of machine learning."
"http://dbpedia.org/resource/Statistical_relational_learning"	"Statistical relational learning"	"Machine learning"	"Statistical relational learning (SRL) is a subdiscipline of artificial intelligence and machine learning that is concerned with domain models that exhibit both uncertainty (which can be dealt with using statistical methods) and complex, relational structure. Typically, the knowledge representation formalisms developed in SRL use (a subset of) first-order logic to describe relational properties of a domain in a general manner (universal quantification) and draw upon probabilistic graphical models (such as Bayesian networks or Markov networks) to model the uncertainty; some also build upon the methods of inductive logic programming. Significant contributions to the field have been made since the late 1990s. As is evident from the characterization above, the field is not strictly limited to learning aspects; it is equally concerned with reasoning (specifically probabilistic inference) and knowledge representation. Therefore, alternative terms that reflect the main foci of the field include statistical relational learning and reasoning (emphasizing the importance of reasoning) and first-order probabilistic languages (emphasizing the key properties of the languages with which models are represented)."
"http://dbpedia.org/resource/Structural_risk_minimization"	"Structural risk minimization"	"Machine learning"	"Structural risk minimization (SRM) is an inductive principle of use in machine learning. Commonly in machine learning, a generalized model must be selected from a finite data set, with the consequent problem of overfitting – the model becoming too strongly tailored to the particularities of the training set and generalizing poorly to new data. The SRM principle addresses this problem by balancing the model's complexity against its success at fitting the training data. The SRM principle was first set out in a 1974 paper by Vladimir Vapnik and Alexey Chervonenkis and uses the VC dimension."
"http://dbpedia.org/resource/Predictive_state_representation"	"Predictive state representation"	"Machine learning"	"In computer science, a predictive state representation (PSR) is a new way to model a state of controlled dynamical system from a history of actions taken and resulting observations. PSR captures the state of a system as a vector of predictions for future tests (experiments) that can be done on the system. A test is a sequence of action-observation pairs and its prediction is the probability of the test’s observation- sequence happening if the test’s action-sequence were to be executed on the system. One of the advantage of using PSR is that the predictions are directly related to observable quantities. This is in contrast to other models of dynamical systems, such as partially observable Markov decision processes (POMDPs) where the state of the system is represented as a probability distribution over unobserved nominal states."
"http://dbpedia.org/resource/Eager_learning"	"Eager learning"	"Machine learning"	"In artificial intelligence, eager learning is a learning method in which the system tries to construct a general, input-independent target function during training of the system, as opposed to lazy learning, where generalization beyond the training data is delayed until a query is made to the system. The main advantage gained in employing an eager learning method, such as an artificial neural network, is that the target function will be approximated globally during training, thus requiring much less space than using a lazy learning system. Eager learning systems also deal much better with noise in the training data. Eager learning is an example of offline learning, in which post-training queries to the system have no effect on the system itself, and thus the same query to the system will always produce the same result. The main disadvantage with eager learning is that it is generally unable to provide good local approximations in the target function."
"http://dbpedia.org/resource/Cross-validation_(statistics)"	"Cross-validation (statistics)"	"Machine learning"	"Cross-validation, sometimes called rotation estimation, is a model validation technique for assessing how the results of a statistical analysis will generalize to an independent data set. It is mainly used in settings where the goal is prediction, and one wants to estimate how accurately a predictive model will perform in practice. In a prediction problem, a model is usually given a dataset of known data on which training is run (training dataset), and a dataset of unknown data (or first seen data) against which the model is tested (testing dataset). The goal of cross validation is to define a dataset to ""test"" the model in the training phase (i.e., the validation dataset), in order to limit problems like overfitting, give an insight on how the model will generalize to an independent dataset (i.e., an unknown dataset, for instance from a real problem), etc. One round of cross-validation involves partitioning a sample of data into complementary subsets, performing the analysis on one subset (called the training set), and validating the analysis on the other subset (called the validation set or testing set). To reduce variability, multiple rounds of cross-validation are performed using different partitions, and the validation results are averaged over the rounds. One of the main reasons for using cross-validation instead of using the conventional validation (e.g. partitioning the data set into two sets of 70% for training and 30% for test) is that there is not enough data available to partition it into separate training and test sets without losing significant modelling or testing capability. In these cases, a fair way to properly estimate model prediction performance is to use cross-validation as a powerful general technique. In summary, cross-validation combines (averages) measures of fit (prediction error) to derive a more accurate estimate of model prediction performance."
"http://dbpedia.org/resource/Query_level_feature"	"Query level feature"	"Machine learning"	"A query level feature or QLF is a ranking feature utilized in a machine-learned ranking algorithm. Example QLFs: 
*  How many times has this query been run in the last month? 
*  How many words are in the query? 
*  What is the sum/average/min/max/median of the BM25F values for the query?"
"http://dbpedia.org/resource/Dimensionality_reduction"	"Dimensionality reduction"	"Machine learning"	"In machine learning and statistics, dimensionality reduction or dimension reduction is the process of reducing the number of random variables under consideration, via obtaining a set of principal variables. It can be divided into feature selection and feature extraction."
"http://dbpedia.org/resource/Statistical_classification"	"Statistical classification"	"Machine learning"	"In machine learning and statistics, classification is the problem of identifying to which of a set of categories (sub-populations) a new observation belongs, on the basis of a training set of data containing observations (or instances) whose category membership is known. An example would be assigning a given email into ""spam"" or ""non-spam"" classes or assigning a diagnosis to a given patient as described by observed characteristics of the patient (gender, blood pressure, presence or absence of certain symptoms, etc.). Classification is an example of pattern recognition. In the terminology of machine learning, classification is considered an instance of supervised learning, i.e. learning where a training set of correctly identified observations is available. The corresponding unsupervised procedure is known as clustering, and involves grouping data into categories based on some measure of inherent similarity or distance. Often, the individual observations are analyzed into a set of quantifiable properties, known variously as explanatory variables or features. These properties may variously be categorical (e.g. ""A"", ""B"", ""AB"" or ""O"", for blood type), ordinal (e.g. ""large"", ""medium"" or ""small""), integer-valued (e.g. the number of occurrences of a particular word in an email) or real-valued (e.g. a measurement of blood pressure). Other classifiers work by comparing observations to previous observations by means of a similarity or distance function. An algorithm that implements classification, especially in a concrete implementation, is known as a classifier. The term ""classifier"" sometimes also refers to the mathematical function, implemented by a classification algorithm, that maps input data to a category. Terminology across fields is quite varied. In statistics, where classification is often done with logistic regression or a similar procedure, the properties of observations are termed explanatory variables (or independent variables, regressors, etc.), and the categories to be predicted are known as outcomes, which are considered to be possible values of the dependent variable. In machine learning, the observations are often known as instances, the explanatory variables are termed features (grouped into a feature vector), and the possible categories to be predicted are classes. Other fields may use different terminology: e.g. in community ecology, the term ""classification"" normally refers to cluster analysis, i.e. a type of unsupervised learning, rather than the supervised learning described in this article."
"http://dbpedia.org/resource/Document_classification"	"Document classification"	"Machine learning"	"Document classification or document categorization is a problem in library science, information science and computer science. The task is to assign a document to one or more classes or categories. This may be done ""manually"" (or ""intellectually"") or algorithmically. The intellectual classification of documents has mostly been the province of library science, while the algorithmic classification of documents is mainly in information science and computer science. The problems are overlapping, however, and there is therefore interdisciplinary research on document classification. The documents to be classified may be texts, images, music, etc. Each kind of document possesses its special classification problems. When not otherwise specified, text classification is implied. Documents may be classified according to their subjects or according to other attributes (such as document type, author, printing year etc.). In the rest of this article only subject classification is considered. There are two main philosophies of subject classification of documents: the content-based approach and the request-based approach."
"http://dbpedia.org/resource/Grammar_induction"	"Grammar induction"	"Machine learning"	"Grammar induction, also known as grammatical inference or syntactic pattern recognition, refers to the process in machine learning of learning a formal grammar (usually as a collection of re-write rules or productions or alternatively as a finite state machine or automaton of some kind) from a set of observations, thus constructing a model which accounts for the characteristics of the observed objects. More generally, grammatical inference is that branch of machine learning where the instance space consists of discrete combinatorial objects such as strings, trees and graphs."
"http://dbpedia.org/resource/Semantic_analysis_(machine_learning)"	"Semantic analysis (machine learning)"	"Machine learning"	"In machine learning, semantic analysis of a corpus is the task of building structures that approximate concepts from a large set of documents. It generally does not involve prior semantic understanding of the documents. Latent semantic analysis (sometimes latent semantic indexing), is a class of techniques where documents are represented as vectors in term space. A prominent example is PLSI. Latent Dirichlet allocation involves attributing document terms to topics. n-grams and hidden Markov models work by representing the term stream as a markov chain where each term is derived from the few terms before it."
"http://dbpedia.org/resource/Cleverbot"	"Cleverbot"	"Machine learning"	"Cleverbot is a web application that uses an artificial intelligence algorithm to have conversations with humans. It was created by the British AI scientist Rollo Carpenter, who also created Jabberwacky, a similar web application. In its first decade, Cleverbot held several thousand conversations with Carpenter and his associates. Since launching on the web in 1997, the number of conversations held has exceeded 200 million. Besides the web application, Cleverbot is also available as an iOS, Android, and Windows Phone app."
"http://dbpedia.org/resource/Multilinear_principal_component_analysis"	"Multilinear principal component analysis"	"Machine learning"	"Multilinear Principal Component Analysis (MPCA) is a multilinear extension of principal component analysis (PCA). MPCA is employed in the analysis of n-way arrays, i.e. a cube or hyper-cube of numbers, also informally referred to as a ""data tensor"". N-way arrays may be decomposed, analyzed, or modeled by 
*  linear tensor models such as CANDECOMP/Parafac, or  
*  multilinear tensor models, such multilinear principal component analysis (MPCA), or multilinear independent component analysys (MICA), etc. The origin of MPCA can be traced back to the Tucker decomposition and Peter Kroonenberg's ""M-mode PCA/3-mode PCA"" work. In 2000, De Lathauwer etal. restated Tucker and Kroonenberg's work in clear and concise numerical computational terms in their SIAM paper entitled Multilinear Singular Value Decomposition, (HOSVD) and in their paper ""On the Best Rank-1 and Rank-(R1, R2, ..., RN ) Approximation of Higher-order Tensors"". Circa 2001, Vasilescu reframed the data analysis, recognition and synthesis problems as multilinear tensor problems based on the insight that most observed data are the compositional consequence of several causal factors of data formation, and are well suited for multi-modal data tensor analysis. The power of the tensor framework was showcased by analyzing human motion joint angels, facial images or textures in terms of their causal factors of data formation in the following works: Human Motion Signatures(CVPR 2001, ICPR 2002), face recognition - TensorFaces,(ECCV 2002, CVPR 2003, etc.) and computer graphics -- TensorTextures(Siggraph 2004). Historically, MPCA has been referred to as ""M-mode PCA"", a terminology which was coined by Peter Kroonenberg in 1980. In 2005, Vasilescu and Terzopoulos introduced the Multilinear PCA terminology as a way to better differentiate between linear and multilinear tensor decomposition, as well as, to better differentiate between the work that computed 2nd order statistics associated with each data tensor mode(axis), and subsequent work on Multilinear Independent Component Analysis that computed higher order statistics associated with each tensor mode/axis. Multilinear PCA may be applied to compute the causal factors of data formation,or as signal processing tool on data tensors whose individual observation have either been vectorized , or whose observations are treated as matrix  and concatenated into a data tensor. MPCA computes a set of orthonormal matrices associated with each mode of the data tensor which are analogous to the orthonormal row and column space of a matrix computed by the matrix SVD. This transformation aims to capture as high a variance as possible, accounting for as much of the variability in the data associated with each data tensor mode(axis)."
"http://dbpedia.org/resource/Test_set"	"Test set"	"Machine learning"	"In many areas of information science, finding predictive relationships from data is a very important task. Initial discovery of relationships is usually done with a training set while a test set and validation set are used for evaluating whether the discovered relationships hold. More formally, a training set is a set of data used to discover potentially predictive relationships. A test set is a set of data used to assess the strength and utility of a predictive relationship. Test and training sets are used in intelligent systems, machine learning, genetic programming and statistics."
"http://dbpedia.org/resource/Predictive_learning"	"Predictive learning"	"Machine learning"	"Predictive learning is a technique of machine learning in which an agent tries to build a model of its environment by trying out different actions in various circumstances. It uses knowledge of the effects its actions appear to have, turning them into planning operators. These allow the agent to act purposefully in its world. Predictive learning is one attempt to learn with a minimum of pre-existing mental structure. It may have been inspired by Piaget's account of how children construct knowledge of the world by interacting with it. Gary Drescher's book 'Made-up Minds' was seminal for the area. Another more recent predictive learning theory is Jeff Hawkins' memory-prediction framework, which is laid out in his On Intelligence."
"http://dbpedia.org/resource/Inductive_transfer"	"Inductive transfer"	"Machine learning"	"Inductive transfer, or transfer learning, is a research problem in machine learning that focuses on storing knowledge gained while solving one problem and applying it to a different but related problem. For example, the abilities acquired while learning to walk presumably apply when one learns to run, and knowledge gained while learning to recognize cars could apply when recognizing trucks. This area of research bears some relation to the long history of psychological literature on transfer of learning, although formal ties between the two fields are limited. The earliest cited work on transfer in machine learning is attributed to Lorien Pratt who formulated the discriminability-based transfer (DBT) algorithm in 1993. In 1997, the journal Machine Learning published a special issue devoted to Inductive Transfer and by 1998, the field had advanced to include multi-task learning, along with a more formal analysis of its theoretical foundations. Learning to Learn, edited by Sebastian Thrun and Pratt, is a comprehensive overview of the state of the art of inductive transfer at the time of its publication. Inductive transfer has also been applied in cognitive science, with the journal Connection Sciencepublishing a special issue on Reuse of Neural Networks through Transfer in 1996. Notably, scientists have developed algorithms for inductive transfer in Markov logic networks and Bayesian networks. Furthermore, researchers have applied techniques for transfer to problems in text classification, and spam filtering."
"http://dbpedia.org/resource/Cognitive_robotics"	"Cognitive robotics"	"Machine learning"	"Cognitive robotics is concerned with endowing a robot with intelligent behavior by providing it with a processing architecture that will allow it to learn and reason about how to behave in response to complex goals in a complex world. Cognitive robotics may be considered the engineering branch of embodied cognitive science and embodied embedded cognition."
"http://dbpedia.org/resource/Computational_learning_theory"	"Computational learning theory"	"Machine learning"	"In computer science, computational learning theory (or just learning theory) is a subfield of Artificial Intelligence devoted to studying the design and analysis of machine learning algorithms."
"http://dbpedia.org/resource/Conditional_random_field"	"Conditional random field"	"Machine learning"	"Conditional random fields (CRFs) are a class of  statistical modelling method often applied in pattern recognition and machine learning, where they are used for structured prediction. Whereas an ordinary classifier predicts a label for a single sample without regard to ""neighboring"" samples, a CRF can take context into account; e.g., the linear chain CRF popular in natural language processing predicts sequences of labels for sequences of input samples. CRFs are a type of discriminative undirected probabilistic graphical model. It is used to encode known relationships between observations and construct consistent interpretations. It is often used for labeling or parsing of sequential data, such as natural language text or biological sequencesand in computer vision.Specifically, CRFs find applications in shallow parsing,named entity recognition,gene finding and peptide critical functional region finding,among other tasks, being an alternative to the related hidden Markov models (HMMs). In computer vision, CRFs are often used for object recognition and image segmentation."
"http://dbpedia.org/resource/Confusion_matrix"	"Confusion matrix"	"Machine learning"	"In the field of machine learning and specifically the problem of statistical classification, a confusion matrix, also known as an error matrix, is a specific table layout that allows visualization of the performance of an algorithm, typically a supervised learning one (in unsupervised learning it is usually called a matching matrix). Each column of the matrix represents the instances in a predicted class while each row represents the instances in an actual class (or vice versa). The name stems from the fact that it makes it easy to see if the system is confusing two classes (i.e. commonly mislabelling one as another). It is a special kind of contingency table, with two dimensions (""actual"" and ""predicted""), and identical sets of ""classes"" in both dimensions (each combination of dimension and class is a variable in the contingency table)."
"http://dbpedia.org/resource/Empirical_risk_minimization"	"Empirical risk minimization"	"Machine learning"	"Empirical risk minimization (ERM) is a principle in statistical learning theory which defines a family of learning algorithms and is used to give theoretical bounds on the performance of learning algorithms."
"http://dbpedia.org/resource/Feature_(machine_learning)"	"Feature (machine learning)"	"Machine learning"	"In machine learning and pattern recognition, a feature is an individual measurable property of a phenomenon being observed. Choosing informative, discriminating and independent features is a crucial step for effective algorithms in pattern recognition, classification and regression. Features are usually numeric, but structural features such as strings and graphs are used in syntactic pattern recognition.The concept of ""feature"" is related to that of explanatory variable used in statistical techniques such as linear regression. The initial set of raw features can be redundant and too large to be managed. Therefore, a preliminary step in many applications of machine learning and pattern recognition consists of selecting a subset of features, or constructing a new and reduced set of features to facilitate learning, and to improve generalization and interpretability. Extracting or selecting features is a combination of art and science; developing systems to do so is known as feature engineering. It requires the experimentation of multiple possibilities and the combination of automated techniques with the intuition and knowledge of the domain expert. Automating this process is feature learning, where a machine not only uses features for learning, but learns the features itself."
"http://dbpedia.org/resource/Feature_vector"	"Feature vector"	"Machine learning"	"In pattern recognition and machine learning, a feature vector is an n-dimensional vector of numerical features that represent some object. Many algorithms in machine learning require a numerical representation of objects, since such representations facilitate processing andstatistical analysis. When representing images, the feature values might correspond to the pixels of an image, when representing texts perhaps term occurrence frequencies. Feature vectors are equivalent to the vectors of explanatory variables used in statistical procedures such as linear regression. Feature vectors are often combined with weights using a dot product in order to construct a linear predictor function that is used to determine a score for making a prediction. The vector space associated with these vectors is often called the feature space. In order to reduce the dimensionality of the feature space, a number of dimensionality reduction techniques can be employed. Higher-level features can be obtained from already available features and added to the feature vector, for example for the study of diseases the feature 'Age' is useful and is defined as Age = 'Year of death' - 'Year of birth' . This process is referred to as feature construction. Feature construction is the application of a set of constructive operators to a set of existing features resulting in construction of new features. Examples of such constructive operators include checking for the equality conditions {=, ≠}, the arithmetic operators {+,−,×, /}, the array operators {max(S), min(S), average(S)} as well as other more sophisticated operators, for example count(S,C) that counts the number of features in the feature vector S satisfying some condition C or, for example, distances to other recognition classes generalized by some accepting device. Feature construction has long been considered a powerful tool for increasing both accuracy and understanding of structure, particularly in high-dimensional problems. Applications include studies of disease and emotion recognition from speech."
"http://dbpedia.org/resource/Inferential_theory_of_learning"	"Inferential theory of learning"	"Machine learning"	"Inferential theory of learning (ITL) is an area of machine learning which describes inferential processes performed by learning agents. ITL has been developed by Ryszard S. Michalski in 1980s. In ITL learning process is viewed as a search (inference) through hypotheses space guided by a specific goal. Results of learning need to be stored, in order to be used in the future."
"http://dbpedia.org/resource/Knowledge_integration"	"Knowledge integration"	"Machine learning"	"Knowledge integration is the process of synthesizing multiple knowledge models (or representations) into a common model (representation). Compared to information integration, which involves merging information having different schemas and representation models, knowledge integration focuses more on synthesizing the understanding of a given subject from different perspectives. For example, multiple interpretations are possible of a set of student grades, typically each from a certain perspective. An overall, integrated view and understanding of this information can be achieved if these interpretations can be put under a common model, say, a student performance index. The Web-based Inquiry Science Environment (WISE), from the University of California at Berkeley has been developed along the lines of knowledge integration theory. Knowledge integration has also been studied as the process of incorporating new information into a body of existing knowledge with an interdisciplinary approach. This process involves determining how the new information and the existing knowledge interact, how existing knowledge should be modified to accommodate the new information, and how the new information should be modified in light of the existing knowledge. A learning agent that actively investigates the consequences of new information can detect and exploit a variety of learning opportunities; e.g., to resolve knowledge conflicts and to fill knowledge gaps. By exploiting these learning opportunities the learning agent is able to learn beyond the explicit content of the new information. The machine learning program KI, developed by Murray and Porter at the University of Texas at Austin, was created to study the use of automated and semi-automated knowledge integration to assist knowledge engineers constructing a large knowledge base. A possible technique which can be used is semantic matching. More recently, a technique useful to minimize the effort in mapping validation and visualization has been presented which is based on Minimal Mappings. Minimal mappings are high quality mappings such that i) all the other mappings can be computed from them in time linear in the size of the input graphs, and ii) none of them can be dropped without losing property i). The University of Waterloo operates a Bachelor of Knowledge Integration undergraduate degree program as an academic major or minor. The program started in 2008."
"http://dbpedia.org/resource/Matrix_regularization"	"Matrix regularization"	"Machine learning"	"In the field of statistical learning theory, matrix regularization generalizes notions of vector regularization to cases where the object to be learned is a matrix. The purpose of regularization is to enforce conditions, for example sparsity or smoothness, that can produce stable predictive functions. For example, in the more common vector framework, Tikhonov regularization optimizes over to find a vector, , that is a stable solution to the regression problem. When the system is described by a matrix rather than a vector, this problem can be written as where the vector norm enforcing a regularization penalty on has been extended to a matrix norm on . Matrix Regularization has applications in matrix completion, multivariate regression, and multi-task learning. Ideas of feature and group selection can also be extended to matrices, and these can be generalized to the nonparametric case of multiple kernel learning."
"http://dbpedia.org/resource/Statistical_learning_theory"	"Statistical learning theory"	"Machine learning"	"Statistical learning theory is a framework for machine learningdrawing from the fields of statistics and functional analysis. Statistical learning theory deals with the problem of finding a predictive function based on data. Statistical learning theory has led to successful applications in fields such as computer vision, speech recognition, bioinformatics and baseball."
"http://dbpedia.org/resource/Solomonoff's_theory_of_inductive_inference"	"Solomonoff's theory of inductive inference"	"Machine learning"	"Ray Solomonoff's theory of universal inductive inference is a theory of prediction based on logical observations, such as predicting the next symbol based upon a given series of symbols. The only assumption that the theory makes is that the environment follows some unknown but computable probability distribution. It is a mathematical formalization of Occam's razor and the Principle of Multiple Explanations. Prediction is done using a completely Bayesian framework. The universal prior is taken over the class of all computable sequences—this is the universal a priori probability distribution;no computable hypothesis will have a zero probability. This means that Bayes rule of causation can be used in predicting the continuation of any particular computable sequence."
"http://dbpedia.org/resource/Evaluation_of_binary_classifiers"	"Evaluation of binary classifiers"	"Machine learning"	"The evaluation of binary classifiers compares two methods of assigning a binary attribute, one of which is usually a standard method and the other is being investigated. There are many metrics that can be used to measure the performance of a classifier or predictor; different fields have different preferences for specific metrics due to different goals. For example, in medicine sensitivity and specificity are often used, while in computer science precision and recall are preferred. An important distinction is between metrics that are independent on the prevalence (how often each category occurs in the population), and metrics that depend on the prevalence – both types are useful, but they have very different properties."
"http://dbpedia.org/resource/Bias–variance_tradeoff"	"Bias–variance tradeoff"	"Machine learning"	"In statistics and machine learning, the bias–variance tradeoff (or dilemma) is the problem of simultaneously minimizing two sources of error that prevent supervised learning algorithms from generalizing beyond their training set: 
*  The bias is error from erroneous assumptions in the learning algorithm. High bias can cause an algorithm to miss the relevant relations between features and target outputs (underfitting). 
*  The variance is error from sensitivity to small fluctuations in the training set. High variance can cause overfitting: modeling the random noise in the training data, rather than the intended outputs. The bias–variance decomposition is a way of analyzing a learning algorithm's expected generalization error with respect to a particular problem as a sum of three terms, the bias, variance, and a quantity called the irreducible error, resulting from noise in the problem itself. This tradeoff applies to all forms of supervised learning: classification, regression (function fitting), and structured output learning. It has also been invoked to explain the effectiveness of heuristics in human learning."
"http://dbpedia.org/resource/Rule_induction"	"Rule induction"	"Machine learning"	"Rule induction is an area of machine learning in which formal rules are extracted from a set of observations. The rules extracted may represent a full scientific model of the data, or merely represent local patterns in the data."
"http://dbpedia.org/resource/Constrained_conditional_model"	"Constrained conditional model"	"Machine learning"	"A constrained conditional model (CCM) is a machine learning and inference framework that augments the learning of conditional (probabilistic or discriminative) models with declarative constraints. The constraint can be used as a way to incorporate expressive prior knowledge into the model and bias the assignments made by the learned model to satisfy these constraints. The framework can be used to support decisions in an expressive output space while maintaining modularity and tractability of training and inference. Models of this kind have recently attracted much attention within the natural language processing (NLP) community.Formulating problems as constrained optimization problems over the output of learned models has several advantages. It allows one to focus on the modeling of problems by providing the opportunity to incorporate domain-specific knowledge as global constraints using a first order language. Using this declarative framework frees the developer from low level feature engineering while capturing the problem's domain-specific properties and guarantying exact inference. From a machine learning perspective it allows decoupling the stage of model generation (learning) from that of the constrained inference stage, thus helping to simplify the learning stage while improving the quality of the solutions. For example, in the case of generating compressed sentences, rather than simply relying on a language model to retain the most commonly used n-grams in the sentence, constraints can be used to ensure that if a modifier is kept in the compressed sentence, its subject will also be kept."
"http://dbpedia.org/resource/Dropout_(neural_networks)"	"Dropout (neural networks)"	"Machine learning"	"Dropout is a regularization technique for reducing overfitting in neural networks by preventing complex co-adaptations on training data. It is a very efficient way of performing model averaging with neural networks. The term ""dropout"" refers to dropping out units (both hidden and visible) in a neural network."
"http://dbpedia.org/resource/Linear_separability"	"Linear separability"	"Machine learning"	"In Euclidean geometry, linear separability is a geometric property of a pair of sets of points. This is most easily visualized in two dimensions (the Euclidean plane) by thinking of one set of points as being colored blue and the other set of points as being colored red. These two sets are linearly separable if there exists at least one line in the plane with all of the blue points on one side of the line and all the red points on the other side. This idea immediately generalizes to higher-dimensional Euclidean spaces if line is replaced by hyperplane. The problem of determining if a pair of sets is linearly separable and finding a separating hyperplane if they are arises in several areas. In statistics and machine learning, classifying certain types of data is a problem for which good algorithms exist that are based on this concept."
"http://dbpedia.org/resource/Machine_learning"	"Machine learning"	"Machine learning"	"Machine learning is the subfield of computer science that ""gives computers the ability to learn without being explicitly programmed"" (Arthur Samuel, 1959). Evolved from the study of pattern recognition and computational learning theory in artificial intelligence, machine learning explores the study and construction of algorithms that can learn from and make predictions on data – such algorithms overcome following strictly static program instructions by making data-driven predictions or decisions, through building a model from sample inputs. Machine learning is employed in a range of computing tasks where designing and programming explicit algorithms is unfeasible; example applications include spam filtering, optical character recognition (OCR), search engines and computer vision. Machine learning is closely related to (and often overlaps with) computational statistics, which also focuses in prediction-making through the use of computers. It has strong ties to mathematical optimization, which delivers methods, theory and application domains to the field. Machine learning is sometimes conflated with data mining, where the latter subfield focuses more on exploratory data analysis and is known as unsupervised learning. Within the field of data analytics, machine learning is a method used to devise complex models and algorithms that lend themselves to prediction; in commercial use, this is known as predictive analytics. These analytical models allow researchers, data scientists, engineers, and analysts to ""produce reliable, repeatable decisions and results"" and uncover ""hidden insights"" through learning from historical relationships and trends in the data."
"http://dbpedia.org/resource/Multivariate_adaptive_regression_splines"	"Multivariate adaptive regression splines"	"Machine learning"	"In statistics, multivariate adaptive regression splines (MARS) is a form of regression analysis introduced by Jerome H. Friedman in 1991. It is a non-parametric regression techniqueand can be seen as an extension of linear models thatautomatically models nonlinearities and interactions between variables. The term ""MARS"" is trademarked and licensed to Salford Systems. In order to avoid trademark infringements, many open source implementations of MARS are called ""Earth""."
"http://dbpedia.org/resource/Orange_(software)"	"Orange (software)"	"Machine learning"	"Orange is a free software machine learning and data mining package (written in Python). It has a visual programming front-end for explorative data analysis and visualization, and can also be used as a Python library. The program is maintained and developed by the Bioinformatics Laboratory of the Faculty of Computer and Information Science at University of Ljubljana."
"http://dbpedia.org/resource/Semi-supervised_learning"	"Semi-supervised learning"	"Machine learning"	"Semi-supervised learning is a class of supervised learning tasks and techniques that also make use of unlabeled data for training – typically a small amount of labeled data with a large amount of unlabeled data. Semi-supervised learning falls between unsupervised learning (without any labeled training data) and supervised learning (with completely labeled training data). Many machine-learning researchers have found that unlabeled data, when used in conjunction with a small amount of labeled data, can produce considerable improvement in learning accuracy. The acquisition of labeled data for a learning problem often requires a skilled human agent (e.g. to transcribe an audio segment) or a physical experiment (e.g. determining the 3D structure of a protein or determining whether there is oil at a particular location). The cost associated with the labeling process thus may render a fully labeled training set infeasible, whereas acquisition of unlabeled data is relatively inexpensive. In such situations, semi-supervised learning can be of great practical value. Semi-supervised learning is also of theoretical interest in machine learning and as a model for human learning. As in the supervised learning framework, we are given a set of independently identically distributed examples with corresponding labels . Additionally, we are given unlabeled examples . Semi-supervised learning attempts to make use of this combined information to surpass the classification performance that could be obtained either by discarding the unlabeled data and doing supervised learning or by discarding the labels and doing unsupervised learning. Semi-supervised learning may refer to either transductive learning or inductive learning. The goal of transductive learning is to infer the correct labels for the given unlabeled data only. The goal of inductive learning is to infer the correct mapping from to . Intuitively, we can think of the learning problem as an exam and labeled data as the few example problems that the teacher solved in class. The teacher also provides a set of unsolved problems. In the transductive setting, these unsolved problems are a take-home exam and you want to do well on them in particular. In the inductive setting, these are practice problems of the sort you will encounter on the in-class exam. It is unnecessary (and, according to Vapnik's principle, imprudent) to perform transductive learning by way of inferring a classification rule over the entire input space; however, in practice, algorithms formally designed for transduction or induction are often used interchangeably."
"http://dbpedia.org/resource/Stability_(learning_theory)"	"Stability (learning theory)"	"Machine learning"	"Stability, also known as algorithmic stability, is a notion in computational learning theory of how a  machine learning algorithm is perturbed by small changes to its inputs. A stable learning algorithm is one for which the prediction does not change much when the training data is modified slightly. For instance, consider a machine learning algorithm that is being trained to recognize handwritten letters of the alphabet, using 1000 examples of handwritten letters and their labels (""A"" to ""Z"") as a training set. One way to modify this training set is to leave out an example, so that only 999 examples of handwritten letters and their labels are available. A stable learning algorithm would produce a similar classifier with both the 1000-element and 999-element training sets. Stability can be studied for many types of learning problems, from language learning to inverse problems in physics and engineering, as it is a property of the learning process rather than the type of information being learned. The study of stability gained importance in computational learning theory in the 2000s when it was shown to have a connection with generalization. It was shown that for large classes of learning algorithms, notably empirical risk minimization algorithms, certain types of stability ensure good generalization."
"http://dbpedia.org/resource/Version_space_learning"	"Version space learning"	"Machine learning"	"Version space learning is a logical approach to machine learning, specifically binary classification. Version space learning algorithms search a predefined space of hypotheses, viewed as a set of logical sentences. Formally, the hypothesis space is a disjunction (i.e., either hypothesis 1 is true, or hypothesis 2, or any subset of the hypotheses 1 through n). A version space learning algorithm is presented with examples, which it will use to restrict its hypothesis space; for each example x, the hypotheses that are inconsistent with x are removed from the space. This iterative refining of the hypothesis space is called the candidate elimination algorithm, the hypothesis space maintained inside the algorithm its version space."
"http://dbpedia.org/resource/Category_utility"	"Category utility"	"Machine learning"	"Category utility is a measure of ""category goodness"" defined in  and . It attempts to maximize both the probability that two objects in the same category have attribute values in common, and the probability that objects from different categories have different attribute values. It was intended to supersede more limited measures of category goodness such as ""cue validity"" (; ) and ""collocation index"" (). It provides a normative information-theoretic measure of the predictive advantage gained by the observer who possesses knowledge of the given category structure (i.e., the class labels of instances) over the observer who does not possess knowledge of the category structure. In this sense the motivation for the category utility measure is similar to the information gain metric used in decision tree learning. In certain presentations, it is also formally equivalent to the mutual information, as discussed below. A review of category utility in its probabilistic incarnation, with applications to machine learning, is provided in , pp. 260–262)."
"http://dbpedia.org/resource/Domain_adaptation"	"Domain adaptation"	"Machine learning"	"Domain Adaptation is a field associated with machine learning and transfer learning. This scenario arises when we aim at learning from a source data distribution a well performing model on a different (but related) target data distribution. For instance, one of the tasks of the common spam filtering problem consists in adapting a model from one user (the source distribution) to a new one who receives significantly different emails (the target distribution).Note that, when more than one source distribution is available the problem is referred to as multi-source domain adaptation."
"http://dbpedia.org/resource/Savi_Technology"	"Savi Technology"	"Machine learning"	"Savi Technology was founded in 1989 and is based in Alexandria, Virginia. Savi provides the most complete Sensor Analytics solutions for organizations that face critical decisions based on the location and status of their assets."
"http://dbpedia.org/resource/Skymind"	"Skymind"	"Machine learning"	"Skymind is a machine intelligence company supporting the open source deep learning framework Deeplearning4j and the JVM-based scientific computing library ND4J. The company was founded in 2014 by Adam Gibson and Chris Nicholson, and is headquartered in San Francisco, California. It is privately funded."
"http://dbpedia.org/resource/AlphaGo"	"AlphaGo"	"Machine learning"	"AlphaGo is a computer program developed by Google DeepMind in London to play the board game Go. In October 2015, it became the first Computer Go program to beat a professional human Go player without handicaps on a full-sized 19×19 board. In March 2016, it beat Lee Sedol in a five-game match, the first time a computer Go program has beaten a 9-dan professional without handicaps. Although it lost to Lee Sedol in the fourth game, Lee resigned the final game, giving a final score of 4 games to 1 in favour of AlphaGo. In recognition of beating Lee Sedol, AlphaGo was awarded an honorary 9-dan by the Korea Baduk Association. AlphaGo's algorithm uses a Monte Carlo tree search to find its moves based on knowledge previously ""learned"" by machine learning, specifically by an artificial neural network (a deep learning method) by extensive training, both from human and computer play."
"http://dbpedia.org/resource/Dataiku"	"Dataiku"	"Machine learning"	"Dataiku is a French computer software startup company headquartered in Paris. The company develops a collaborative data science software platform aimed towards helping data teams deliver Big data solutions and services efficiently."
"http://dbpedia.org/resource/MysteryVibe"	"MysteryVibe"	"Machine learning"	"MysteryVibe is a British health & lifestyle brand that designs, develops and manufactures luxury pleasure products in the UK. While only 2 years old, they have witnessed a phenomenal rise with customers in 50+ countries and features in leading conferences and major publications including Glamour, Elle, WSJ, CNBC, The Guardian, FastCompany, TechCrunch, NBC, VentureBeat, Psychologies, Stylist and Evening Standard. MysteryVibe is the creator of Crescendo, a pleasure toy that is renowned for many world firsts: platform toy, body-adapting vibrator, six-motor massager, designed by 1,000 users in 51 countries. It is unique in the way it helps users discover and stimulate their erogenous zones by adapting to their bodies with the help of its proprietary bend-and-hold technology. MysteryVibe is headquartered in London, UK and the first company in their class to have been created as a collective of world leading partners - Design with Seymourpowell; Software with Fueled & Untitled Kingdom; and Sensory experiences with CondimentJunkie. They are also unique in their focus on collaboration with Tech giants and are building their company with close support from Google Launchpad, Amazon AWS Activate, IBM Global Entrepreneur and Facebook FbStart."
"http://dbpedia.org/resource/Developmental_robotics"	"Developmental robotics"	"Machine learning"	"Developmental robotics (DevRob), sometimes called epigenetic robotics, is a scientific field which aims at studying the developmental mechanisms, architectures and constraints that allow lifelong and open-ended learning of new skills and new knowledge in embodied machines. As in human children, learning is expected to be cumulative and of progressively increasing complexity, and to result from self-exploration of the world in combination with social interaction. The typical methodological approach consists in starting from theories of human and animal development elaborated in fields such as developmental psychology, neuroscience, developmental and evolutionary biology, and linguistics, then to formalize and implement them in robots, sometimes exploring extensions or variants of them. The experimentation of those models in robots allows researchers to confront them with reality, and as a consequence developmental robotics also provides feedback and novel hypotheses on theories of human and animal development. Developmental robotics is related to, but differs from, evolutionary robotics (ER). ER uses populations of robots that evolve over time, whereas DevRob is interested in how the organization of a single robot's control system develops through experience, over time. DevRob is also related to work done in the domains of robotics and artificial life."
"http://dbpedia.org/resource/Generative_model"	"Generative model"	"Machine learning"	"In probability and statistics, a generative model is a model for randomly generating observable data values, typically given some hidden parameters. It specifies a joint probability distribution over observation and label sequences. Generative models are used in machine learning for either modeling data directly (i.e., modeling observations drawn from a probability density function), or as an intermediate step to forming a conditional probability density function. A conditional distribution can be formed from a generative model through Bayes' rule. Shannon (1948) gives an example in which a table of frequencies of English word pairs is used to generate a sentence beginning with ""representing and speedily is an good""; which is not proper English but which will increasingly approximate it as the table is moved from word pairs to word triplets etc. Generative models contrast with discriminative models, in that a generative model is a full probabilistic model of all variables, whereas a discriminative model provides a model only for the target variable(s) conditional on the observed variables. Thus a generative model can be used, for example, to simulate (i.e. generate) values of any variable in the model, whereas a discriminative model allows only sampling of the target variables conditional on the observed quantities. Despite the fact that discriminative models do not need to model the distribution of the observed variables, they cannot generally express more complex relationships between the observed and target variables. They don't necessarily perform better than generative models at classification and regression tasks. In modern applications the two classes are seen as complementary or as different views of the same procedure. Examples of generative models include: 
*  Gaussian mixture model and other types of mixture model 
*  Hidden Markov model 
*  Probabilistic context-free grammar 
*  Naive Bayes 
*  Averaged one-dependence estimators 
*  Latent Dirichlet allocation 
*  Restricted Boltzmann machine 
*  Generative adversarial networks If the observed data are truly sampled from the generative model, then fitting the parameters of the generative model to maximize the data likelihood is a common method. However, since most statistical models are only approximations to the true distribution, if the model's application is to infer about a subset of variables conditional on known values of others, then it can be argued that the approximation makes more assumptions than are necessary to solve the problem at hand. In such cases, it can be more accurate to model the conditional density functions directly using a discriminative model (see above), although application-specific details will ultimately dictate which approach is most suitable in any particular case."
"http://dbpedia.org/resource/Lazy_learning"	"Lazy learning"	"Machine learning"	"In machine learning, lazy learning is a learning method in which generalization beyond the training data is delayed until a query is made to the system, as opposed to in eager learning, where the system tries to generalize the training data before receiving queries. The main advantage gained in employing a lazy learning method, such as Case based reasoning, is that the target function will be approximated locally, such as in the k-nearest neighbor algorithm. Because the target function is approximated locally for each query to the system, lazy learning systems can simultaneously solve multiple problems and deal successfully with changes in the problem domain. The disadvantages with lazy learning include the large space requirement to store the entire training dataset. Particularly noisy training data increases the case base unnecessarily, because no abstraction is made during the training phase. Another disadvantage is that lazy learning methods are usually slower to evaluate, though this is coupled with a faster training phase. Lazy classifiers are most useful for large datasets with few attributes."
"http://dbpedia.org/resource/Bag-of-words_model"	"Bag-of-words model"	"Machine learning"	"The bag-of-words model is a simplifying representation used in natural language processing and information retrieval (IR). In this model, a text (such as a sentence or a document) is represented as the bag (multiset) of its words, disregarding grammar and even word order but keeping multiplicity. The bag-of-words model has also been used for computer vision. The bag-of-words model is commonly used in methods of document classification where the (frequency of) occurrence of each word is used as a feature for training a classifier. An early reference to ""bag of words"" in a linguistic context can be found in Zellig Harris's 1954 article on Distributional Structure."
"http://dbpedia.org/resource/Learning_to_rank"	"Learning to rank"	"Machine learning"	"Learning to rank or machine-learned ranking (MLR) is the application of machine learning, typically supervised, semi-supervised or reinforcement learning, in the construction of ranking models for information retrieval systems. Training data consists of lists of items with some partial order specified between items in each list. This order is typically induced by giving a numerical or ordinal score or a binary judgment (e.g. ""relevant"" or ""not relevant"") for each item. The ranking model's purpose is to rank, i.e. produce a permutation of items in new, unseen lists in a way which is ""similar"" to rankings in the training data in some sense."
"http://dbpedia.org/resource/Quantum_machine_learning"	"Quantum machine learning"	"Machine learning"	"Quantum machine learning is a newly emerging interdisciplinary research area between quantum physics and computer science that summarises efforts to combine quantum mechanics with methods of machine learning. Quantum machine learning models or algorithms intend to use the advantages of quantum information in order to improve classical methods of machine learning, for example by developing efficient implementations of expensive classical algorithms on a quantum computer. However, quantum machine learning also includes the vice versa approach, namely applying classical methods of machine learning to quantum information theory. Although yet in its infancy, quantum machine learning is met with high expectations of providing a solution for big data analysis using the ‘parallel’ power of quantum computation. This trend is underlined by recent investments of companies such as Google and Microsoft into quantum computing hardware and research. However, quantum machine learning is still in its infancy and requires more theoretical foundations as well as solid scientific results in order to mature to a full academic discipline."
"http://dbpedia.org/resource/Learnable_function_class"	"Learnable function class"	"Machine learning"	"In statistical learning theory, a learnable function class is a set of functions for which an algorithm can be devised to asymptotically minimize the expected risk, uniformly over all probability distributions. The concept of learnable classes are closely related to regularization in machine learning, and provides large sample justifications for certain learning algorithms."
"http://dbpedia.org/resource/Offline_learning"	"Offline learning"	"Machine learning"	"In machine learning, systems which employ offline learning do not change their approximation of the target function when the initial training phase has been completed. These systems are also typically examples of eager learning."
"http://dbpedia.org/resource/Occam_learning"	"Occam learning"	"Machine learning"	"In computational learning theory, Occam learning is a model of algorithmic learning where the objective of the learner is to output a succinct representation of received training data. This is closely related to probably approximately correct (PAC) learning, where the learner is evaluated on its predictive power of a test set. Occam learnability implies PAC learning, and for a wide variety of concept classes, the converse is also true: PAC learnability implies Occam learnability."
"http://dbpedia.org/resource/Random_projection"	"Random projection"	"Machine learning"	"In mathematics and statistics, random projection is a technique used to reduce the dimensionality of a set of points which lie in Euclidean space. Random projection methods are powerful methods known for their simplicity and less erroneous output compared with other methods. According to experimental results, random projection preserve distances well, but empirical results are sparse."
"http://dbpedia.org/resource/Accuracy_paradox"	"Accuracy paradox"	"Machine learning"	"The accuracy paradox for predictive analytics states that predictive models with a given level of accuracy may have greater predictive power than models with higher accuracy. It may be better to avoid the accuracy metric in favor of other metrics such as precision and recall. Accuracy is often the starting point for analyzing the quality of a predictive model, as well as an obvious criterion for prediction. Accuracy measures the ratio of correct predictions to the total number of cases evaluated. It may seem obvious that the ratio of correct predictions to cases should be a key metric. A predictive model may have high accuracy, but be useless. In an example predictive model for an insurance fraud application, all cases that are predicted as high-risk by the model will be investigated. To evaluate the performance of the model, the insurance company has created a sample data set of 10,000 claims. All 10,000 cases in the validation sample have been carefully checked and it is known which cases are fraudulent. To analyze the quality of the model, the insurance uses the table of confusion. The definition of accuracy, the table of confusion for model M1Fraud, and the calculation of accuracy for model M1Fraud is shown below. where TN is the number of true negative cases FP is the number of false positive cases FN is the number of false negative cases TP is the number of true positive cases Formula 1: Definition of Accuracy Table 1: Table of Confusion for Fraud Model M1Fraud. Formula 2: Accuracy for model M1Fraud With an accuracy of 98.0% model M1Fraud appears to perform fairly well. The paradox lies in the fact that accuracy can be easily improved to 98.5% by always predicting ""no fraud"". The table of confusion and the accuracy for this trivial “always predict negative” model M2Fraud and the accuracy of this model are shown below. Table 2: Table of Confusion for Fraud Model M2Fraud. Formula 3: Accuracy for model M2Fraud Model M2Fraudreduces the rate of inaccurate predictions from 2% to 1.5%. This is an apparent improvement of 25%. The new model M2Fraud shows fewer incorrect predictions and markedly improved accuracy, as compared to the original model M1Fraud, but is obviously useless. The alternative model M2Fraud does not offer any value to the company for preventing fraud. The less accurate model is more useful than the more accurate model. Model improvements should not be measured in terms of accuracy gains. It may be going too far to say that accuracy is irrelevant, but caution is advised when using accuracy in the evaluation of predictive models."
"http://dbpedia.org/resource/User_behavior_analytics"	"User behavior analytics"	"Machine learning"	"User behavior analytics (""UBA"") as defined by Gartner, is a cybersecurity process about detection of insider threats, targeted attacks, and financial fraud. UBA solutions look at patterns of human behavior, and then apply algorithms and statistical analysis to detect meaningful anomalies from those patterns—anomalies that indicate potential threats.' Instead of tracking devices or security events, UBA tracks a system's users. Big data platforms like Apache Hadoop are increasing UBA functionality by allowing them to analyze petabytes worth of data to detect insider threats and advanced persistent threats. The problem UBA responds to, as described by Nemertes Research CEO Johna Till Johnson, is that ""Security systems provide so much information that it's tough to uncover information that truly indicates a potential for real attack. Analytics tools help make sense of the vast amount of data that SIEM, IDS/IPS, system logs, and other tools gather. UBA tools use a specialized type of security analytics that focuses on the behavior of systems and the people using them. UBA technology first evolved in the field of marketing, to help companies understand and predict consumer-buying patterns. But as it turns out, UBA can be extraordinarily useful in the security context too."" Developments in UBA technology led Gartner to also recognize user and entity behavior analytics (""UEBA""). This expanded definition includes devices, applications, servers, data, or anything with an IP address. ""When end users have been compromised, malware can lay dormant and go undetected for months. Rather than trying to find where the outsider entered, UEBAs allow for quicker detection by using algorithms to detect insider threats."" Particularly in the computer security market, there are many vendors for UEBA applications. They can be ""differentiated by whether they are designed to monitor on-premises or cloud-based software as a service (SaaS) applications; the methods in which they obtain the source data; the type of analytics they use (i.e., packaged analytics, user-driven or vendor-written), and the service delivery method (i.e., on-premises or a cloud-based)."" Though not intended to be an exhaustive list, representative vendors include: 
*  Microsoft ATA (Advanced Threat Analytics) 
*  Veriato 
*  Bay Dynamics 
*  Darktrace 
*  E8 Security 
*  Exabeam 
*  Fortscale 
*  Gurucul 
*  LightCyber 
*  Niara 
*  Preempt 
*  Securonix 
*  Sqrrl 
*  Splunk 
*  Varonis According to the 2015 market guide released by Gartner, ""the UEBA market grew substantially in 2015; UEBA vendors grew their customer base, market consolidation began, and Gartner client interest in UEBA and security analytics increased."" The report further projected, ""Over the next three years, leading UEBA platforms will become preferred systems for security operations and investigations at some of the organizations they serve. It will be—and in some cases already is—much easier to discover some security events and analyze individual offenders in UEBA than it is in many legacy security monitoring systems."""
"http://dbpedia.org/resource/Binary_classification"	"Binary classification"	"Machine learning"	"Binary or binomial classification is the task of classifying the elements of a given set into two groups on the basis of a classification rule. Instancing a decision whether an item has or not some qualitative property, some specified characteristic, some typical binary classification tasks are: 
*  Medical testing to determine if a patient has certain disease or not – the classification property is the presence of the disease. 
*  A ""pass or fail"" test method or quality control in factories, i.e. deciding if a specification has or has not been met – a Go/no go classification. 
*  Information retrieval, namely deciding whether a page or an article should be in the result set of a search or not – the classification property is the relevance of the article, or the usefulness to the user. Binary classification is dichotomization applied to practical purposes, and therefore an important point is that in many practical binary classification problems, the two groups are not symmetric – rather than overall accuracy, the relative proportion of different types of errors is of interest. For example, in medical testing, a false positive (detecting a disease when it is not present) is considered differently from a false negative (not detecting a disease when it is present). Porting human discriminative abilities to scientific soundness and technical practice is far from trivial."
"http://dbpedia.org/resource/Early_stopping"	"Early stopping"	"Machine learning"	"In machine learning, early stopping is a form of regularization used to avoid overfitting when training a learner with an iterative method, such as gradient descent. Such methods update the learner so as to make it better fit the training data with each iteration. Up to a point, this improves the learner's performance on data outside of the training set. Past that point, however, improving the learner's fit to the training data comes at the expense of increased generalization error. Early stopping rules provide guidance as to how many iterations can be run before the learner begins to over-fit. Early stopping rules have been employed in many different machine learning methods, with varying amounts of theoretical foundation."
"http://dbpedia.org/resource/Unsupervised_learning"	"Unsupervised learning"	"Machine learning"	"Unsupervised learning is the machine learning task of inferring a function to describe hidden structure from unlabeled data. Since the examples given to the learner are unlabeled, there is no error or reward signal to evaluate a potential solution – this distinguishes unsupervised learning from supervised learning and reinforcement learning. Unsupervised learning is closely related to the problem of density estimation in statistics. However, unsupervised learning also encompasses many other techniques that seek to summarize and explain key features of the data. Approaches to unsupervised learning include: 
*  clustering 
*  k-means 
*  mixture models 
*  hierarchical clustering, 
*  anomaly detection 
*  Neural Networks 
* Hebbian Learning 
*  Approaches for learning latent variable models such as 
*  Expectation–maximization algorithm (EM) 
*  Method of moments 
*  Blind signal separation techniques, e.g., 
*  Principal component analysis, 
*  Independent component analysis, 
*  Non-negative matrix factorization, 
*  Singular value decomposition."
"http://dbpedia.org/resource/Product_of_experts"	"Product of experts"	"Machine learning"	"Product of experts (PoE) is a machine learning technique. It models a probability distribution by combining the output from several simpler distributions.It was proposed by Geoff Hinton, along with an algorithm for training the parameters of such a system. The core idea is to combine several probability distributions (""experts"") by multiplying their density functions—making the PoE classification similar to an ""and"" operation. This allows each expert to make decisions on the basis of a few dimensions without having to cover the full dimensionality of a problem. This is related to (but quite different from) a mixture model, where several probability distributions are combined via an ""or"" operation, which is a weighted sum of their density functions."
"http://dbpedia.org/resource/Bongard_problem"	"Bongard problem"	"Machine learning"	"A Bongard problem is a kind of puzzle invented by the Russian computer scientist Mikhail Moiseevich Bongard (Михаил Моисеевич Бонгард, 1924–1971), probably in the mid-1960s. They were published in his eponymous 1967 book on pattern recognition. Bongard, in the introduction of the book (which deals with a number of topics including perceptrons) credits the ideas in it to a group including M. N. Vaintsvaig, V. V. Maksimov, and M. S. Smirnov."
"http://dbpedia.org/resource/Concept_drift"	"Concept drift"	"Machine learning"	"In predictive analytics and machine learning, the concept drift means that the statistical properties of the target variable, which the model is trying to predict, change over time in unforeseen ways. This causes problems because the predictions become less accurate as time passes. The term concept refers to the quantity to be predicted. More generally, it can also refer to other phenomena of interest besides the target concept, such as an input, but, in the context of concept drift, the term commonly refers to the target variable."
"http://dbpedia.org/resource/Subclass_reachability"	"Subclass reachability"	"Machine learning"	"In computational learning theory in mathematics, given a class of concepts C, a subclass D is reachable if there exists a partial approximation S of some concept such that D contains exactly those concepts in C that are extensions to S (i.e., D=C|S)."
"http://dbpedia.org/resource/Mixture_model"	"Mixture model"	"Machine learning"	"In statistics, a mixture model is a probabilistic model for representing the presence of subpopulations within an overall population, without requiring that an observed data set should identify the sub-population to which an individual observation belongs. Formally a mixture model corresponds to the mixture distribution that represents the probability distribution of observations in the overall population. However, while problems associated with ""mixture distributions"" relate to deriving the properties of the overall population from those of the sub-populations, ""mixture models"" are used to make statistical inferences about the properties of the sub-populations given only observations on the pooled population, without sub-population identity information. Some ways of implementing mixture models involve steps that attribute postulated sub-population-identities to individual observations (or weights towards such sub-populations), in which case these can be regarded as types of unsupervised learning or clustering procedures. However, not all inference procedures involve such steps. Mixture models should not be confused with models for compositional data, i.e., data whose components are constrained to sum to a constant value (1, 100%, etc.). However, compositional models can be thought of as mixture models, where members of the population are sampled at random. Conversely, mixture models can be thought of as compositional models, where the total size of the population has been normalized to 1."
"http://dbpedia.org/resource/Robot_learning"	"Robot learning"	"Machine learning"	"Robot learning is a research field at the intersection of machine learning and robotics. It studies techniques allowing a robot to acquire novel skills or adapt to its environment through learning algorithms. The embodiment of the robot, situated in a physical embedding, provides at the same time specific difficulties (e.g. high-dimensionality, real time constraints for collecting data and learning) and opportunities for guiding the learning process (e.g. sensorimotor synergies, motor primitives). Example of skills that are targeted by learning algorithms include sensorimotor skills such as locomotion, grasping, active object categorization, as well as interactive skills such as joint manipulation of an object with a human peer, and linguistic skills such as the grounded and situated meaning of human language. Learning can happen either through autonomous self-exploration or through guidance from a human teacher, like for example in robot learning by imitation. Robot learning can be closely related to adaptive control, reinforcement learning as well as developmental robotics which considers the problem of autonomous lifelong acquisition of repertoires of skills.While machine learning is frequently used by computer vision algorithms employed in the context of robotics, these applications are usually not referred to as ""robot learning""."
"http://dbpedia.org/resource/Deeplearning4j"	"Deeplearning4j"	"Machine learning"	"Deeplearning4j is a deep learning programming library written for Java and the Java virtual machine (JVM) and a computing framework with wide support for deep learning algorithms. Deeplearning4j includes implementations of the restricted Boltzmann machine, deep belief net, deep autoencoder, stacked denoising autoencoder and recursive neural tensor network, word2vec, doc2vec, and GloVe. These algorithms all include distributed parallel versions that integrate with Apache Hadoop and Spark. Deeplearning4j is open-source software released under Apache License 2.0, developed mainly by a machine learning group in San Francisco led by Adam Gibson. It is supported commercially by the startup Skymind. It is the only open-source project listed on Google's Word2vec page for its Java implementation."
"http://dbpedia.org/resource/OpenNN"	"OpenNN"	"Machine learning"	"OpenNN (Open Neural Networks Library) is a software library written in the C++ programming language which implements neural networks, a main area of deep learning research. The library is open source, licensed under the GNU Lesser General Public License."
"http://dbpedia.org/resource/DeepMind"	"DeepMind"	"Machine learning"	"DeepMind Technologies Limited is a British artificial intelligence company founded in September 2010. It was acquired by Google in 2014. The company has created a neural network that learns how to play video games in a fashion similar to that of humans, as well as a Neural Turing Machine, or a neural network that may be able to access an external memory like a conventional Turing machine, resulting in a computer that mimics the short-term memory of the human brain. The company made headlines in 2016 after its AlphaGo program beat a human professional Go player for the first time."
"http://dbpedia.org/resource/Logic_learning_machine"	"Logic learning machine"	"Machine learning"	"Logic Learning Machine (LLM) is a machine learning method based on the generation of intelligible rules. LLM is an efficient implementation of the Switching Neural Network (SNN) paradigm, developed by Marco Muselli, from the Italian National Research Council.Logic Learning Machine is implemented in the Rulex suite. LLM has been employed in different fields, including orthopaedic patient classification, DNA microarray analysis  and Clinical Decision Support System."
"http://dbpedia.org/resource/Instantaneously_trained_neural_networks"	"Instantaneously trained neural networks"	"Machine learning"	"Instantaneously trained neural networks are feedforward artificial neural networks that create a new hidden neuron node for each novel training sample. The weights to this hidden neuron separate out not only this training sample but others that are near it, thus providing generalization. This training can be done in a variety of ways and the most popular network in this family is called the CC4 network where the separation is done using the nearest hyperplane that can be written down instantaneously. These networks use unary coding for an effective representation of the data sets. Instantaneously trained neural networks have been proposed as models of short term learning and used in web search, and financial time series prediction applications. They have also been used in instant classification of documents and for deep learning and data mining. As in other neural networks, their normal use is as software, but they have also been implemented in hardware using FPGAs and by optical implementation."
"http://dbpedia.org/resource/Hyperparameter_optimization"	"Hyperparameter optimization"	"Machine learning"	"In the context of machine learning, hyperparameter optimization or model selection is the problem of choosing a set of hyperparameters for a learning algorithm, usually with the goal of optimizing a measure of the algorithm's performance on an independent data set. Often cross-validation is used to estimate this generalization performance. Hyperparameter optimization contrasts with actual learning problems, which are also often cast as optimization problems, but optimize a loss function on the training set alone. In effect, learning algorithms learn parameters that model/reconstruct their inputs well, while hyperparameter optimization is to ensure the model does not overfit its data by tuning, e.g., regularization."
"http://dbpedia.org/resource/Feature_hashing"	"Feature hashing"	"Machine learning"	"In machine learning, feature hashing, also known as the hashing trick (by analogy to the kernel trick), is a fast and space-efficient way of vectorizing features, i.e. turning arbitrary features into indices in a vector or matrix. It works by applying a hash function to the features and using their hash values as indices directly, rather than looking the indices up in an associative array."
"http://dbpedia.org/resource/Inductive_programming"	"Inductive programming"	"Machine learning"	"Inductive programming (IP) is a special area of automatic programming, covering research from artificial intelligence and programming, which addresses learning of typically declarative (logic or functional) and often recursive programs from incomplete specifications, such as input/output examples or constraints. Depending on the programming language used, there are several kinds of inductive programming. Inductive functional programming, which uses functional programming languages such as Lisp or Haskell, and most especially inductive logic programming, which uses logic programming languages such as Prolog and other logical representations such as description logics, have been more prominent, but other (programming) language paradigms have also been used, such as constraint programming or probabilistic programming."
"http://dbpedia.org/resource/Darkforest"	"Darkforest"	"Machine learning"	"Darkforest is a computer go program developed by Facebook, based on deep learning techniques using a convolutional neural network. Its updated version Darkfores2 combines the techniques of its predecessor with Monte Carlo tree search. The MCTS effectively takes tree search methods commonly seen in computer chess programs and randomizes them. With the update, the system is known as Darkfmcts3. Darkforest is of similar strength to programs like CrazyStone and Zen. It has not been tested an against professional human player, however, Google's AlphaGo program won against a professional player in October 2015 using a similar combination of techniques . Darkforest is named after Liu Cixin's science fiction novel The Dark Forest."
"http://dbpedia.org/resource/Feature_learning"	"Feature learning"	"Machine learning"	"In machine learning, feature learning or representation learning is a set of techniques that learn a feature: a transformation of raw data input to a representation that can be effectively exploited in machine learning tasks. This obviates manual feature engineering, which is otherwise necessary, and allows a machine to both learn at a specific task (using the features) and learn the features themselves: to learn how to learn. Feature learning is motivated by the fact that machine learning tasks such as classification often require input that is mathematically and computationally convenient to process. However, real-world data such as images, video, and sensor measurement is usually complex, redundant, and highly variable. Thus, it is necessary to discover useful features or representations from raw data. Traditional hand-crafted features often require expensive human labor and often rely on expert knowledge. Also, they normally do not generalize well. This motivates the design of efficient feature learning techniques, to automate and generalize this. Feature learning can be divided into two categories: supervised and unsupervised feature learning, analogous to these categories in machine learning generally. 
* In supervised feature learning, features are learned with labeled input data. Examples include neural networks, multilayer perceptron, and (supervised) dictionary learning. 
* In unsupervised feature learning, features are learned with unlabeled input data. Examples include dictionary learning, independent component analysis, sparse autoencoders, matrix factorization, and various forms of clustering."
"http://dbpedia.org/resource/Word2vec"	"Word2vec"	"Machine learning"	"Word2vec is a group of related models that are used to produce word embeddings. These models are shallow, two-layer neural networks that are trained to reconstruct linguistic contexts of words. Word2vec takes as its input a large corpus of text and produces a high-dimensional space (typically of several hundred dimensions), with each unique word in the corpus being assigned a corresponding vector in the space. Word vectors are positioned in the vector space such that words that share common contexts in the corpus are located in close proximity to one another in the space. Word2vec was created by a team of researchers led by Tomas Mikolov at Google. The algorithm has been subsequently analysed and explained by other researchers and a Bayesian version of the algorithm is proposed as well. Embedding vectors created using the Word2vec algorithm have many advantages compared to earlier algorithms like Latent Semantic Analysis."
"http://dbpedia.org/resource/AIXI"	"AIXI"	"Machine learning"	"AIXI ['ai̯k͡siː] is a theoretical mathematical formalism for artificial general intelligence.It combines Solomonoff induction with sequential decision theory.AIXI was first proposed by Marcus Hutter in 2000 and the results below are proved in Hutter's 2005 book Universal Artificial Intelligence. AIXI is a reinforcement learning agent;it maximizes the expected total rewards received from the environment.Intuitively, it simultaneously considers every computable hypothesis.In each time step, it looks at every possible program and evaluates how many rewards that program generates depending on the next action taken.The promised rewards are then weighted by the subjective belief that this program constitutes the true environment.This belief is computed from the length of the program: longer programs are considered less likely, in line with Occam's razor. AIXI then selects the action that has the highest expected total reward in the weighted sum of all these programs."
"http://dbpedia.org/resource/Kernel_embedding_of_distributions"	"Kernel embedding of distributions"	"Machine learning"	"In machine learning, the kernel embedding of distributions (also called the kernel mean or mean map) comprises a class of nonparametric methods in which a probability distribution is represented as an element of a reproducing kernel Hilbert space (RKHS). A generalization of the individual data-point feature mapping done in classical kernel methods, the embedding of distributions into infinite-dimensional feature spaces can preserve all of the statistical features of arbitrary distributions, while allowing one to compare and manipulate distributions using Hilbert space operations such as inner products, distances, projections, linear transformations, and spectral analysis. This learning framework is very general and can be applied to distributions over any space on which a sensible kernel function (measuring similarity between elements of ) may be defined. For example, various kernels have been proposed for learning from data which are: vectors in , discrete classes/categories, strings, graphs/networks, images, time series, manifolds, dynamical systems, and other structured objects. The theory behind kernel embeddings of distributions has been primarily developed by Alex Smola, Le Song , Arthur Gretton, and Bernhard Schölkopf. The analysis of distributions is fundamental in machine learning and statistics, and many algorithms in these fields rely on information theoretic approaches such as entropy, mutual information, or Kullback–Leibler divergence. However, to estimate these quantities, one must first either perform density estimation, or employ sophisticated space-partitioning/bias-correction strategies which are typically infeasible for high-dimensional data. Commonly, methods for modeling complex distributions rely on parametric assumptions that may be unfounded or computationally challenging (e.g. Gaussian mixture models), while nonparametric methods like kernel density estimation (Note: the smoothing kernels in this context have a different interpretation than the kernels discussed here) or characteristic function representation (via the Fourier transform of the distribution) break down in high-dimensional settings. Methods based on the kernel embedding of distributions sidestep these problems and also possess the following advantages: 1.  
*  Data may be modeled without restrictive assumptions about the form of the distributions and relationships between variables 2.  
*  Intermediate density estimation is not needed 3.  
*  Practitioners may specify the properties of a distribution most relevant for their problem (incorporating prior knowledge via choice of the kernel) 4.  
*  If a characteristic kernel is used, then the embedding can uniquely preserve all information about a distribution, while thanks to the kernel trick, computations on the potentially infinite-dimensional RKHS can be implemented in practice as simple Gram matrix operations  5.  
*  Dimensionality-independent rates of convergence for the empirical kernel mean (estimated using samples from the distribution) to the kernel embedding of the true underlying distribution can be proven. 6.  
*  Learning algorithms based on this framework exhibit good generalization ability and finite sample convergence, while often being simpler and more effective than information theoretic methods Thus, learning via the kernel embedding of distributions offers a principled drop-in replacement for information theoretic approaches and is a framework which not only subsumes many popular methods in machine learning and statistics as special cases, but also can lead to entirely new learning algorithms."
"http://dbpedia.org/resource/Vanishing_gradient_problem"	"Vanishing gradient problem"	"Machine learning"	"In machine learning, the vanishing gradient problem is a difficulty found in training artificial neural networks with gradient-based learning methods and backpropagation. In such methods, each of the neural network's weights receives an update proportional to the gradient of the error function with respect to the current weight in each iteration of training. Traditional activation functions such as the hyperbolic tangent function have gradients in the range (−1, 1) or [0, 1), and backpropagation computes gradients by the chain rule. This has the effect of multiplying n of these small numbers to compute gradients of the ""front"" layers in an n-layer network, meaning that the gradient (error signal) decreases exponentially with n and the front layers train very slowly. With the advent of the back-propagation algorithm in the 1970s, many researchers tried to train supervised deep artificial neural networks from scratch, initially with little success. Sepp Hochreiter's diploma thesis of 1991 formally identified the reason for this failure in the ""vanishing gradient problem"", which not only affects many-layered feedforward networks, but also recurrent neural networks. The latter are trained by unfolding them into very deep feedforward networks, where a new layer is created for each time step of an input sequence processed by the network. When activation functions are used whose derivatives can take on larger values, one risks encountering the related exploding gradient problem."
"http://dbpedia.org/resource/Evolvability_(computer_science)"	"Evolvability (computer science)"	"Machine learning"	"The term evolvability is used for a recent framework of computational learning introduced by Leslie Valiant in his paper of the same name and described below. The aim of this theory is to model biological evolution and categorize which types of mechanisms are evolvable. Evolution is an extension of PAC learning and learning from statistical queries."
"http://dbpedia.org/resource/Multiple-instance_learning"	"Multiple-instance learning"	"Machine learning"	"In machine learning, multiple-instance learning (MIL) is a variation on supervised learning. Instead of receiving a set of instances which are individually labeled, the learner receives a set of labeled bags, each containing many instances. In the simple case of multiple-instance binary classification, a bag may be labeled negative if all the instances in it are negative. On the other hand, a bag is labeled positive if there is at least one instance in it which is positive. From a collection of labeled bags, the learner tries to either (i) induce a concept that will label individual instances correctly or (ii) learn how to label bags without inducing the concept. Take image classification for example in . Given an image, we want to know its target class based on its visual content. For instance, the target class might be ""beach"", where the image contains both ""sand"" and ""water"". In MIL terms, the image is described as a bag , where each is the feature vector (called instance) extracted from the corresponding i-th region in the image and N is the total regions (instances) partitioning the image. The bag is labeled positive (""beach"") if it contains both ""sand"" region instances and ""water"" region instances. Multiple-instance learning was originally proposed under this name by , but earlier examples of similar research exist, for instance in the work on handwritten digit recognition by . Recent reviews of the MIL literature include , which provides an extensive review and comparative study of the different paradigms, and , which provides a thorough review of the different assumptions used by different paradigms in the literature. Examples of where MIL is applied are: 
*  Molecule activity 
*  Predicting binding sites of Calmodulin binding proteins  
*  Predicting function for alternatively spliced isoforms , 
*  Image classification  
*  Text or document categorization  
*  Predicting functional binding sites of MicroRNA targets Numerous researchers have worked on adapting classical classification techniques, such as support vector machines or boosting, to work within the context of multiple-instance learning."
"http://dbpedia.org/resource/Bayesian_interpretation_of_kernel_regularization"	"Bayesian interpretation of kernel regularization"	"Machine learning"	"In machine learning, kernel methods arise from the assumption of an inner product space or similarity structure on inputs. For some such methods, such as support vector machines (SVMs), the original formulation and its regularization were not Bayesian in nature. It is helpful to understand them from a Bayesian perspective. Because the kernels are not necessarily positive semidefinite, the underlying structure may not be inner product spaces, but instead more general reproducing kernel Hilbert spaces. In Bayesian probability kernel methods are a key component of Gaussian processes, where the kernel function is known as the covariance function. Kernel methods have traditionally been used in supervised learning problems where the input space is usually a space of vectors while the output space is a space of scalars. More recently these methods have been extended to problems that deal with multiple outputs such as in multi-task learning. In this article we analyze the connections between the regularization and the Bayesian point of view for kernel methods in the case of scalar outputs. A mathematical equivalence between the regularization and the Bayesian point of view is easily proved in cases where the reproducing kernel Hilbert space is finite-dimensional. The infinite-dimensional case raises subtle mathematical issues; we will consider here the finite-dimensional case. We start with a brief review of the main ideas underlying kernel methods for scalar learning, and briefly introduce the concepts of regularization and Gaussian processes. We then show how both points of view arrive at essentially equivalent estimators, and show the connection that ties them together."
"http://dbpedia.org/resource/Ball_tree"	"Ball tree"	"Machine learning"	"In computer science, a ball tree, balltree or metric tree, is a space partitioning data structure for organizing points in a multi-dimensional space. The ball tree gets its name from the fact that it partitions data points into a nested set of hyperspheres known as ""balls"". The resulting data structure has characteristics that make it useful for a number of applications, most notably nearest neighbor search."
"http://dbpedia.org/resource/The_Master_Algorithm"	"The Master Algorithm"	"Machine learning"	"The Master Algorithm: How the Quest for the Ultimate Learning Machine Will Remake Our World is a book by Pedro Domingos released in 2015. Domingos wrote it in order to generate interest from people outside the field. Towards the end of the book, while reviewing his invention of the Markov logic network  he pictures a ""master algorithm"" allowing technology to allow machine learning algorithms to asymptotically grow to a perfect understanding of how the world and people in it work."
"http://dbpedia.org/resource/Trax_Image_Recognition"	"Trax Image Recognition"	"Machine learning"	"Trax Image Recognition is a technology company based in Singapore. Its computer vision technology is used by FMCG companies such as Coca-Cola for monitoring and analysing retail shelves across large numbers of stores. The service reduces the time an employee needs to spend on audits to check inventory, shelf display and product promotions. It is also gathers more extensive data such as product assortment, shelf space, pricing, promotions, shelf location and arrangement of products on display. This market intelligence is valuable to FMCG manufacturers because they pay large sums for space in supermarkets and stores. For example, in the US companies pay approximately $18 billion for shelf space. The computer vision technology is able to recognise products that are similar or identical such as branded drinks or shampoo bottles whilst also being able to differentiate between them based on variety and size. It piloted its machine learning algorithms with initial customers, allowing its algorithm to learn about different products. As the company processes more images, the better it gets at recognising the same products in different shapes and sizes. Founded in 2010, Trax has over 40 customers in the FMCG industry, including beverage giant Coca-Cola and brewer Anheuser-Busch InBev. Its service is available in 32 markets and the company's development centre is located in Tel Aviv. In December 2014 it announced its fourth round of investment of US$15 million."
"http://dbpedia.org/resource/Multilinear_subspace_learning"	"Multilinear subspace learning"	"Machine learning"	"Multilinear subspace learning is an approach to dimensionality reduction.  Dimensionality reduction can be performed on data tensor whose observations have been vectorized and organized into a data tensor, or whose observations are matrices that are concatenated into a data tensor. Here are some examples of data tensors whose observations are vectorized or whose observations are matrices concatenated into data tensor images (2D/3D), video sequences (3D/4D), and hyperspectral cubes (3D/4D). The mapping from a high-dimensional vector space to a set of lower dimensional vector spaces is a multilinear projection. Multilinear subspace learning#Algorithms are higher-order generalizations of linear subspace learning methods such as principal component analysis (PCA), independent component analysis (ICA), linear discriminant analysis (LDA) and canonical correlation analysis (CCA)."
"http://dbpedia.org/resource/Feature_scaling"	"Feature scaling"	"Machine learning"	"Feature scaling is a method used to standardize the range of independent variables or features of data. In data processing, it is also known as data normalization and is generally performed during the data preprocessing step."
"http://dbpedia.org/resource/Mountain_Car"	"Mountain Car"	"Machine learning"	"Mountain Car, a standard testing domain in reinforcement learning, is a problem in which an under-powered car must drive up a steep hill. Since gravity is stronger than the car's engine, even at full throttle, the car cannot simply accelerate up the steep slope. The car is situated in a valley and must learn to leverage potential energy by driving up the opposite hill before the car is able to make it to the goal at the top of the rightmost hill. The domain has been used as a test bed in various reinforcement learning papers."
"http://dbpedia.org/resource/Learning_with_errors"	"Learning with errors"	"Machine learning"	"Learning with errors (LWE) is a problem in machine learning that is conjectured to be hard to solve. Introduced by Oded Regev in 2005, it is a generalization of the parity learning problem. Regev showed, furthermore, that the LWE problem is as hard to solve as several worst-case lattice problems. The LWE problem has recently been used as a hardness assumption to create public-key cryptosystems, such as the ring learning with errors key exchange by Peikert. An algorithm is said to solve the LWE problem if, when given access to samples where and , with the assurance, for some fixed linear function that with high probability and deviates from it according to some known noise model, the algorithm can recreate or some close approximation of it with high probability."
"http://dbpedia.org/resource/Random_indexing"	"Random indexing"	"Machine learning"	"Random indexing is a dimension reduction method and computational framework for Distributional semantics, based on the insight that very-high-dimensional Vector Space Model implementations are impractical, that models need not grow in dimensionality when new items (e.g. new terminology) is encountered, and that a high-dimensional model can be projected into a space of lower dimensionality without compromising L2 distance metrics if the resulting dimensions are chosen appropriately, which is the original point of the random projection approach to dimension reduction first formulated as the Johnson–Lindenstrauss lemma. Locality-sensitive hashing has some of the same starting points. Random indexing, as used in representation of language, originates from the work of Pentti Kanerva on Sparse distributed memory, and can be described as an incremental formulation of a random projection. It can be also verified that random indexing is a random projection technique for the construction of Euclidean spaces---i.e. L2 normed vector spaces. In Euclidean spaces, random projections are elucidated using the Johnson–Lindenstrauss lemma. TopSig  extends the Random Indexing model to produce bit vectors for comparison with the Hamming distance similarity function. It is used for improving the performance of information retrieval and document clustering. In a similar line of research, Random Manhattan Integer Indexing is proposed for improving the performance of the methods that employ the Manhattan distance between text units. Many random indexing methods primarily generate similarity from co-occurrence of items in a corpus. Reflexive Random Indexing  generates similarity from co-occurrence and from shared occurrence with other items."
"http://dbpedia.org/resource/Linear_predictor_function"	"Linear predictor function"	"Machine learning"	"In statistics and in machine learning, a linear predictor function is a linear function (linear combination) of a set of coefficients and explanatory variables (independent variables), whose value is used to predict the outcome of a dependent variable. This sort of function usually comes in linear regression, where the coefficients are called regression coefficients. However, they also occur in various types of linear classifiers (e.g. logistic regression, perceptrons, support vector machines, and linear discriminant analysis), as well as in various other models, such as principal component analysis and factor analysis. In many of these models, the coefficients are referred to as ""weights""."
"http://dbpedia.org/resource/Manifold_regularization"	"Manifold regularization"	"Machine learning"	"In machine learning, Manifold regularization is a technique for using the shape of a dataset to constrain the functions that should be learned on that dataset. In many machine learning problems, the data to be learned do not cover the entire input space. For example, a facial recognition system may not need to classify any possible image, but only the subset of images that contain faces. The technique of manifold learning assumes that the relevant subset of data comes from a manifold, a mathematical structure with useful properties. The technique also assumes that the function to be learned is smooth: data with different labels are not likely to be close together, and so the labeling function should not change quickly in areas where there are likely to be many data points. Because of this assumption, a manifold regularization algorithm can use unlabeled data to inform where the learned function is allowed to change quickly and where it is not, using an extension of the technique of Tikhonov regularization. Manifold regularization algorithms can extend supervised learning algorithms in semi-supervised learning and transductive learning settings, where unlabeled data are available. The technique has been used for applications including medical imaging, geographical imaging, and object recognition."
"http://dbpedia.org/resource/Base_rate"	"Base rate"	"Machine learning"	"In probability and statistics, base rate generally refers to the (base) class probabilities unconditioned on featural evidence, frequently also known as prior probabilities. For example, if it were the case that 1% of the public were ""medical professionals"", and 99% of the public were not ""medical professionals"", then the base rate of medical professionals is simply 1%. In the sciences, including medicine, the base rate is critical for comparison. It may at first seem impressive that 1000 people beat their winter cold while using 'Treatment X', until we look at the entire 'Treatment X' population and find that the base rate of success is actually only 1/100 (i.e. 100,000 people tried the treatment, but the other 99,000 people never really beat their winter cold). The treatment's effectiveness is clearer when such base rate information (i.e. ""1000 people... out of how many?"") is available. Note that controls may likewise offer further information for comparison; maybe the control groups, who were using no treatment at all, had their own base rate success of 5/100. Controls thus indicate that 'Treatment X' actually makes things worse, despite that initial proud claim about 1000 people. The normative method for integrating base rates (prior probabilities) and featural evidence (likelihoods) is given by Bayes' rule."
"http://dbpedia.org/resource/Committee_machine"	"Committee machine"	"Machine learning"	"A committee machine is a type of artificial neural network using a divide and conquer strategy in which the responses of multiple neural networks (experts) are combined into a single response. The combined response of the committee machine is supposed to be superior to those of its constituent experts. Compare with ensembles of classifiers."
"http://dbpedia.org/resource/Probability_matching"	"Probability matching"	"Machine learning"	"Probability matching is a decision strategy in which predictions of class membership are proportional to the class base rates. Thus, if in the training set positive examples are observed 60% of the time, and negative examples are observed 40% of the time, then the observer using a probability-matching strategy will predict (for unlabeled examples) a class label of ""positive"" on 60% of instances, and a class label of ""negative"" on 40% of instances. The optimal Bayesian decision strategy (to maximize the number of correct predictions, see ) in such a case is to always predict ""positive"" (i.e., predict the majority category in the absence of other information), which has 60% chance of winning rather than matching which has 52% of winning (where p is the probability of positive realization, the result of matching would be , here ). The probability-matching strategy is of psychological interest because it is frequently employed by human subjects in decision and classification studies (where it may be related to Thompson sampling)."
"http://dbpedia.org/resource/Data_pre-processing"	"Data pre-processing"	"Machine learning"	"Data pre-processing is an important step in the data mining process. The phrase ""garbage in, garbage out"" is particularly applicable to data mining and machine learning projects. Data-gathering methods are often loosely controlled, resulting in out-of-range values (e.g., Income: −100), impossible data combinations (e.g., Sex: Male, Pregnant: Yes), missing values, etc. Analyzing data that has not been carefully screened for such problems can produce misleading results. Thus, the representation and quality of data is first and foremost before running an analysis. If there is much irrelevant and redundant information present or noisy and unreliable data, then knowledge discovery during the training phase is more difficult. Data preparation and filtering steps can take considerable amount of processing time. Data pre-processing includes cleaning, normalization, transformation, feature extraction and selection, etc. The product of data pre-processing is the final training set. Kotsiantis et al. (2006) present a well-known algorithm for each step of data pre-processing."
"http://dbpedia.org/resource/Ensembles_of_classifiers"	"Ensembles of classifiers"	"Machine learning"	"Recently in the area of machine learning the concept of combining classifiers is proposed as a new direction for the improvement of the performance of individual classifiers. These classifiers could be based on a variety of classification methodologies, and could achieve different rate of correctly classified individuals. The goal of classification result integration algorithms is to generate more certain, precise and accurate system results. Dietterich (2001) provides an accessible and informal reasoning, from statistical, computational and representational viewpoints, of why ensembles can improve results."
"http://dbpedia.org/resource/Uncertain_data"	"Uncertain data"	"Machine learning"	"In computer science, uncertain data is data that contains noise that makes it deviate from the correct, intended or original values. In the age of big data, uncertainty or data veracity is one of the defining characteristics of data. Data is constantly growing in volume, variety, velocity and uncertainty (1/veracity). Uncertain data is found in abundance today on the web, in sensor networks, within enterprises both in their structured and unstructured sources. For example, there may be uncertainty regarding the address of a customer in an enterprise dataset, or the temperature readings captured by a sensor due to aging of the sensor. In 2012 IBM called out managing uncertain data at scale in its global technology outlook report that presents a comprehensive analysis looking three to ten years into the future seeking to identify significant, disruptive technologies that will change the world. In order to make confident business decisions based on real-world data, analyses must necessarily account for many different kinds of uncertainty present in very large amounts of data. Analyses based on uncertain data will have an effect on the quality of subsequent decisions, so the degree and types of inaccuracies in this uncertain data cannot be ignored. Uncertain data is found in the area of sensor networks; text where noisy text is found in abundance on social media, web and within enterprises where the structured and unstructured data may be old, outdated, or plain incorrect; in modeling where the mathematical model may only be an approximation of the actual process. When representing such data in a database, some indication of the probability of the correctness of the various values also needs to be estimated. There are three main models of uncertain data in databases. In attribute uncertainty, each uncertain attribute in a tuple is subject to its own independent probability distribution. For example, if readings are taken of temperature and wind speed, each would be described by its own probability distribution, as knowing the reading for one measurement would not provide any information about the other. In correlated uncertainty, multiple attributes may be described by a joint probability distribution. For example, if readings are taken of the position of an object, and the x- and y-coordinates stored, the probability of different values may depend on the distance from the recorded coordinates. As distance depends on both coordinates, it may be appropriate to use a joint distribution for these coordinates, as they are not independent. In tuple uncertainty, all the attributes of a tuple are subject to a joint probability distribution. This covers the case of correlated uncertainty, but also includes the case where there is a probability of a tuple not belonging in the relevant relation, which is indicated by all the probabilities not summing to one. For example, assume we have the following tuple from a probabilistic database: Then, the tuple has 10% chance of not existing in the database."
"http://dbpedia.org/resource/Apprenticeship_learning"	"Apprenticeship learning"	"Machine learning"	"Apprenticeship learning, or apprenticeship via inverse reinforcement learning (AIRP), is a concept in the field of artificial intelligence and machine learning, developed by Pieter Abbeel, Associate Professor in Berkeley's EECS department, and Andrew Ng, Associate Professor in Stanford University's Computer Science Department. It was incepted in 2004. AIRP deals with ""Markov decision process where we are not explicitly given a reward function, but where instead we can observe an expert demonstrating the task that we want to learn to perform"" AIRP concept is closely related to reinforcement learning (RL) that is a sub-area of machine learning concerned with how an agent ought to take actions in an environment so as to maximize some notion of long-term reward. AIRP algorithms are used when the reward function is unknown. The algorithms use observations of the behavior of an expert to teach the agent the optimal actions in certain states of the environment. AIRP is a special case of the general area of learning from demonstration (LfD), where the goal is to learn a complex task by observing a set of expert traces (demonstrations). AIRP is the intersection of LfD and RL."
"http://dbpedia.org/resource/Algorithmic_inference"	"Algorithmic inference"	"Machine learning"	"Algorithmic inference gathers new developments in the statistical inference methods made feasible by the powerful computing devices widely available to any data analyst. Cornerstones in this field are computational learning theory, granular computing, bioinformatics, and, long ago, structural probability ().The main focus is on the algorithms which compute statistics rooting the study of a random phenomenon, along with the amount of data they must feed on to produce reliable results. This shifts the interest of mathematicians from the study of the distribution laws to the functional properties of the statistics, and the interest of computer scientists from the algorithms for processing data to the information they process."
"http://dbpedia.org/resource/Uniform_convergence_(combinatorics)"	"Uniform convergence (combinatorics)"	"Machine learning"	"For a class of predicates defined on a set and a set of samples , where , the empirical frequency of on is . The Uniform Convergence Theorem states, roughly,that if is ""simple"" and we draw samples independently (with replacement) from according to a distribution , then with high probability all the empirical frequency will be close to its expectation, where the expectation is given by . Here ""simple"" means that the Vapnik-Chernovenkis dimension of the class is small relative to the size of the sample.In other words, a sufficiently simple collection of functions behaves roughly the same on a small random sample as it does on the distribution as a whole."
"http://dbpedia.org/resource/Leave-one-out_error"	"Leave-one-out error"	"Machine learning"	"
*  Expected-to-leave-one-out error () Stability An algorithm f has stability if for each n there exists a and a such that: , with and going to zero for"
"http://dbpedia.org/resource/Similarity_learning"	"Similarity learning"	"Machine learning"	"Similarity learning is an area of supervised machine learning in artificial intelligence. It is closely related to regression and classification, but the goal is to learn from examples a similarity function that measures how similar or related two objects are. It has applications in ranking, in recommendation systems, visual identity tracking, face verification and speaker verification."
"http://dbpedia.org/resource/Representer_theorem"	"Representer theorem"	"Machine learning"	"In statistical learning theory, a representer theorem is any of several related results stating that a minimizer of a regularized empirical risk function defined over a reproducing kernel Hilbert space can be represented as a finite linear combination of kernel products evaluated on the input points in the training set data."
"http://dbpedia.org/resource/Large_margin_nearest_neighbor"	"Large margin nearest neighbor"	"Machine learning"	"Large margin nearest neighbor (LMNN) classification is a statistical machine learning algorithm for metric learning. It learns a pseudometric designed for k-nearest neighbor classification. The algorithm is based on semidefinite programming, a sub-class of convex optimization. The goal of supervised learning (more specifically classification) is to learn a decision rule that can categorize data instances into pre-defined classes. The k-nearest neighbor rule assumes a training data set of labeled instances (i.e. the classes are known). It classifies a new data instance with the class obtained from the majority vote of the k closest (labeled) training instances. Closeness is measured with a pre-defined metric. Large margin nearest neighbors is an algorithm that learns this global (pseudo-)metric in a supervised fashion to improve the classification accuracy of the k-nearest neighbor rule."
"http://dbpedia.org/resource/Bing_Predicts"	"Bing Predicts"	"Machine learning"	"Bing Predicts is a prediction engine developed by Microsoft that uses machine learning from data on trending social media topics (and sentiment towards those topics), along with trending searches on Bing. It predicts the outcomes of political elections, popular reality shows, and major sporting events. Predictions can be accessed through the Bing search engine."
"http://dbpedia.org/resource/Inductive_probability"	"Inductive probability"	"Machine learning"	"Inductive probability attempts to give the probability of future events based on past events. It is the basis for inductive reasoning, and gives the mathematical basis for learning and the perception of patterns. It is a source of knowledge about the world. There are three sources of knowledge: inference, communication, and deduction. Communication relays information found using other methods. Deduction establishes new facts based on existing facts. Only inference establishes new facts from data. The basis of inference is Bayes' theorem. But this theorem is sometimes hard to apply and understand. The simpler method to understand inference is in terms of quantities of information. Information describing the world is written in a language. For example, a simple mathematical language of propositions may be chosen. Sentences may be written down in this language as strings of characters. But in the computer it is possible to encode these sentences as strings of bits (1s and 0s). Then the language may be encoded so that the most commonly used sentences are the shortest. This internal language implicitly represents probabilities of statements. Occam's razor says the ""simplest theory, consistent with the data is most likely to be correct"". The ""simplest theory"" is interpreted as the representation of the theory written in this internal language. The theory with the shortest encoding in this internal language is most likely to be correct."
"http://dbpedia.org/resource/Adversarial_machine_learning"	"Adversarial machine learning"	"Machine learning"	"Adversarial machine learning is a research field that lies at the intersection of machine learning and computer security. It aims to enable the safe adoption of machine learning techniques in adversarial settings like spam filtering, malware detection and biometric recognition. The problem arises from the fact that machine learning techniques were originally designed for stationary environments in which the training and test data are assumed to be generated from the same (although possibly unknown) distribution. In the presence of intelligent and adaptive adversaries, however, this working hypothesis is likely to be violated to at least some degree (depending on the adversary). In fact, a malicious adversary can carefully manipulate the input data exploiting specific vulnerabilities of learning algorithms to compromise the whole system security. Examples include: attacks in spam filtering, where spam messages are obfuscated through misspelling of bad words or insertion of good words; attacks in computer security, e.g., to obfuscate malware code within network packets  or mislead signature detection; attacks in biometric recognition, where fake biometric traits may be exploited to impersonate a legitimate user (biometric spoofing)  or to compromise users’ template galleries that are adaptively updated over time."
"http://dbpedia.org/resource/Elastic_matching"	"Elastic matching"	"Machine learning"	"Elastic matching is one of the pattern recognition techniques in computer science. Elastic matching (EM) is also known as deformable template, flexible matching, or nonlinear template matching. Elastic matching can be defined as an optimization problem of two-dimensional warping specifying corresponding pixels between subjected images."
"http://dbpedia.org/resource/Catastrophic_interference"	"Catastrophic interference"	"Machine learning"	"Catastrophic interference, also known as catastrophic forgetting, is the tendency of an artificial neural network to completely and abruptly forget previously learned information upon learning new information. Neural networks are an important part of the network approach and connectionist approach to cognitive science. These networks use computer simulations to try and model human behaviours, such as memory and learning. Catastrophic interference is an important issue to consider when creating connectionist models of memory. It was originally brought to the attention of the scientific community by research from McCloskey and Cohen (1989), and Ractcliff (1990). It is a radical manifestation of the ‘sensitivity-stability’ dilemma  or the ‘stability-plasticity’ dilemma. Specifically, these problems refer to the issue of being able to make an artificial neural network that is sensitive to, but not disrupted by, new information. Lookup tables and connectionist networks lie on the opposite sides of the stability plasticity spectrum. The former remains completely stable in the presence of new information but lacks the ability to generalize, i.e. infer general principles, from new inputs. On the other hand, connectionist networks like the standard backpropagation network are very sensitive to new information and can generalize on new inputs. Backpropagation models can be considered good models of human memory insofar as they mirror the human ability to generalize but these networks often exhibit less stability than human memory. Notably, these backpropagation networks are susceptible to catastrophic interference. This is considered an issue when attempting to model human memory because, unlike these networks, humans typically do not show catastrophic forgetting. Thus, the issue of catastrophic interference must be eradicated from these backpropagation models in order to enhance the plausibility as models of human memory."
"http://dbpedia.org/resource/Coupled_pattern_learner"	"Coupled pattern learner"	"Machine learning"	"Coupled Pattern Learner (CPL) is a machine learning algorithm which couples the semi-supervised learning of categories and relations to forestall the problem of semantic drift associated with boot-strap learning methods."
"http://dbpedia.org/resource/Timeline_of_machine_learning"	"Timeline of machine learning"	"Machine learning"	"This page is a timeline of machine learning. Major discoveries, achievements, milestones and other major events are included."
"http://dbpedia.org/resource/Parity_learning"	"Parity learning"	"Machine learning"	"Parity learning is a problem in machine learning. An algorithm that solves this problem must guess the function ƒ, given some samples (x, ƒ(x)) and the assurance that ƒ computes the parity of bits at some fixed locations. The samples are generated using some distribution over the input. The problem is easy to solve using Gaussian elimination provided that a sufficient number of samples (from a distribution which is not too skewed) are provided to the algorithm."
"http://dbpedia.org/resource/Universal_portfolio_algorithm"	"Universal portfolio algorithm"	"Machine learning"	"The universal portfolio algorithm is a portfolio selection algorithm from the field of machine learning and information theory. The algorithm learns adaptively from historical data and maximizes the log-optimal growth rate in the long run. It was introduced by the late Stanford University information theorist Thomas M. Cover. The algorithm rebalances the portfolio at the beginning of each trading period. At the beginning of the first trading period it starts with a naive diversification. In the following trading periods the portfolio composition depends on the historical total return of all possible constant-rebalanced portfolios."
"http://dbpedia.org/resource/Sequence_labeling"	"Sequence labeling"	"Machine learning"	"In machine learning, sequence labeling is a type of pattern recognition task that involves the algorithmic assignment of a categorical label to each member of a sequence of observed values. A common example of a sequence labeling task is part of speech tagging, which seeks to assign a part of speech to each word in an input sentence or document. Sequence labeling can be treated as a set of independent classification tasks, one per member of the sequence. However, accuracy is generally improved by making the optimal label for a given element dependent on the choices of nearby elements, using special algorithms to choose the globally best set of labels for the entire sequence at once. As an example of why finding the globally best label sequence might produce better results than labeling one item at a time, consider the part-of-speech tagging task just described. Frequently, many words are members of multiple parts of speech, and the correct label of such a word can often be deduced from the correct label of the word to the immediate left or right. For example, the word ""sets"" can be either a noun or verb. In a phrase like ""he sets the books down"", the word ""he"" is unambiguously a pronoun, and ""the"" unambiguously a determiner, and using either of these labels, ""sets"" can be deduced to be a verb, since nouns very rarely follow pronouns and are less likely to precede determiners than verbs are. But in other cases, only one of the adjacent words is similarly helpful. In ""he sets and then knocks over the table"", only the word ""he"" to the left is helpful (cf. ""...picks up the sets and then knocks over...""). Conversely, in ""... and also sets the table"" only the word ""the"" to the right is helpful (cf. ""... and also sets of books were ...""). An algorithm that proceeds from left to right, labeling one word at a time, can only use the tags of left-adjacent words and might fail in the second example above; vice versa for an algorithm that proceeds from right to left. Most sequence labeling algorithms are probabilistic in nature, relying on statistical inference to find the best sequence. The most common statistical models in use for sequence labeling make a Markov assumption, i.e. that the choice of label for a particular word is directly dependent only on the immediately adjacent labels; hence the set of labels forms a Markov chain. This leads naturally to the hidden Markov model (HMM), one of the most common statistical models used for sequence labeling. Other common models in use are the maximum entropy Markov model and conditional random field."
"http://dbpedia.org/resource/Feature_engineering"	"Feature engineering"	"Machine learning"	"Feature engineering is the process of using domain knowledge of the data to create features that make machine learning algorithms work. Feature engineering is fundamental to the application of machine learning, and is both difficult and expensive. The need for manual feature engineering can be obviated by automated feature learning. Feature engineering is an informal topic, but it is considered essential in applied machine learning. Coming up with features is difficult, time-consuming, requires expert knowledge. ""Applied machine learning"" is basically feature engineering.— Andrew Ng, Machine Learning and AI via Brain simulationsWhen working on a machine learning problem, feature engineering is manually designing what the input x's should be.— Shayne Miel, ""What is the intuitive explanation of feature engineering in machine learning?"""
"http://dbpedia.org/resource/M-Theory_(learning_framework)"	"M-Theory (learning framework)"	"Machine learning"	"In Machine Learning and Computer Vision, M-Theory is a learning framework inspired by feed-forward processing in the ventral stream of visual cortex and originally developed for recognition and classification of objects in visual scenes. M-Theory was later applied to other areas, such as speech recognition. On certain image recognition tasks, algorithms based on a specific instantiation of M-Theory, HMAX, achieved human-level performance. The core principle of M-Theory is extracting representations invariant to various transformations of images (translation, scale, 2D and 3D rotation and others). In contrast with other approaches using invariant representations, in M-Theory they are not hardcoded into the algorithms, but learned. M-Theory also shares some principles with Compressed Sensing. The theory proposes multilayered hierarchical learning architecture, similar to that of visual cortex."
"http://dbpedia.org/resource/Kernel_random_forest"	"Kernel random forest"	"Machine learning"	"In machine learning, kernel random forests establish the connection between random forests and kernel methods. By slightly modifying their definition, random forests can be rewritten as kernel methods, which are more interpretable and easier to analyze."
"http://dbpedia.org/resource/Zeroth_(software)"	"Zeroth (software)"	"Machine learning"	"Zeroth is a platform for brain-inspired computing from Qualcomm. It is based around a neural processing unit (NPU) AI accelerator chip and a software API to interact with the platform. It makes a form of machine learning known as deep learning available to mobile devices. It is used for image and sound processing, including speech recognition. The software operates locally rather than as a cloud application. Mobile chip maker Qualcomm announced in March 2015 that it would bundle the software with its next major mobile device chip, the Snapdragon 820 processor."
"http://dbpedia.org/resource/Preference_learning"	"Preference learning"	"Machine learning"	"Preference learning is a subfield in machine learning in which the goal is to learn a predictive preference model from observed preference information. In the view of supervised learning, preference learning trains on a set of items which have preferences toward labels or other items and predicts the preferences for all items. While the concept of preference learning has been emerged for some time in many fields such as economics, it's a relatively new topic in Artificial Intelligence research. Several workshops have been discussing preference learning and related topics in the past decade."
"http://dbpedia.org/resource/Proximal_gradient_methods_for_learning"	"Proximal gradient methods for learning"	"Machine learning"	"Proximal gradient (forward backward splitting) methods for learning is an area of research in optimization and statistical learning theory which studies algorithms for a general class of convex regularization problems where the regularization penalty may not be differentiable. One such example is regularization (also known as Lasso) of the form Proximal gradient methods offer a general framework for solving regularization problems from statistical learning theory with penalties that are tailored to a specific problem application. Such customized penalties can help to induce certain structure in problem solutions, such as sparsity (in the case of lasso) or group structure (in the case of group lasso)."
"http://dbpedia.org/resource/Inductive_functional_programming"	"Inductive functional programming"	"Machine learning"	"Inductive Functional Programming (IFP) is a special kind of inductive programming that uses functional programs as representation for examples, programs and background knowledge. The term is frequently used to make a distinction from inductive logic programming, which uses logic programs."
"http://dbpedia.org/resource/Bayesian_optimization"	"Bayesian optimization"	"Machine learning"	"Bayesian optimization is a sequential design strategyfor global optimization of black-box functions that doesn't require derivatives."
"http://dbpedia.org/resource/Sample_complexity"	"Sample complexity"	"Machine learning"	"The sample complexity of a machine learning algorithm represents the number of training-samples that it needs in order to successfully learn a target function. More precisely, the sample complexity is the number of training-samples that we need to supply to the algorithm, so that the function returned by the algorithm is within an arbitrarily small error of the best possible function, with probability arbitrarily close to 1. There are two variants of sample complexity: 
*  The weak variant fixes a particular input-output distribution; 
*  The strong variant takes the worst-case sample complexity over all input-output distributions. The No Free Lunch theorem, discussed below, proves that, in general, the strong sample complexity is infinite. I.e, there is no algorithm that can learn the globally-optimal target function using a finite number of training samples. However, if we are only interested in a particular class of target functions (e.g, only linear functions) then the sample complexity is finite, and it depends linearly on the VC dimension on the class of target functions."
"http://dbpedia.org/resource/Semantic_folding"	"Semantic folding"	"Machine learning"	"Semantic folding theory describes a procedure for encoding the semantics of natural language text in a semantically grounded binary representation. This approach provides a framework for modelling how language data is processed by the neocortex."
"http://dbpedia.org/resource/Spike-and-slab_variable_selection"	"Spike-and-slab variable selection"	"Machine learning"	"Spike-and-slab regression is a Bayesian variable selection technique that is particularly useful when the number of possible predictors is larger than the number of observations. Initially, the idea of the spike-and-slab model was proposed by Mitchell & Beauchamp (1988). The approach was further significantly developed by Madigan & Raftery (1994) and George & McCulloch (1997). The final adjustments to the model were done by Ishwaran & Rao (2005)."
"http://dbpedia.org/resource/Deep_feature_synthesis"	"Deep feature synthesis"	"Machine learning"	"Deep Feature Synthesis is an algorithm developed by James Max Kanter and Kalyan Veeramachaneni in their paper ""Deep Feature Synthesis: Towards Automating Data Science Endeavors"""
"http://dbpedia.org/resource/Machine_Teaching"	"Machine Teaching"	"Machine learning"	"Machine teaching is the inverse problem to machine learning. The objective is to build the input training data when a learning algorithm and desired outputs are given. In essence, this is similar to how human teachers construct lesson plan and exercises to obtain desired learning in their students. The primary focus of the machine teaching is to build training data set that is optimal in some sense."
"http://dbpedia.org/resource/Bradley–Terry_model"	"Bradley–Terry model"	"Machine learning"	"The Bradley–Terry model is a probability model that can predict the outcome of a comparison. Given a pair of individuals i and j drawn from some population, it estimates the probability that the pairwise comparison i > j turns out true, as where pi is a positive real-valued score assigned to individual i. The comparison i > j can be read as ""i is preferred to j"", ""i ranks higher than j"", or ""i beats j"", depending on the application. For example, pi may represent the skill of a team in a sports tournament, estimated from the number of times i has won a match. then represents the probability that i will win a match against j. Another example used to explain the model's purpose is that of scoring products in a certain category by quality. While it's hard for a person to draft a direct ranking of (many) brands of wine, it may be feasible to compare a sample of pairs of wines and say, for each pair, which one is better. The Bradley–Terry model can then be used to derive a full ranking."
"http://dbpedia.org/resource/Knowledge_Vault"	"Knowledge Vault"	"Machine learning"	"The Knowledge Vault is a knowledge base created by Google. As of 2014, it contained 1.6 billion facts which had been collated automatically from the Internet. The Knowledge Graph pulled in information from structured sources like Freebase, Wikidata and Wikipedia, while the Knowledge Vault is an accumulation of facts from across the entire web, including unstructured sources. ""Facts"" in Knowledge Vault also include a confidence value, giving the capability of distinguishing between knowledge statements that have a high probability of being true from others that may be less likely to be true (based on the source that Google obtained the data from and other factors). The concept behind the Knowledge Vault was presented in a paper authored by a Google Research team."
"http://dbpedia.org/resource/Bayesian_structural_time_series"	"Bayesian structural time series"	"Machine learning"	"Bayesian Structural Time Series (BSTS) model is a machine learning technique used for feature selection, time series forecasting, nowcasting, inferring causal impact and other. The model is designed to work with time series data. The model has also promising application in the field of analytical marketing. In particular, it can be used in order to asses how much different marketing campaigns have contributed to the change in web search volumes, product sales, brand popularity and other relevant indicators (difference-in-differences model is a usual alternative approach in this case). ""In contrast to classical difference-in-differences schemes, state-space models make it possible to (i) infer the temporal evolution of attributable impact, (ii) incorporate empirical priors on the parameters in a fully Bayesian treatment, and (iii) flexibly accommodate multiple sources of variation, including the time-varying influence of contemporaneous covariates, i.e., synthetic controls."""
"http://dbpedia.org/resource/Stochastic_block_model"	"Stochastic block model"	"Machine learning"	"The stochastic block model is a generative model for random graphs. This model tends to produce graphs containing communities, subsets characterized by being connected with one another with particular edge densities. For example, edges may be more common within communities than between communities. The stochastic block model is important in statistics, machine learning, and network science, where it serves as a useful benchmark for the task of recovering community structure in graph data."
"http://dbpedia.org/resource/Action_model_learning"	"Action model learning"	"Machine learning"	"Action model learning (sometimes abbreviated action learning) is an area of machine learning concerned with creation and modification of software agent's knowledge about effects and preconditions of the actions that can be executed within its environment. This knowledge is usually represented in logic-based action description language and used as the input for automated planners. Learning action models is important when goals change. When an agent acted for a while, it can use its accumulated knowledge about actions in the domain to make better decisions. Thus, learning action models differs from reinforcement learning. It enables reasoning about actions instead of expensive trials in the world. Action model learning is a form of inductive reasoning, where new knowledge is generated based on agent's observations. It differs from standard supervised learning in that correct input/output pairs are never presented, nor imprecise action models explicitly corrected. Usual motivation for action model learning is the fact that manual specification of action models for planners is often a difficult, time consuming, and error-prone task (especially in complex environments)."
"http://dbpedia.org/resource/Local_case-control_sampling"	"Local case-control sampling"	"Machine learning"	"In machine learning, local case-control sampling  is an algorithm used to reduce the complexity of training a logistic regression classifier. The algorithm reduces the training complexity by selecting a small subsample of the original dataset for training. It assumes the availability of a (unreliable) pilot estimation of the parameters. It then performs a single pass over the entire dataset using the pilot estimation to identify the most ""surprising"" samples. In practice, the pilot may come from prior knowledge or training using a subsample of the dataset. The algorithm is most effective when the underlying dataset is imbalanced. It exploits the structures of conditional imbalanced datasets more efficiently than alternative methods, such as case control sampling and weighted case control sampling."
"http://dbpedia.org/resource/Glossary_of_artificial_intelligence"	"Glossary of artificial intelligence"	"Machine learning"	"This glossary of artificial intelligence terms is about artificial intelligence, its sub-disciplines, and related fields."
"http://dbpedia.org/resource/Algorithm_Selection"	"Algorithm Selection"	"Machine learning"	"Algorithm Selection (sometimes also called per-instance algorithm selection or offline algorithm selection) is a meta-algorithmic technique to choose an algorithm from a portfolio on an instance-by-instance basis. It is motivated by the observation that on many practical problems, algorithms have different performances. That is, while one algorithm performs well on some instances, it performs poorly on others and vice versa for another algorithm. If we can identify when to use which algorithm, we can get the best of both worlds and improve overall performance. This is what algorithm selection aims to do. The only prerequisite for applying algorithm selection techniques is that there exists (or that there can be constructed) a set of complementary algorithms."
"http://dbpedia.org/resource/List_of_datasets_for_machine_learning_research"	"List of datasets for machine learning research"	"Machine learning"	"These datasets are used for machine learning research and have been cited in peer-reviewed academic journals and other publications. Datasets are an integral part of the field of machine learning. Major advances in this field can result from advances in learning algorithms (such as deep learning), computer hardware, and, less-intuitively, the availability of high-quality training datasets. High-quality labeled training datasets for supervised and semi-supervised machine learning algorithms are usually difficult and expensive to produce because of the large amount of time needed to label the data. Although they do not need to be labeled, high-quality datasets for unsupervised learning can also be difficult and costly to produce. This list aggregates high-quality datasets that have been shown to be of value to the machine learning research community from multiple different data repositories to provide greater coverage of the topic than is otherwise available."
"http://dbpedia.org/resource/Movidius_(company)"	"Movidius (company)"	"Machine learning"	"Movidius is a company based in San Mateo, California that designs specialised low-power processor chips for computer vision and deep-learning. It was announced that the company was to be acquired by Intel in September 2016."
"http://dbpedia.org/resource/Error_Tolerance_(PAC_learning)"	"Error Tolerance (PAC learning)"	"Machine learning"	"In PAC learning, error tolerance refers to the ability of an algorithm to learn when the examples received have been corrupted in some way. In fact, this is a very common and important issue since in many applications it is not possible to access noise-free data. Noise can interfere with the learning process at different levels: the algorithm may receive data that have been occasionally mislabeled, or the inputs may have some false information, or the classification of the examples may have been maliciously adulterated."
"http://dbpedia.org/resource/Isotropic_position"	"Isotropic position"	"Machine learning"	"In the fields of machine learning, the theory of computation, and random matrix theory, a probability distribution over vectors is said to be in isotropic position if its covariance matrix is equal to the identity matrix."
"http://dbpedia.org/resource/Multiple_instance_learning"	"Multiple instance learning"	"Machine learning"	"Depending on the type and variation in training data, machine learning can be roughly categorized into three frameworks: supervised learning, unsupervised learning, and reinforcement learning. Multiple instance learning (MIL) falls under the supervised learning framework, where every training instance has a label, either discrete or real valued. MIL deals with problems with incomplete knowledge of labels in training sets. More precisely, in multiple-instance learning, the training set consists of labeled “bags”, each of which is a collection of unlabeled instances. A bag is positively labeled if at least one instance in it is positive, and is negatively labeled if all instances in it are negative. The goal of the MIL is to predict the labels of new, unseen bags. Convenient and simple example for MIL was given in. Imagine several people, and each of them has a key chain that contains few keys. Some of these people are able to enter a certain room, and some aren’t. The task is then to predict whether a certain key or a certain key chain can get you into that room. To solve this problem we need to find the exact key that is common for all the “positive” key chains. If we can correctly identify this key, we can also correctly classify an entire key chain - positive if it contains the required key, or negative if it doesn’t."
"http://dbpedia.org/resource/Sparse_dictionary_learning"	"Sparse dictionary learning"	"Machine learning"	"Sparse dictionary learning is a representation learning method which aims at finding a sparse representation of the input data (also known as sparse coding) in the form of a linear combination of basic elements as well as those basic elements themselves. These elements are called atoms and they compose a dictionary. Atoms in the dictionary are not required to be orthogonal, and they may be an over-complete spanning set. This problem setup also allows the dimensionality of the signals being represented to be higher than the one of the signals being observed. The above two properties lead to having seemingly redundant atoms that allow multiple representations of the same signal but also provide an improvement in sparsity and flexibility of the representation. One of the key principles of dictionary learning is that the dictionary has to be inferred from the input data. The emergence of sparse dictionary learning methods was stimulated by the fact that in signal processing one typically wants to represent the input data using as few components as possible. Before this approach the general practice was to use predefined dictionaries (such as fourier or wavelet transforms). However, in certain cases a dictionary that is trained fit the input data can significantly improve the sparsity, which has applications in data decomposition, compression and analysis and has been used in the fields of image denoising and classification, video and audio processing."
"http://dbpedia.org/resource/Structured_sparsity_regularization"	"Structured sparsity regularization"	"Machine learning"	"Structured sparsity regularization is a class of methods, and an area of research in statistical learning theory, that extend and generalize sparsity regularization learning methods. Both sparsity and structured sparsity regularization methods seek to exploit the assumption that the output variable (i.e., response, or dependent variable) to be learned can be described by a reduced number of variables in the input space (i.e., the domain, space of features or explanatory variables). Sparsity regularization methods focus on selecting the input variables that best describe the output. Structured sparsity regularization methods generalize and extend sparsity regularization methods, by allowing for optimal selection over structures like groups or networks of input variables in . Common motivation for the use of structured sparsity methods are model interpretability, high-dimensional learning (where dimensionality of may be higher than the number of observations ), and reduction of computational complexity. Moreover, structured sparsity methods allow to incorporate prior assumptions on the structure of the input variables, such as overlapping groups, non-overlapping groups, and acyclic graphs. Examples of uses of structured sparsity methods include face recognition, magnetic resonance image (MRI) processing, socio-linguistic analysis in natural language processing, and analysis of genetic expression in breast cancer."
"http://dbpedia.org/resource/Online_machine_learning"	"Online machine learning"	"Machine learning algorithms"	"In computer science, online machine learning is a method of machine learning in which data becomes available in a sequential order and is used to update our best predictor for future data at each step, as opposed to batch learning techniques which generate the best predictor by learning on the entire training data set at once. Online learning is a common technique used in areas of machine learning where it is computationally infeasible to train over the entire dataset, requiring the need of out-of-core algorithms. It is also used in situations where it is necessary for the algorithm to dynamically adapt to new patterns in the data, or when the data itself is generated as a function of time, e.g. stock price prediction. Two general modelling strategies exist for online learning models: statistical learning models and adversarial models. In statistical learning models (e.g. stochastic gradient descent, perceptrons), the data samples are assumed to be independent and identically distributed random variables (i.e they are not adapting with time), and the algorithm just has a limited access to the data. In adversarial models, looking at the learning problem as a game between two players (the learner vs the data generator), the goal is to minimize losses regardless of the move played by the other player. In this model, the opponent is allowed to dynamically adapt the data generated based on the output of the learning algorithm. Spam filtering falls in this category, as the adversary will dynamically generate new spam based on the current behavior of the spam detector. Examples of algorithms in this model include follow the leader, follow the regularized leader, etc."
"http://dbpedia.org/resource/Leabra"	"Leabra"	"Machine learning algorithms"	"Leabra stands for Local, Error-driven and Associative, Biologically Realistic Algorithm. It is a model of learning which is a balance between Hebbian and error-driven learning with other network-derived characteristics. This model is used to mathematically predict outcomes based on inputs and previous learning influences. This model is heavily influenced by and contributes to neural network designs and models.This algorithm is the default algorithm in emergent (successor of PDP++) when making a new project, and is extensively used in various simulations. Hebbian learning is performed using conditional principal components analysis (CPCA) algorithm with correction factor for sparse expected activity levels. Error-driven learning is performed using GeneRec, which is a generalization of the recirculation algorithm, and approximates Almeida–Pineda recurrent backpropagation. The symmetric, midpoint version of GeneRec is used, which is equivalent to the contrastive Hebbian learning algorithm (CHL). See O'Reilly (1996; Neural Computation) for more details. The activation function is a point-neuron approximation with both discrete spiking and continuous rate-code output. Layer or unit-group level inhibition can be computed directly using a k-winners-take-all (KWTA) function, producing sparse distributed representations. The net input is computed as an average, not a sum, over connections, based on normalized, sigmoidally transformed weight values, which are subject to scaling on a connection-group level to alter relative contributions. Automatic scaling is performed to compensate for differences in expected activity level in the different projections. Documentation about this algorithm can be found in the book ""Computational Explorations in Cognitive Neuroscience: Understanding the Mind by Simulating the Brain"" published by MIT press. and in the Emergent Documentation"
"http://dbpedia.org/resource/Bootstrap_aggregating"	"Bootstrap aggregating"	"Machine learning algorithms"	"Bootstrap aggregating, also called bagging, is a machine learning ensemble meta-algorithm designed to improve the stability and accuracy of machine learning algorithms used in statistical classification and regression. It also reduces variance and helps to avoid overfitting. Although it is usually applied to decision tree methods, it can be used with any type of method. Bagging is a special case of the model averaging approach."
"http://dbpedia.org/resource/Q-learning"	"Q-learning"	"Machine learning algorithms"	"Q-learning is a model-free reinforcement learning technique. Specifically, Q-learning can be used to find an optimal action-selection policy for any given (finite) Markov decision process (MDP). It works by learning an action-value function that ultimately gives the expected utility of taking a given action in a given state and following the optimal policy thereafter. A policy is a rule that the agent follows in selecting actions, given the state it is in. When such an action-value function is learned, the optimal policy can be constructed by simply selecting the action with the highest value in each state. One of the strengths of Q-learning is that it is able to compare the expected utility of the available actions without requiring a model of the environment. Additionally, Q-learning can handle problems with stochastic transitions and rewards, without requiring any adaptations. It has been proven that for any finite MDP, Q-learning eventually finds an optimal policy, in the sense that the expected value of the total reward return over all successive steps, starting from the current state, is the maximum achievable."
"http://dbpedia.org/resource/Rprop"	"Rprop"	"Machine learning algorithms"	"Rprop, short for resilient backpropagation, is a learning heuristic for supervised learning in feedforward artificial neural networks. This is a first-order optimization algorithm. This algorithm was created by Martin Riedmiller and Heinrich Braun in 1992. Similarly to the Manhattan update rule, Rprop takes into account only the sign of the partial derivative over all patterns (not the magnitude), and acts independently on each ""weight"". For each weight, if there was a sign change of the partial derivative of the total error function compared to the last iteration, the update value for that weight is multiplied by a factor η−, where η− < 1. If the last iteration produced the same sign, the update value is multiplied by a factor of η+, where η+ > 1. The update values are calculated for each weight in the above manner, and finally each weight is changed by its own update value, in the opposite direction of that weight's partial derivative, so as to minimise the total error function. η+ is empirically set to 1.2 and η− to 0.5. Next to the cascade correlation algorithm and the Levenberg–Marquardt algorithm, Rprop is one of the fastest weight update mechanisms. RPROP is a batch update algorithm."
"http://dbpedia.org/resource/FastICA"	"FastICA"	"Machine learning algorithms"	"FastICA is an efficient and popular algorithm for independent component analysis invented by Aapo Hyvärinen at Helsinki University of Technology. Like most ICA algorithms, FastICA seeks an orthogonal rotation of prewhitened data, through a fixed-point iteration scheme, that maximizes a measure of non-Gaussianity of the rotated components. Non-gaussianity serves as a proxy for statistical independence, which is a very strong condition and requires infinite data to verify. FastICA can also be alternatively derived as an approximative Newton iteration."
"http://dbpedia.org/resource/Growing_self-organizing_map"	"Growing self-organizing map"	"Machine learning algorithms"	"A growing self-organizing map (GSOM) is a growing variant of the popular self-organizing map (SOM). The GSOM was developed to address the issue of identifying a suitable map size in the SOM. It starts with a minimal number of nodes (usually 4) and grows new nodes on the boundary based on a heuristic. By using the value called Spread Factor (SF), the data analyst has the ability to control the growth of the GSOM. All the starting nodes of the GSOM are boundary nodes, i.e. each node has the freedom to grow in its own direction at the beginning. (Fig. 1) New Nodes are grown from the boundary nodes. Once a node is selected for growing all its free neighboring positions will be grown new nodes. The figure shows the three possible node growth options for a rectangular GSOM."
"http://dbpedia.org/resource/Genetic_Algorithm_for_Rule_Set_Production"	"Genetic Algorithm for Rule Set Production"	"Machine learning algorithms"	"Genetic Algorithm for Rule Set Production (GARP) is a computer program based on genetic algorithm that creates ecological niche models for species. The generated models describe environmental conditions (precipitation, temperatures, elevation, etc.) under which the species should be able to maintain populations. As input, local observations of species and related environmental parameters are used which describe potential limits of the species' capabilities to survive. Such environmental parameters are commonly stored in geographical information systems. A GARP model is a random set of mathematical rules which can be read as limiting environmental conditions. Each rule is considered as a gene; the set of genes is combined in random ways to further generate many possible models describing the potential of the species to occur."
"http://dbpedia.org/resource/Dynamic_time_warping"	"Dynamic time warping"	"Machine learning algorithms"	"In time series analysis, dynamic time warping (DTW) is an algorithm for measuring similarity between two temporal sequences which may vary in speed. For instance, similarities in walking could be detected using DTW, even if one person was walking faster than the other, or if there were accelerations and decelerations during the course of an observation. DTW has been applied to temporal sequences of video, audio, and graphics data — indeed, any data which can be turned into a linear sequence can be analyzed with DTW. A well known application has been automatic speech recognition, to cope with different speaking speeds. Other applications include speaker recognition and online signature recognition. Also it is seen that it can be used in partial shape matching application. In general, DTW is a method that calculates an optimal match between two given sequences (e.g. time series) with certain restrictions. The sequences are ""warped"" non-linearly in the time dimension to determine a measure of their similarity independent of certain non-linear variations in the time dimension. This sequence alignment method is often used in time series classification. Although DTW measures a distance-like quantity between two given sequences, it doesn't guarantee the triangle inequality to hold."
"http://dbpedia.org/resource/Expectation–maximization_algorithm"	"Expectation–maximization algorithm"	"Machine learning algorithms"	"In statistics, an expectation–maximization (EM) algorithm is an iterative method for finding maximum likelihood or maximum a posteriori (MAP) estimates of parameters in statistical models, where the model depends on unobserved latent variables. The EM iteration alternates between performing an expectation (E) step, which creates a function for the expectation of the log-likelihood evaluated using the current estimate for the parameters, and a maximization (M) step, which computes parameters maximizing the expected log-likelihood found on the E step. These parameter-estimates are then used to determine the distribution of the latent variables in the next E step."
"http://dbpedia.org/resource/Query_level_feature"	"Query level feature"	"Machine learning algorithms"	"A query level feature or QLF is a ranking feature utilized in a machine-learned ranking algorithm. Example QLFs: 
*  How many times has this query been run in the last month? 
*  How many words are in the query? 
*  What is the sum/average/min/max/median of the BM25F values for the query?"
"http://dbpedia.org/resource/Backpropagation"	"Backpropagation"	"Machine learning algorithms"	"Backpropagation, an abbreviation for ""backward propagation of errors"", is a common method of training artificial neural networks used in conjunction with an optimization method such as gradient descent. The method calculates the gradient of a loss function with respect to all the weights in the network, so that the gradient is fed to the optimization method which in turn uses it to update the weights, in an attempt to minimize the loss function. Backpropagation requires a known, desired output for each input value in order to calculate the loss function gradient. It is therefore usually considered to be a supervised learning method, although it is also used in some unsupervised networks such as autoencoders. It is a generalization of the delta rule to multi-layered feedforward networks, made possible by using the chain rule to iteratively compute gradients for each layer. Backpropagation requires that the activation function used by the artificial neurons (or ""nodes"") be differentiable."
"http://dbpedia.org/resource/Local_outlier_factor"	"Local outlier factor"	"Machine learning algorithms"	"In anomaly detection, the local outlier factor (LOF) is an algorithm proposed by Markus M. Breunig, Hans-Peter Kriegel, Raymond T. Ng and Jörg Sander in 2000 for finding anomalous data points by measuring the local deviation of a given data point with respect to its neighbours. LOF shares some concepts with DBSCAN and OPTICS such as the concepts of ""core distance"" and ""reachability distance"", which are used for local density estimation."
"http://dbpedia.org/resource/Evolutionary_multimodal_optimization"	"Evolutionary multimodal optimization"	"Machine learning algorithms"	"In applied mathematics, multimodal optimization deals with optimization tasks that involve finding all or most of the multiple (at least locally optimal) solutions of a problem, as opposed to a single best solution.Evolutionary multimodal optimization is a branch of evolutionary computation, which is closely related to machine learning. Wong provides a short survey, wherein the chapter of Shir and the book of Preuss cover the topic in more detail."
"http://dbpedia.org/resource/State-Action-Reward-State-Action"	"State-Action-Reward-State-Action"	"Machine learning algorithms"	"State-Action-Reward-State-Action (SARSA) is an algorithm for learning a Markov decision process policy, used in the reinforcement learning area of machine learning. It was introduced in a technical note  where the alternative name SARSA was only mentioned as a footnote. This name simply reflects the fact that the main function for updating the Q-value depends on the current state of the agent ""S1"", the action the agent chooses ""A1"", the reward ""R"" the agent gets for choosing this action, the state ""S2"" that the agent will now be in after taking that action, and finally the next action ""A2"" the agent will choose in its new state. Taking every letter in the quintuple (st, at, rt, st+1, at+1) yields the word SARSA."
"http://dbpedia.org/resource/Forward–backward_algorithm"	"Forward–backward algorithm"	"Machine learning algorithms"	"The forward–backward algorithm is an  inference algorithm for hidden Markov models which computes the posterior marginals of all hidden state variables given a sequence of observations/emissions , i.e. it computes, for all hidden state variables , the distribution . This inference task is usually called smoothing. The algorithm makes use of the principle of dynamic programming to compute efficiently the values that are required to obtain the posterior marginal distributions in two passes. The first pass goes forward in time while the second goes backward in time; hence the name forward–backward algorithm. The term forward–backward algorithm is also used to refer to any algorithm belonging to the general class of algorithms that operate on sequence models in a forward–backward manner. In this sense, the descriptions in the remainder of this article refer but to one specific instance of this class."
"http://dbpedia.org/resource/Radial_basis_function_network"	"Radial basis function network"	"Machine learning algorithms"	"In the field of mathematical modeling, a radial basis function network is an artificial neural network that uses radial basis functions as activation functions. The output of the network is a linear combination of radial basis functions of the inputs and neuron parameters. Radial basis function networks have many uses, including function approximation, time series prediction, classification, and system control. They were first formulated in a 1988 paper by Broomhead and Lowe, both researchers at the Royal Signals and Radar Establishment."
"http://dbpedia.org/resource/Reinforcement_learning"	"Reinforcement learning"	"Machine learning algorithms"	"Reinforcement learning is an area of machine learning inspired by behaviorist psychology, concerned with how software agents ought to take actions in an environment so as to maximize some notion of cumulative reward. The problem, due to its generality, is studied in many other disciplines, such as game theory, control theory, operations research, information theory, simulation-based optimization, multi-agent systems, swarm intelligence, statistics, and genetic algorithms. In the operations research and control literature, the field where reinforcement learning methods are studied is called approximate dynamic programming. The problem has been studied in the theory of optimal control, though most studies are concerned with the existence of optimal solutions and their characterization, and not with the learning or approximation aspects. In economics and game theory, reinforcement learning may be used to explain how equilibrium may arise under bounded rationality. In machine learning, the environment is typically formulated as a Markov decision process (MDP) as many reinforcement learning algorithms for this context utilize dynamic programming techniques. The main difference between the classical techniques and reinforcement learning algorithms is that the latter do not need knowledge about the MDP and they target large MDPs where exact methods become infeasible. Reinforcement learning differs from standard supervised learning in that correct input/output pairs are never presented, nor sub-optimal actions explicitly corrected. Further, there is a focus on on-line performance, which involves finding a balance between exploration (of uncharted territory) and exploitation (of current knowledge). The exploration vs. exploitation trade-off in reinforcement learning has been most thoroughly studied through the multi-armed bandit problem and in finite MDPs."
"http://dbpedia.org/resource/Temporal_difference_learning"	"Temporal difference learning"	"Machine learning algorithms"	"Temporal difference (TD) learning is a prediction-based machine learning method. It has primarily been used for the reinforcement learning problem, and is said to be ""a combination of Monte Carlo ideas and dynamic programming (DP) ideas."" TD resembles a Monte Carlo method because it learns by sampling the environment according to some policy, and is related to dynamic programming techniques as it approximates its current estimate based on previously learned estimates (a process known as bootstrapping). The TD learning algorithm is related to the temporal difference model of animal learning. As a prediction method, TD learning considers that subsequent predictions are often correlated in some sense. In standard supervised predictive learning, one learns only from actually observed values: A prediction is made, and when the observation is available, the prediction mechanism is adjusted to better match the observation. As elucidated by Richard Sutton, the core idea of TD learning is that one adjusts predictions to match other, more accurate, predictions about the future. This procedure is a form of bootstrapping, as illustrated with the following example: ""Suppose you wish to predict the weather for Saturday, and you have some model that predicts Saturday's weather, given the weather of each day in the week. In the standard case, you would wait until Saturday and then adjust all your models. However, when it is, for example, Friday, you should have a pretty good idea of what the weather would be on Saturday - and thus be able to change, say, Monday's model before Saturday arrives."" Mathematically speaking, both in a standard and a TD approach, one would try to optimize some cost function, related to the error in our predictions of the expectation of some random variable, E[z]. However, while in the standard approach one in some sense assumes E[z] = z (the actual observed value), in the TD approach we use a model. For the particular case of reinforcement learning, which is the major application of TD methods, z is the total return and E[z] is given by the Bellman equation of the return."
"http://dbpedia.org/resource/Skill_chaining"	"Skill chaining"	"Machine learning algorithms"	"Skill chaining is a skill discovery method in continuous reinforcement learning."
"http://dbpedia.org/resource/Quickprop"	"Quickprop"	"Machine learning algorithms"	"Quickprop is an iterative method for determining the minimum of the loss function of an artificial neural network, following an algorithm inspired by the Newton's method. Sometimes, the algorithm is classified to the group of the second order learning methods. It follows a quadratic approximation of the previous gradient step and the current gradient, which is expected to be closed to the minimum of the loss function, under the assumption that the loss function is locally approximately square, trying to describe it by means of an upwardly open parabola. The minimum is sought in the vertex of the parabola. The procedure requires only local information of the artificial neuron to which it is applied.The k-th approximation step is given by: Being the neuron j weight of its i input and E is the loss function. The Quickprop algorithm is an implementation of the error backpropagation algorithm, but the network can behave chaotically during the learning phase due to large step sizes."
"http://dbpedia.org/resource/CN2_algorithm"	"CN2 algorithm"	"Machine learning algorithms"	"The CN2 induction algorithm is a learning algorithm for rule induction. It is designed to work even when the training data is imperfect. It is based on ideas from the AQ algorithm and the ID3 algorithm. As a consequence it creates a rule set like that created by AQ but is able to handle noisy data like ID3."
"http://dbpedia.org/resource/Loss_functions_for_classification"	"Loss functions for classification"	"Machine learning algorithms"	"In machine learning and mathematical optimization, loss functions for classification are computationally feasible loss functions representing the price paid for inaccuracy of predictions in classification problems. Given as the vector space of all possible inputs, and Y = {–1,1} as the vector space of all possible outputs, we wish to find a function which best maps to . However, because of incomplete information, noise in the measurement, or probabilistic components in the underlying process, it is possible for the same to generate different . As a result, the goal of the learning problem is to minimize expected risk, defined as where represents the loss function, and represents the probability distribution of the data, which can equivalently be written using Bayes' theorem as In practice, the probability distribution is unknown. Consequently, utilizing a training set of independently and identically distributed samples drawn from the data sample space, one seeks to  minimize empirical risk as a proxy for expected risk. (See statistical learning theory for a more detailed description.) For computational ease, it is standard practice to write loss functions as functions of only one variable. Within classification, loss functions are generally written solely in terms of the product of the true classifier and the predicted value . Selection of a loss function within this framework impacts the optimal which minimizes empirical risk, as well as the computational complexity of the learning algorithm. Given the binary nature of classification, a natural selection for a loss function (assuming equal cost for false positives and false negatives) would be the 0–1 indicator function which takes the value of 0 if the predicted classification equals that of the true class or a 1 if the predicted classification does not match the true class. This selection is modeled by where indicates the Heaviside step function.However, this loss function is non-convex and non-smooth, and solving for the optimal solution is an NP-hard combinatorial optimization problem. As a result, it is better to substitute continuous, convex loss function surrogates which are tractable for commonly used learning algorithms. In addition to their computational tractability, one can show that the solutions to the learning problem using these loss surrogates allows for the recovery of the actual solution to the original classification problem. Some of these surrogates are described below."
"http://dbpedia.org/resource/Non-negative_matrix_factorization"	"Non-negative matrix factorization"	"Machine learning algorithms"	"Non-negative matrix factorization (NMF), also non-negative matrix approximation is a group of algorithms in multivariate analysis and linear algebra where a matrix V is factorized into (usually) two matrices W and H, with the property that all three matrices have no negative elements. This non-negativity makes the resulting matrices easier to inspect. Also, in applications such as processing of audio spectrograms or muscular activity, non-negativity is inherent to the data being considered. Since the problem is not exactly solvable in general, it is commonly approximated numerically. NMF finds applications in such fields as computer vision, document clustering, chemometrics, audio signal processing and recommender systems."
"http://dbpedia.org/resource/Out-of-bag_error"	"Out-of-bag error"	"Machine learning algorithms"	"Out-of-bag (OOB) error, also called out-of-bag estimate, is a method of measuring the prediction error of random forests, boosted decision trees, and other machine learning models utilizing bootstrap aggregating to sub-sample data sampled used for training. OOB is the mean prediction error on each training sample xᵢ, using only the trees that did not have xᵢ in their bootstrap sample. Subsampling allows one to define an out-of-bag estimate of the prediction performance improvement by evaluating predictions on those observations which were not used in the building of the next base learner. Out-of-bag estimates help avoid the need for an independent validation dataset, but often underestimate actual performance improvement and the optimal number of iterations."
"http://dbpedia.org/resource/Dominance-based_rough_set_approach"	"Dominance-based rough set approach"	"Machine learning algorithms"	"Dominance-based rough set approach (DRSA) is an extension of rough set theory for multi-criteria decision analysis (MCDA), introduced by Greco, Matarazzo and Słowiński. The main change comparing to the classical rough sets is the substitution of the indiscernibility relation by a dominance relation, which permits to deal with inconsistencies typical to consideration of criteria and preference-ordered decision classes."
"http://dbpedia.org/resource/K-nearest_neighbors_algorithm"	"K-nearest neighbors algorithm"	"Machine learning algorithms"	"In pattern recognition, the k-Nearest Neighbors algorithm (or k-NN for short) is a non-parametric method used for classification and regression. In both cases, the input consists of the k closest training examples in the feature space. The output depends on whether k-NN is used for classification or regression: 
*  In k-NN classification, the output is a class membership. An object is classified by a majority vote of its neighbors, with the object being assigned to the class most common among its k nearest neighbors (k is a positive integer, typically small). If k = 1, then the object is simply assigned to the class of that single nearest neighbor. 
*  In k-NN regression, the output is the property value for the object. This value is the average of the values of its k nearest neighbors. k-NN is a type of instance-based learning, or lazy learning, where the function is only approximated locally and all computation is deferred until classification. The k-NN algorithm is among the simplest of all machine learning algorithms. Both for classification and regression, it can be useful to assign weight to the contributions of the neighbors, so that the nearer neighbors contribute more to the average than the more distant ones. For example, a common weighting scheme consists in giving each neighbor a weight of 1/d, where d is the distance to the neighbor. The neighbors are taken from a set of objects for which the class (for k-NN classification) or the object property value (for k-NN regression) is known. This can be thought of as the training set for the algorithm, though no explicit training step is required. A shortcoming of the k-NN algorithm is that it is sensitive to the local structure of the data. The algorithm is not to be confused with k-means, another popular machine learning technique."
"http://dbpedia.org/resource/Weighted_Majority_Algorithm"	"Weighted Majority Algorithm"	"Machine learning algorithms"	"In machine learning, Weighted Majority Algorithm (WMA) is a meta-learning algorithm used to construct a compound algorithm from a pool of prediction algorithms, which could be any type of learning algorithms, classifiers, or even real human experts.The algorithm assumes that we have no prior knowledge about the accuracy of the algorithms in the pool, but there are sufficient reasons to believe that one or more will perform well. Assume that the problem is a binary decision problem. To construct the compound algorithm, a positive weight is given to each of the algorithms in the pool. The compound algorithm then collects weighted votes from all the algorithms in the pool, and gives the prediction that has a higher vote. If the compound algorithm makes a mistake, the algorithms in the pool that contributed to the wrong predicting will be discounted by a certain ratio β where 0<β<1. It can be shown that the upper bounds on the number of mistakes made in a given sequence of predictions from a pool of algorithms is if one algorithm in makes at most mistakes. There are many variations of the Weighted Majority Algorithm to handle different situations, like shifting targets, infinite pools, or randomized predictions. The core mechanism remain similar, with the final performances of the compound algorithm bounded by a function of the performance of the specialist (best performing algorithm) in the pool."
"http://dbpedia.org/resource/Linde–Buzo–Gray_algorithm"	"Linde–Buzo–Gray algorithm"	"Machine learning algorithms"	"The Linde–Buzo–Gray algorithm (introduced by Yoseph Linde, Andrés Buzo and Robert M. Gray in 1980) is a vector quantization algorithm to derive a good codebook. It is similar to the k-means method in data clustering."
"http://dbpedia.org/resource/Multiple_kernel_learning"	"Multiple kernel learning"	"Machine learning algorithms"	"Multiple kernel learning refers to a set of machine learning methods that use a predefined set of kernels and learn an optimal linear or non-linear combination of kernels as part of the algorithm. Reasons to use multiple kernel learning include a) the ability to select for an optimal kernel and parameters from a larger set of kernels, reducing bias due to kernel selection while allowing for more automated machine learning methods, and b) combining data from different sources (e.g. sound and images from a video) that have different notions of similarity and thus require different kernels. Instead of creating a new kernel, multiple kernel algorithms can be used to combine kernels already established for each individual data source. Multiple kernel learning approaches have been used in many applications, such as event recognition in video, object recognition in images, and biomedical data fusion."
"http://dbpedia.org/resource/Stochastic_gradient_descent"	"Stochastic gradient descent"	"Machine learning algorithms"	"Stochastic gradient descent (often shortened in SGD), also known as incremental gradient descent, is a stochastic approximation of the gradient descent optimization method for minimizing an objective function that is written as a sum of differentiable functions.In other words, SGD tries to find minimums or maximums by iteration."
"http://dbpedia.org/resource/Logic_learning_machine"	"Logic learning machine"	"Machine learning algorithms"	"Logic Learning Machine (LLM) is a machine learning method based on the generation of intelligible rules. LLM is an efficient implementation of the Switching Neural Network (SNN) paradigm, developed by Marco Muselli, from the Italian National Research Council.Logic Learning Machine is implemented in the Rulex suite. LLM has been employed in different fields, including orthopaedic patient classification, DNA microarray analysis  and Clinical Decision Support System."
"http://dbpedia.org/resource/Kernel_principal_component_analysis"	"Kernel principal component analysis"	"Machine learning algorithms"	"In the field of multivariate statistics, kernel principal component analysis (kernel PCA) is an extension of principal component analysis (PCA) using techniques of kernel methods. Using a kernel, the originally linear operations of PCA are performed in a reproducing kernel Hilbert space."
"http://dbpedia.org/resource/IDistance"	"IDistance"	"Machine learning algorithms"	"In pattern recognition, the iDistance is an indexing and query processing technique for k-nearest neighbor queries on point data in multi-dimensional metric spaces. The kNN query is one of the hardest problems on multi-dimensional data, especially when the dimensionality of the data is high. The iDistance is designed to process kNN queries in high-dimensional spaces efficiently and it is especially good for skewed data distributions, which usually occur in real-life data sets."
"http://dbpedia.org/resource/LogitBoost"	"LogitBoost"	"Machine learning algorithms"	"In machine learning and computational learning theory, LogitBoost is a boosting algorithm formulated by Jerome Friedman, Trevor Hastie, and Robert Tibshirani. The original paper casts the AdaBoost algorithm into a statistical framework. Specifically, if one considers AdaBoost as a generalized additive model and then applies the cost functional of logistic regression, one can derive the LogitBoost algorithm."
"http://dbpedia.org/resource/Sparse_PCA"	"Sparse PCA"	"Machine learning algorithms"	"Sparse principal component analysis (sparse PCA) is a specialised technique used in statistical analysis and, in particular, in the analysis of multivariate data sets. It extends the classic method of principal component analysis (PCA) for the reduction of dimensionality of data by adding sparsity constraint on the input variables. Ordinary principal component analysis (PCA) uses a vector space transform to reduce multidimensional data sets to lower dimensions. It finds linear combinations of input variables, and transforms them into new variables (called principal components) that correspond to directions of maximal variance in the data. The number of new variables created by these linear combinations is usually much lower than the number of input variables in the original dataset, while still explaining most of the variance present in the data. A particular disadvantage of ordinary PCA is that the principal components are usually linear combinations of all input variables. Sparse PCA overcomes this disadvantage by finding linear combinations that contain just a few input variables."
"http://dbpedia.org/resource/Minimum_redundancy_feature_selection"	"Minimum redundancy feature selection"	"Machine learning algorithms"	"Minimum redundancy feature selection is an algorithm frequently used in a method to accurately identify characteristics of genes and phenotypes and narrow down their relevance and is usually described in its pairing with relevant feature selection as Minimum Redundancy Maximum Relevance (mRMR). Feature selection, one of the basic problems in pattern recognition and machine learning, identifies subsets of data that are relevant to the parameters used and is normally called Maximum Relevance. These subsets often contain material which is relevant but redundant and mRMR attempts to address this problem by removing those redundant subsets. mRMR has a variety of applications in many areas such as cancer diagnosis and speech recognition. Features can be selected in many different ways. One scheme is to select features that correlate strongest to the classification variable. This has been called maximum-relevance selection. Many heuristic algorithms can be used, such as the sequential forward, backward, or floating selections. On the other hand features can be selected to be mutually far away from each other while still having ""high"" correlation to the classification variable. This scheme, termed as Minimum Redundancy Maximum Relevance (mRMR) selection has been found to be more powerful than the maximum relevance selection. As a special case, the ""correlation"" can be replaced by the statistical dependency between variables. Mutual information can be used to quantify the dependency. In this case, it is shown that mRMR is an approximation to maximizing the dependency between the joint distribution of the selected features and the classification variable. Studies have tried different measures for redundancy and relevance measures. A recent study compared several measures within the context of biomedical images."
"http://dbpedia.org/resource/Quadratic_unconstrained_binary_optimization"	"Quadratic unconstrained binary optimization"	"Machine learning algorithms"	"Quadratic unconstrained binary optimization (QUBO) is a pattern matching technique, common in machine learning applications. QUBO is an NP hard problem. QUBO problems may sometimes be well-suited to algorithms aided by quantum annealing. QUBO is given by the formula:"
"http://dbpedia.org/resource/Randomized_weighted_majority_algorithm"	"Randomized weighted majority algorithm"	"Machine learning algorithms"	"The randomized weighted majority algorithm is an algorithm in machine learning theory.It improves the mistake bound of the weighted majority algorithm. Imagine that every morning before the stock market opens,we get a prediction from each of our ""experts"" about whether the stock market will go up or down.Our goal is to somehow combine this set of predictions into a single prediction that we then use to make a buy or sell decision for the day.The RWMA gives us a way to do this combination such that our prediction record will benearly as good as that of the single best expert in hindsight."
"http://dbpedia.org/resource/HEXQ"	"HEXQ"	"Machine learning algorithms"	"HEXQ is a reinforcement learning algorithm created by Bernhard Hengst, which attempts to solve a Markov Decision Process by decomposing it hierarchically. Bernhard Hengst (2002). ""Discovering Hierarchy in Reinforcement Learning with HEXQ""."
"http://dbpedia.org/resource/Wake-sleep_algorithm"	"Wake-sleep algorithm"	"Machine learning algorithms"	"The wake-sleep algorithm is an unsupervised learning algorithm for a stochastic multilayer neural network. The algorithm adjusts the parameters so as to produce a good density estimator. There are two learning phases, the “wake” phase and the “sleep” phase, which are performed alternately. It was first designed as a model for brain functioning using variational Bayesian learning. After that, the algorithm was adapted to machine learning. It can be viewed as a way to train a Helmholtz Machine"
"http://dbpedia.org/resource/T-distributed_stochastic_neighbor_embedding"	"T-distributed stochastic neighbor embedding"	"Machine learning algorithms"	"t-distributed stochastic neighbor embedding (t-SNE) is a machine learning algorithm for dimensionality reduction developed by Laurens van der Maaten and Geoffrey Hinton. It is a nonlinear dimensionality reduction technique that is particularly well-suited for embedding high-dimensional data into a space of two or three dimensions, which can then be visualized in a scatter plot. Specifically, it models each high-dimensional object by a two- or three-dimensional point in such a way that similar objects are modeled by nearby points and dissimilar objects are modeled by distant points. The t-SNE algorithm comprises two main stages. First, t-SNE constructs a probability distribution over pairs of high-dimensional objects in such a way that similar objects have a high probability of being picked, whilst dissimilar points have an extremely small probability of being picked. Second, t-SNE defines a similar probability distribution over the points in the low-dimensional map, and it minimizes the Kullback–Leibler divergence between the two distributions with respect to the locations of the points in the map. Note that whilst the original algorithm uses the Euclidean distance between objects as the base of its similarity metric, this should be changed as appropriate. t-SNE has been used in a wide range of applications, including computer security research, music analysis, cancer research, bioinformatics, and biomedical signal processing."
"http://dbpedia.org/resource/Almeida–Pineda_recurrent_backpropagation"	"Almeida–Pineda recurrent backpropagation"	"Machine learning algorithms"	"Almeida–Pineda recurrent backpropagation is an extension to the backpropagation algorithm that is applicable to recurrent neural networks. It is a type of supervised learning. A recurrent neural network for this algorithm consists of some input units, some output units and eventually some hidden units. For a given set of (input, target) states, the network is trained to settle into a stable activation state with the output units in the target state, based on a given input state clamped on the input units."
"http://dbpedia.org/resource/PVLV"	"PVLV"	"Machine learning algorithms"	"The primary value learned value (PVLV) model is a possible explanation for the reward-predictive firing properties of dopamine (DA) neurons. It simulates behavioral and neural data on Pavlovian conditioning and the midbrain dopaminergic neurons that fire in proportion to unexpected rewards. It is an alternative to the temporal-differences (TD) algorithm. It is used as part of Leabra."
"http://dbpedia.org/resource/GeneRec"	"GeneRec"	"Machine learning algorithms"	"GeneRec is a generalization of the recirculation algorithm, and approximates Almeida-Pineda recurrent backpropagation. It is used as part of the Leabra algorithm for error-driven learning. The symmetric, midpoint version of GeneRec is equivalent to the contrastive Hebbian learning algorithm (CHL)."
"http://dbpedia.org/resource/Dehaene–Changeux_model"	"Dehaene–Changeux model"	"Machine learning algorithms"	"The Dehaene–Changeux model (DCM), also known as the global neuronal workspace or the global cognitive workspace model is a part of Bernard Baars's ""global workspace model"" for consciousness. It is a computer model of the neural correlates of consciousness programmed as a neural network. It attempts to reproduce the swarm behaviour  of the brain's higher cognitive functions such as consciousness, decision-making and the central executive functions. It was developed by cognitive neuroscientists Stanislas Dehaene and Jean-Pierre Changeux beginning in 1986. It has been used to provide a predictive framework to the study of inattentional blindness and the solving of the Tower of London test."
"http://dbpedia.org/resource/Diffusion_map"	"Diffusion map"	"Machine learning algorithms"	"Diffusion maps is a dimensionality reduction or feature extraction algorithm introduced by R. R. Coifman and S. Lafon. It computes a family of embeddings of a data set into Euclidean space (often low-dimensional) whose coordinates can be computed from the eigenvectors and eigenvalues of a diffusion operator on the data. The Euclidean distance between points in the embedded space is equal to the ""diffusion distance"" between probability distributions centered at those points. Different from linear dimensionality reduction methods such as principal component analysis (PCA) and multi-dimensional scaling (MDS), diffusion maps is part of the family of nonlinear dimensionality reduction methods which focus on discovering the underlying manifold that the data has been sampled from. By integrating local similarities at different scales, diffusion maps gives a global description of the data-set. Compared with other methods, the diffusion maps algorithm is robust to noise perturbation and is computationally inexpensive."
"http://dbpedia.org/resource/Constructing_skill_trees"	"Constructing skill trees"	"Machine learning algorithms"	"Constructing skill trees (CST) is a hierarchical reinforcement learning algorithm which can build skill trees from a set of sample solution trajectories obtained from demonstration. CST uses an incremental MAP(maximum a posteriori ) change point detection algorithm to segment each demonstration trajectory into skills and integrate the results into a skill tree. CST was introduced by George Konidaris, Scott Kuindersma, Andrew Barto and Roderic Grupen in 2010."
"http://dbpedia.org/resource/Error-driven_learning"	"Error-driven learning"	"Machine learning algorithms"	"Error-driven learning is a sub-area of machine learning concerned with how an agent ought to take actions in an environment so as to minimize some error feedback. It is a type of reinforcement learning."
"http://dbpedia.org/resource/Kernel_methods_for_vector_output"	"Kernel methods for vector output"	"Machine learning algorithms"	"Kernel methods are a well-established tool to analyze the relationship between input data and the corresponding output of a function. Kernels encapsulate the properties of functions in a computationally efficient way and allow algorithms to easily swap functions of varying complexity. In typical machine learning algorithms, these functions produce a scalar output. Recent development of kernel methods for functions with vector-valued output is due, at least in part, to interest in simultaneously solving related problems. Kernels which capture the relationship between the problems allow them to borrow strength from each other. Algorithms of this type include multi-task learning (also called multi-output learning or vector-valued learning), transfer learning, and co-kriging. Multi-label classification can be interpreted as mapping inputs to (binary) coding vectors with length equal to the number of classes. In Gaussian processes, kernels are called covariance functions. Multiple-output functions correspond to considering multiple processes. See Bayesian interpretation of regularization for the connection between the two perspectives."
"http://dbpedia.org/resource/Hyper_basis_function_network"	"Hyper basis function network"	"Machine learning algorithms"	"In machine learning, a Hyper basis function network, or HyperBF network, is a generalization of radial basis function (RBF) networks concept, where the Mahalanobis-like distance is used instead of Euclidean distance measure. Hyper basis function networks were first introduced by Poggio and Girosi in the 1990 paper “Networks for Approximation and Learning”."
"http://dbpedia.org/resource/Prefrontal_cortex_basal_ganglia_working_memory"	"Prefrontal cortex basal ganglia working memory"	"Machine learning algorithms"	"Prefrontal cortex basal ganglia working memory (PBWM) is an algorithm that models working memory in the prefrontal cortex and the basal ganglia. It can be compared to long short-term memory (LSTM) in functionality, but is more biologically explainable. It uses the primary value learned value model to train prefrontal cortex working-memory updating system, based on the biology of the prefrontal cortex and basal ganglia. It is used as part of the Leabra framework and was implemented in Emergent."
"http://dbpedia.org/resource/Manifold_alignment"	"Manifold alignment"	"Machine learning algorithms"	"Manifold alignment is a class of machine learning algorithms that produce projections between sets of data, given that the original data sets lie on a common manifold. The concept was first introduced as such by Ham, Lee, and Saul in 2003, adding a manifold constraint to the general problem of correlating sets of high-dimensional vectors."
